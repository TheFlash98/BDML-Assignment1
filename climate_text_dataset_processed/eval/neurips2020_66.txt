In-N-Out: Pre-Training and Self-Training using Aux-
iliary Information for Out-of-Distribution Robustness
Robbie JonesSang Michael XieAnanya Kumar
Fereshte Khani Tengyu Ma Percy Liang
Stanford University
{rmjones, xie, ananya, fereshte, tengyuma, pliang}@cs.stanford.edu
Abstract
Many machine learning applications used to tackle climate change involve lots
of unlabeled data (such as satellite imagery) along with auxiliary information
such as climate data. In this work, we show how to use auxiliary information in
a semi-supervised setting to improve both in-distribution and out-of-distribution
(OOD) accuracies (e.g. for countries in Africa where we have very little labeled
data). We show that 1) on real-world datasets, the common practice of using
auxiliary information as additional input features improves in-distribution error but
can hurt OOD. Oppositely, we Ô¨Ånd that 2) using auxiliary information as outputs of
auxiliary tasks to pre-train a model improves OOD error. 3) To get the best of both
worlds, we introduce In-N-Out, which Ô¨Årst trains a model with auxiliary inputs and
uses it to pseudolabel all the in-distribution inputs, then pre-trains a model on OOD
auxiliary outputs and Ô¨Åne-tunes this model with the pseudolabels (self-training). We
show both theoretically and empirically on remote sensing datasets for land cover
prediction and cropland prediction that In-N-Out outperforms auxiliary inputs or
outputs alone on both in-distribution and OOD error.
1 Introduction
When models are tested on distributions that are different from the training distribution, they typically
suffer large drops in performance [ 44,2,22,17,3]. This is an especially salient obstacle in remote
sensing applications, which includes central problems such as predicting poverty, crop type, and land
cover from satellite imagery for downstream use in climate change mitigation [ 52,21,49,40,38].
In some developing African countries, labels are scarce due to the lack of economic resources to deploy
human workers to conduct expensive surveys [ 21]. To make accurate predictions in these countries,
we must extrapolate to out-of-distribution (OOD) examples across different geographic terrains and
political borders to create a globally applicable model.
While labels are scarce, auxiliary information (e.g. climate data in remote sensing) is often available for
every input. In many applications, we additionally have access to a large amount of unlabeled data (e.g.
global satellite imagery) along with their corresponding auxiliary information. How should auxiliary
information be used in this setting? One way is the use them directly as auxiliary input features
(aux-inputs ); another is to treat them as prediction outputs for an auxiliary task ( aux-outputs ) in
pre-training or multi-task learning. Which approach is better for in-distribution or OOD performance?
In remote sensing, satellite imagery is paired with aux-inputs to predict the desired output [ 49,54].
Aux-inputs can provide more features to improve in-distribution performance, but in this paper we
Ô¨Ånd they can hurt OOD error. Conversely, aux-output methods such as pre-training, transfer learning,
and multi-task learning may improve OOD performance by changing the inductive bias of the model
through auxiliary supervision [ 50,7]. We prove in a multi-task linear regression setting [ 13,46] that aux-
inputs can improve in-distribution error and worsen OOD error, while aux-outputs improve robustness
to arbitrary covariate shift by Ô¨Årst pre-training on unlabeled data to predict auxiliary information.
Equal contribution.
Tackling Climate Change with Machine Learning workshop at NeurIPS 2020.Aux-in
Data
(X)Auxiliary info
(Z)Labels
(Y)
Aux-out
Unlabeled data
(Xun)Unlabeled aux
(Zun)
In-N-Out
Data
(X)In-distribution
unlabeled data
(Xid
un)Labels
(Y)Pseudo-labels
(Aux-in ( Xid
un;Zid
un))
Step 1: Training (aux-inputs) Step 2: Pre-training (aux-outputs) Step 3: Fine-tuning & self-training
Figure 1: A sketch of the In-N-Out algorithm which consists of three steps: 1) use auxiliary
information as input (Aux-in), 2) use auxiliary information as output targets in pretraining (Aux-out),
3) Ô¨Åne-tune the pretrained model from step 2 using the labeled data and in-distribution unlabeled data
with pseudo-labels generated by the Aux-in model from step 1.
Can we do better than using auxiliary information as inputs or outputs alone? We propose In-N-Out
to combine the beneÔ¨Åts of auxiliary inputs and outputs (Figure 1). In-N-Out Ô¨Årst uses an aux-inputs
model, which has good in-distribution accuracy, to pseudolabel in-distribution unlabeled data. Then
we use these pseudolabels for self-training, where a pretrained model (using aux-outputs) is Ô¨Åne-tuned
on the larger labeled and pseudolabeled dataset. We prove that In-N-Out, which combines self-training
and pretraining, further improves both in-distribution and OOD error over just pretraining.
We show empirical results on CelebA and two remote sensing tasks (land cover and cropland
prediction) that matches the theory. On all datasets, In-N-Out improves OOD accuracy and has
competitive or better in-distribution accuracy (1-2% in-distribution, 2-4% OOD over baseline on
remote sensing tasks) over aux-inputs or aux-outputs alone.
2 Setup
Letx2Rdbe the input (e.g., a satellite image), y2Rbe the target (e.g., crop type), and z2RTbe
auxiliary information‚Äîtypically low-dimensional semantically meaningful features (e.g., soil type).
Training data. LetPidandPooddenote the underlying distribution of (x;y;z )triples in-distribution
and out-of-distribution, respectively. The training data consists of (i) in-distribution labeled
dataf(xi;yi;zi)gn
i=1Pid, (ii) in-distribution unlabeled data f(xid
i;zid
i)gm1
i=1Pid, and (iii)
out-of-distribution unlabeled data f(xood
i;zood
i)gm2
i=1Pood.
2.1 Models
We consider three common ways to use the auxiliary information ( z) to learn a model.
Baseline ( ^fbs).Baseline ignores auxiliary information and learns ^fbs:x7!yfrom labeled data.
Aux-inputs ( ^fin).The aux-inputs model uses the auxiliary information as features and learns a
function ^fin:(x;z)7!yfrom labeled data.
Aux-outputs ( ^fout).The aux-outputs model leverages the auxiliary information zby using them as
the prediction targets of Tauxiliary tasks, in hopes that there is a low-dimensional feature representation
wthat is common to predicting both zandy. Training the aux-outputs model consists of two steps:
In the pre-training step, we use all the unlabeled data to learn a shared feature representation w.
SpeciÔ¨Åcally, we learn a feature map ^hout:x7!wand a mapping ^gz
out:w7!zsuch that ^fout= ^gz
out^hout
minimizes the training loss on all unlabeled data pairs (x;z).
In the transfer step, the model uses the pre-trained feature map ^houtand the labeled data to learn
the mapping gy
out:w7!yfrom feature representation to target. The estimate of the target mapping
is^gy
out= argmingy
out^Rtrans(^hout;gy
out), where ^Rtrans(^hout;gy
out) =1
nPn
i=1`(gy
out(^hout(xi));yi)is the
transfer empirical risk . The Ô¨Ånal aux-outputs model is ^fout= ^gy
out^hout. Like the baseline model, the
aux-outputs model ignores the auxiliary information for prediction.
3 In-N-Out: combining auxiliary inputs and outputs
We analyze these algorithms in a multi-task linear regression setting, formalized in Appendix B and C.
We formalize all theorems in Appendix C and D and prove them in Appendix E, but give an outline here.
2Algorithm 1: In-N-Out (see Section 3 for formal description)
Data: labeled dataf(xi;yi;zi)gn
i=1Pid, in-distribution
unlabeled dataf(xid
i;zid
i)gm1
i=1Pid, OOD unlabeled data f(xood
i;zood
i)gm2
i=1Pood
Learn aux-inputs model ^fin:(x;z)7!yfrom labeled in-distribution data f(xi;yi;zi)gn
i=1Pid;
Pre-train ^hout:x7!zon aux-outputs from all unlabeled data f(xid
i;zid
i)gm1
i=1[f(xood
i;zood
i)gm2
i=1;
Return ^f= ^g^hout:x7!y, trained with pseudolabels of ^fin:f(xi;yi)gn
i=1[f(xid
i;^fin(xid
i;zid
i)gm1
i=1
CelebACroplandLandcoverVisualization (x)Aux Info (z)7 binary attributesVegetation, Lat/LonMeteorological DataTarget (y)Male/female?Cropland/not cropland?Land cover classID-SplitPeople without hatsIA, MN, ILOutside AfricaOOD-SplitPeople with hatsIN, KYAfrica
Figure 2: Summary of the datasets used in our experiments.
Aux-inputs help in-distribution, but hurt OOD. Proposition 1 in Appendix C shows that
aux-inputs help in-distribution but Example 1 shows that aux-inputs can hurt accuracy OOD .
Aux-outputs help OOD. Our Ô¨Årst main contribution is that auxiliary outputs help under arbitrary
covariate shift , assuming we have lots of unlabeled data.
Theorem 1. In the linear setting, the aux-outputs model improves the out-of-distribution risk Rood
over the baseline, for any OOD distribution:
E[Rood(^fout)]E[Rood(^fbs)]: (1)
In-N-Out. We propose the In-N-Out algorithm (Algorithm 1), which combines both the aux-inputs
and aux-outputs models for further complementary gains (Figure 1). Since the aux-inputs model has bet-
ter in-distribution performance, we use it to pseudolabel in-distribution unlabeled data (e.g., data from
non-African locations). The pseudolabeled data provides more effective training samples (self-training)
to Ô¨Åne-tune a model pre-trained by predicting aux-outputs on all unlabeled data (e.g., including Africa).
The In-N-Out model ^f= ^g^houtoptimizes the empirical risk on labeled and pseudolabeled data:
^g=argmin
g(1 )^Rtrans(^hout;g)+^Rst(g) (2)
where ^Rst(g) =1
m1Pm1
i=1`(g(^hout(xid
i));^fin(xid
i;zid
i))is the self-training loss on pseudolabels from
the aux-inputs model, and 2[0;1]trades off between labeled and pseudolabeled losses. In our
experiments, we Ô¨Åne-tune ^gand^houttogether. In the linear regression setting, we show that In-N-Out
improves over aux-outputs for arbitrary covariate shifts. The formal result is in Theorem 4 in
Appendix D, with the proof in Appendix E.
Theorem 2 (Informal) .In the linear setting, if XandZtogether contain most of the information
required to predict Y(E[Var(YjX;Z)]is small), the In-N-Out model improves the out-of-distribution
riskRoodover aux-outputs, for any OOD distribution:
E[Rood(^f)]<E[Rood(^fout)]: (3)
4 Experiments
We show on remote sensing datasets for land cover and cropland prediction that aux-inputs can hurt
OOD performance while aux-outputs improve OOD performance. In-N-Out improves OOD accuracy
and has competitive or better in-distribution accuracy over other models on all datasets.
4.1 Datasets and Experimental Setup
Cropland. Crop type or cropland prediction is an important intermediate problem for crop yield
prediction, which can aid efÔ¨Åcient agricultural practices (‚Äúprecision agriculture‚Äù) that are adaptable to
different crop regions [ 5,23,28,38]. The inputxis a5050RGB image taken by a satellite, the target
3CelebA Cropland Landcover
ID Acc OOD Acc ID Acc OOD Acc ID Acc OOD Acc
Baseline 90.46 0.85 72.641.39 94.500.11 90.300.75 75.920.25 58.311.87
Aux-inputs 92.36 0.29 77.41.33 95.340.22 84.154.23 76.580.44 54.782.01
Aux-outputs 94.00.24 77.680.59 95.120.15 91.630.21 72.480.37 61.030.97
In-N-Out (no pretrain) 93.80.56 78.541.31 94.930.15 91.230.61 76.540.23 59.190.98
In-N-Out 93.42 0.36 79.420.70 95.450.16 91.940.57 77.430.39 61.530.74
In-N-Out + repeated ST 93.760.46 80.380.68 95.530.19 92.180.40 77.100.30 62.610.58
Table 1: Accuracy (%) of models using auxiliary information as input, output, or both. In-N-Out
improves ID and OOD over aux-inputs or aux-outputs. Results are averaged over 5 trials with 90%
intervals. Repeated ST refers to one round of self-training on top of In-N-Out.
yis a binary label that is 1 when the image contains majority cropland, and the auxiliary information
zis the center location coordinate plus 5050vegetation-related bands. We use the Cropland dataset
from Wang et al. [49], with data from the US Midwest. We designate Iowa, Missouri, and Illinois as
in-distribution and Indiana and Kentucky as OOD. Following Wang et al. [49], we use a U-Net-based
model [39]. See Appendix F.1 for details.
Landcover. Land cover prediction involves classifying the land cover type (e.g., ‚Äúgrasslands‚Äù) from
satellite data at a location [ 15,40]). These land cover models can aid climate scientists in tracking
ecological deterioration such as deforestation or melting ice sheets on a global scale. The input xis
a time series measured by NASA‚Äôs MODIS satellite [ 48], the targetyis one of 6 land cover classes,
and the auxiliary information zis climate data (e.g. temperature) from the ERA5 satellite [ 4]. We
designate non-African locations as in-distribution and Africa as OOD. We use a 1D-CNN to handle
the temporal structure in the MODIS data. See Appendix F.2 for details.
CelebA. In CelebA, the input xis a RGB image (resized to 6464), the targetyis a binary label
for gender, and the auxiliary information zare 7 (of 40) binary-valued attributes (e.g. presence of
makeup, beard). We designate the set of images where the celebrity is wearing a hat as OOD. We use
a ResNet18 as the backbone model architecture for all models (see Appendix F.3 for details).
4.2 Results
Table 1 compares the in-distribution (ID) and OOD accuracy of different methods. Each method is
trained with early-stopping and hyperparameters are chosen using a validation set. In our experiments,
we also consider augmenting In-N-Out models with repeated self-training (repeated ST), which has
fueled recent improvements in both domain adaptation and ImageNet classiÔ¨Åcation [ 42,53]. For one
additional round of repeated ST, we use the In-N-Out model to pseudolabel all unlabeled data (both
ID and OOD) and also initialize the weights with the In-N-Out model. In all datasets, pretraining with
aux-outputs improves OOD performance over the baseline, and In-N-Out (with or without repeated
ST) generally improves both in- and out-of-distribution performance over all other models.
In our remote sensing datasets, aux-inputs can induce a tradeoff where increasing ID accuracy hurts
OOD performance. In cropland prediction, even with a small geographic shift (US Midwest), the
baseline model has a signiÔ¨Åcant drop from ID to OOD accuracy (4%). The aux-inputs model improves
ID accuracy almost 1% above the baseline but OOD accuracy drops 6%. In land cover prediction, using
climate features as aux-inputs decreases OOD accuracy by over 4% compared to the baseline. The
aux-outputs model improves OOD, but decreases ID accuracy by 3%. In both datasets, In-N-Out-based
models generally improve both in- and OOD accuracy over all models.
In CelebA, using auxiliary information either as aux-inputs or outputs improves both ID (2-4%) and
OOD accuracy (5%). In-N-Out achieves the best OOD performance and comparable ID performance
even though there is no tradeoff between ID and OOD accuracy. The discrepancy in ID and OOD
behavior compared to Cropland and Landcover underscores the importance of using real-world remote
sensing datasets in addition to synthetic datasets.
5 Conclusion
While auxiliary inputs improve in-distribution and OOD on standard curated datasets, they can
hurt OOD on real-world tasks important for addressing climate change. In contrast, using auxiliary
information as outputs by pretraining improves OOD performance. In-N-Out combines the strengths
of auxiliary inputs and outputs for further improvements. We note that the division between inputs
and auxiliary information is not well-deÔ¨Åned. Our framework applies generally to any division of
the features, but an important further question is how to optimally choose what to use as auxiliary
information under distribution shifts.
4Acknowledgments and Disclosure of Funding
We thank Sherrie Wang, Andreas Schlueter, Albert Gu, Daniel Levy, Pang Wei Koh, and Shiori Sagawa,
and anonymous reviewers for their valuable help and comments. This work was supported by an Open
Philanthropy Project Award, an NSF Frontier Award as part of the Center for Trustworthy Machine
Learning (CTML), SDSI, and SAIL at Stanford University. SMX was supported by an NDSEG
Fellowship. AK was supported by a Stanford Graduate Fellowship.
References
[1]S. Ahmad, A. Kalra, and H. Stephen. Estimating soil moisture using remote sensing data: A
machine learning approach. Advances in Water Resources , 33(1):69‚Äì80, 2010.
[2]E. AlBadawy, A. Saha, and M. Mazurowski. Deep learning for segmentation of brain tumors:
Impact of cross-institutional training and testing. Med Phys. , 45, 2018.
[3]J. Blitzer and F. Pereira. Domain adaptation of natural language processing systems. University
of Pennsylvania , 2007.
[4] C3S. ERA5: Fifth generation of ECMWF atmospheric reanalyses of the global climate, 2017.
[5]Y . Cai, K. Guan, J. Peng, S. Wang, C. Seifert, B. Wardlow, and Z. Li. A high-performance and
in-season classiÔ¨Åcation system of Ô¨Åeld-level crop types using time-series landsat data and a
machine learning approach. Remote Sensing of Environment , 210:74‚Äì84, 2018.
[6]Y . Carmon, A. Raghunathan, L. Schmidt, P. Liang, and J. C. Duchi. Unlabeled data improves
adversarial robustness. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.
[7] R. Caruana. Multitask learning. Machine Learning , 28:41‚Äì75, 1997.
[8]R. Caruana and V . R. de Sa. BeneÔ¨Åtting from the variables that variable selection discards.
Journal of Machine Learning Research (JMLR) , 3, 2003.
[9]Y . Chen, C. Wei, A. Kumar, and T. Ma. Self-training avoids using spurious features under domain
shift. In Advances in Neural Information Processing Systems (NeurIPS) , 2020.
[10] R. DeFries, M. Hansen, and J. Townshend. Global discrimination of land cover types from metrics
derived from A VHRR pathÔ¨Ånder data. Remote Sensing of Environment , 54(3):209‚Äì222, 1995.
[11] R. S. DeFries and J. Townshend. NDVI-derived land cover classiÔ¨Åcations at a global scale.
International Journal of Remote Sensing , 15(17):3567‚Äì3586, 1994.
[12] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Association for Computational Linguistics (ACL) ,
pages 4171‚Äì4186, 2019.
[13] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. Few-shot learning via learning the
representation, provably. arXiv , 2020.
[14] Y . Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and
V . Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning
Research (JMLR) , 17, 2016.
[15] P. O. Gislason, J. A. Benediktsson, and J. R. Sveinsson. Random forests for land cover
classiÔ¨Åcation. Pattern Recognition Letters , 27(4):294‚Äì300, 2006.
[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition (CVPR) , 2016.
[17] D. Hendrycks, K. Lee, and M. Mazeika. Using pre-training can improve model robustness and
uncertainty. In International Conference on Machine Learning (ICML) , 2019.
[18] D. Hendrycks, M. Mazeika, S. Kadavath, and D. Song. Using self-supervised learning can
improve model robustness and uncertainty. In Advances in Neural Information Processing
Systems (NeurIPS) , 2019.
5[19] J. Hoffman, E. Tzeng, T. Park, J. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada:
Cycle consistent adversarial domain adaptation. In International Conference on Machine
Learning (ICML) , 2018.
[20] D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. In Conference
on Learning Theory (COLT) , 2012.
[21] N. Jean, M. Burke, M. Xie, W. M. Davis, D. B. Lobell, and S. Ermon. Combining satellite
imagery and machine learning to predict poverty. Science , 353, 2016.
[22] R. Jia and P. Liang. Adversarial examples for evaluating reading comprehension systems. In
Empirical Methods in Natural Language Processing (EMNLP) , 2017.
[23] M. D. Johnson, W. W. Hsieh, A. J. Cannon, A. Davidson, and F. B√©dard. Crop yield forecasting
on the canadian prairies by remotely sensed vegetation indices and machine learning methods.
Agricultural and Forest Meteorology , 218:74‚Äì84, 2016.
[24] S. K and Z. A. Very deep convolutional networks for large-scale image recognition. In
International Conference on Learning Representations (ICLR) , 2015.
[25] S. Kiranyaz, O. Avci, O. Abdeljaber, T. Ince, M. Gabbouj, and D. J. Inman. 1d convolutional
neural networks and applications: A survey. arXiv preprint arXiv:1905.03554 , 2019.
[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiÔ¨Åcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems (NeurIPS) , pages
1097‚Äì1105, 2012.
[27] A. Kumar, T. Ma, and P. Liang. Understanding self-training for gradual domain adaptation. In
International Conference on Machine Learning (ICML) , 2020.
[28] N. Kussul, M. Lavreniuk, S. Skakun, and A. Shelestov. Deep learning classiÔ¨Åcation of land cover
and crop types using remote sensing data. IEEE Geoscience and Remote Sensing Letters , 14
(5):778‚Äì782, 2017.
[29] D. J. Lary, A. H. Alavi, A. H. Gandomi, and A. L. Walker. Machine learning in geosciences
and remote sensing. Geoscience Frontiers , 7(1):3‚Äì10, 2016.
[30] A. Li, S. Liang, A. Wang, and J. Qin. Estimating crop yield from multi-temporal satellite data
using multivariate regression and neural network techniques. Photogrammetric Engineering
& Remote Sensing , 73(10):1149‚Äì1157, 2007.
[31] R. Lunetta, J. F. Knight, J. E. J. G. Lyon, and L. D. Worthy. Land-cover change detection using
multi-temporal MODIS NDVI data. Remote sensing of environment , 105(2):142‚Äì154, 2006.
[32] A. E. Maxwell, T. A. Warner, and F. Fang. Implementation of machine-learning classiÔ¨Åcation in
remote sensing: an applied review. International Journal of Remote Sensing , 39(9):2784‚Äì2817,
2018.
[33] A. NajaÔ¨Å, S. Maeda, M. Koyama, and T. Miyato. Robustness to adversarial perturbations
in learning from incomplete data. In Advances in Neural Information Processing Systems
(NeurIPS) , 2019.
[34] A. Raghunathan, S. M. Xie, F. Yang, J. C. Duchi, and P. Liang. Understanding and mitigating
the tradeoff between robustness and accuracy. In International Conference on Machine Learning
(ICML) , 2020.
[35] A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. R√©. Snorkel: Rapid training data cre-
ation with weak supervision. In Very Large Data Bases (VLDB) , number 3, pages 269‚Äì282, 2017.
[36] A. J. Ratner, C. M. D. Sa, S. Wu, D. Selsam, and C. R√©. Data programming: Creating large
training sets, quickly. In Advances in Neural Information Processing Systems (NeurIPS) , pages
3567‚Äì3575, 2016.
[37] B. Recht, R. Roelofs, L. Schmidt, and V . Shankar. Do imagenet classiÔ¨Åers generalize to imagenet?
InInternational Conference on Machine Learning (ICML) , 2019.
6[38] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski, A. Lacoste, K. Sankaran, A. R., N. Milojevic-
Dupont, N. Jaques, A. Waldman-Brown, et al. Tackling climate change with machine learning.
arXiv preprint arXiv:1906.05433 , 2019.
[39] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image
segmentation. arXiv , 2015.
[40] M. Ru√üwurm, S. Wang, M. Korner, and D. Lobell. Meta-learning for few-shot land cover
classiÔ¨Åcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pages 200‚Äì201, 2020.
[41] S. Santurkar, D. Tsipras, and A. Madry. Breeds: Benchmarks for subpopulation shift. arXiv , 2020.
[42] R. Shu, H. H. Bui, H. Narui, and S. Ermon. A DIRT-T approach to unsupervised domain
adaptation. In International Conference on Learning Representations (ICLR) , 2018.
[43] M. Sugiyama, M. Krauledat, and K. Muller. Covariate shift adaptation by importance weighted
cross validation. Journal of Machine Learning Research (JMLR) , 8:985‚Äì1005, 2007.
[44] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR) ,
2014.
[45] T. Tao. Topics in random matrix theory . American Mathematical Society, 2012.
[46] N. Tripuraneni, M. I. Jordan, and C. Jin. On the theory of transfer learning: The importance
of task diversity. arXiv , 2020.
[47] J. Uesato, J. Alayrac, P. Huang, R. Stanforth, A. Fawzi, and P. Kohli. Are labels required for
improving adversarial robustness? In Advances in Neural Information Processing Systems
(NeurIPS) , 2019.
[48] E. Vermote. MOD09A1 MODIS/terra surface reÔ¨Çectance 8-day L3 global 500m SIN grid V006.
https://doi.org/10.5067/MODIS/MOD09A1.006 , 2015.
[49] S. Wang, W. Chen, S. M. Xie, G. Azzari, and D. B. Lobell. Weakly supervised deep learning
for segmentation of remote sensing imagery. Remote Sensing , 12, 2020.
[50] K. Weiss, T. M. Khoshgoftaar, and D. Wang. A survey of transfer learning. Journal of Big Data ,
3, 2016.
[51] S. Wu, H. R. Zhang, and C. R√©. Understanding and improving information transfer in multi-task
learning. In International Conference on Learning Representations (ICLR) , 2020.
[52] M. Xie, N. Jean, M. Burke, D. Lobell, and S. Ermon. Transfer learning from deep features
for remote sensing and poverty mapping. In Association for the Advancement of ArtiÔ¨Åcial
Intelligence (AAAI) , 2016.
[53] Q. Xie, M. Luong, E. Hovy, and Q. V . Le. Self-training with noisy student improves imagenet
classiÔ¨Åcation. arXiv , 2020.
[54] C. Yeh, A. Perez, A. Driscoll, G. Azzari, Z. Tang, D. Lobell, S. Ermon, and M. Burke. Using
publicly available satellite imagery and deep learning to understand economic well-being in
africa. Nature Communications , 11, 2020.
[55] B. Zoph, G. Ghiasi, T. Lin, Y . Cui, H. Liu, E. D. Cubuk, and Q. V . Le. Rethinking pre-training
and self-training. arXiv , 2020.
7A Additional related works
Multi-task learning and weak supervision. Caruana and de Sa [8]proposed using poor input
features as a multi-task output, but do not theoretically analyze this. Wu et al. [51] also study multi-task
linear regression. However, their auxiliary tasks must have true parameters that are closely aligned
(small cosine distance) to the target task. Similarly, weak supervision works assume access to weak
labels correlated with the true label [ 36,35]. In our paper, we make no assumptions about the alignment
of the auxiliary and target tasks beyond a shared latent variable while also considering distribution
shifts.
Transfer learning, pre-training, and self-supervision. We support empirical works that show
the success of transfer learning and pre-training in vision and NLP [ 26,24,12]. Theoretically, Du
et al. [13], Tripuraneni et al. [46]study pre-training in a similar linear regression setup. They show
in-distribution generalization bound improvements, but do not consider OOD robustness or combining
with auxiliary inputs. Hendrycks et al. [18] shows empirically that self-supervision can improve
robustness to synthetic corruptions. We show similar robustness beneÔ¨Åts for pre-training on auxiliary
information (not part of the original input).
Self-training for robustness. [34] analyze robust self-training (RST) Carmon et al. [6], NajaÔ¨Å
et al. [33], Uesato et al. [47], which improves the tradeoff between standard and adversarially robust
accuracy, in min-norm linear regression. While related, we work in multi-task linear regression, study
pre-training, and prove robustness to arbitrary covariate shifts (rather than adversarial perturbations).
Kumar et al. [27] show that repeated self-training on gradually shifting unlabeled data can enable
adaptation over time. In-N-Out is complementary and may provide better pseudolabels in each step of
this method. Chen et al. [9]show that self-training can remove spurious features for Gaussian input
features in linear models, whereas our results hold for general input distributions (with density). Zoph
et al. [55] show that self-training and pre-training combine for in-distribution gains. We provide theory
to support this and also show beneÔ¨Åts for OOD robustness.
Domain adaptation. Domain adaptation works account for covariate shift by using unlabeled data
from a target domain to adapt the model [ 42,19,14]. Often, these methods [ 42,19] have a self-training
or entropy minimization component that beneÔ¨Åts from having a better model in the target domain
to begin with. Similarly, domain adversarial methods [ 14] rely on the inductive bias of the source-only
model to correctly align the source and target distributions. In-N-Out may provide a better starting
point for these domain adaptation methods.
B Formal Setup
Letx2Rdbe the input (e.g., a satellite image), y2Rbe the target (e.g., crop type), and z2RTbe
auxiliary information‚Äîtypically low-dimensional semantically meaningful features (e.g., soil type).
Training data. LetPidandPooddenote the underlying distribution of (x;y;z )triples in-distribution
and out-of-distribution, respectively. The training data consists of (i) in-distribution labeled
dataf(xi;yi;zi)gn
i=1Pid, (ii) in-distribution unlabeled data f(xid
i;zid
i)gm1
i=1Pid, and (iii)
out-of-distribution unlabeled data f(xood
i;zood
i)gm2
i=1Pood.
Loss metrics. Our goal is to learn a model from input and auxiliary information to the target,
f:RdRT!R. For a loss function `, the in-distribution population risk of the model fis
Rid(f)=Ex;y;zPid[`(f(x;z);y)], and its OOD population risk is Rood(f)=Ex;y;zPood[`(f(x;z);y)].
B.1 Models
We consider three common ways to use the auxiliary information ( z) to learn a model.
Baseline. The baseline minimizes the empirical risk on labeled data while ignoring the auxiliary
information (accomplished by setting zto 0):
^fbs=argmin
fnX
i=1`(f(xi;0);yi): (4)
Aux-inputs. The aux-inputs model minimizes the empirical risk on labeled data while using the
auxiliary information as features:
^fin=argmin
fnX
i=1`(f(xi;zi);yi): (5)
8Aux-outputs. The aux-outputs model leverages the auxiliary information zby using them as the
prediction targets of Tauxiliary tasks, in hopes that there is a low-dimensional feature representation
that is common to predicting both zandy. Training the aux-outputs model consists of two steps:
In the pre-training step, we use all the unlabeled data to learn a shared feature representation. Let
h:Rd!Rkdenote a feature map and gz
out:Rk!RTdenote a mapping from feature representation
to the auxiliary outputs. Let `auxdenote the loss function for the auxiliary information. We deÔ¨Åne
the empirical risk of handgz
outas:
^Rpre(h;gz
out)=1
m1+m2m1X
i=1`aux(gz
out(h(xid
i));zid
i)+m2X
i=1`aux(gz
out(h(xood
i));zood
i)
: (6)
The estimate of the feature map is ^hout=argminhmingz
out^Rpre(h;gz
out).
In the transfer step, the model uses the pre-trained feature map ^houtand the labeled data to learn the
mappinggy
out:Rk!Rfrom feature representation to target y. We deÔ¨Åne the transfer empirical risk as:
^Rtrans(^hout;gy
out)=1
nnX
i=1`(gy
out(^hout(xi));yi) (7)
The estimate of the target mapping is ^gy
out=argmingy
out^Rtrans(^hout;gy
out). The Ô¨Ånal aux-outputs model is
^fout(x;z)= ^gy
out(^hout(x)): (8)
Like the baseline model, the aux-outputs model ignores the auxiliary information for prediction.
C Theoretical Analysis of Aux-inputs and Aux-outputs Models
We now analyze the baseline, aux-inputs, and aux-outputs models introduced in Section 2. Our setup
extends a linear regression setting commonly used for analyzing multi-task problems [13, 46].
Setup. See Figure 3 for the graphical model. Let w=B?x2Rkbe a low-dimensional latent feature
(kd)shared between auxiliary information zand the target y. Letu2Rmdenote unobserved latent
variables not captured in x. We assume zandyare linear functions of uandw:
y=>
ww+>
uu+; (9)
z=A?w+C?u; (10)
wherePdenotes noise with mean 0and variance 2. As in [ 13], we assume TkandTm
andA?;B?andC?have rankk.
Data. LetPxandPudenote the underlying distribution of xanduin-distribution, and let P0
x,P0
udenote their underlying distribution OOD. We assume xanduare independent, have bounded density
everywhere, and have Ô¨Ånite invertible covariance matrices. We assume the mean of uis zero in-
and out-of-distribution2. We assume we have nm+din-distribution labeled training examples
and unlimited access to unlabeled data both in- and out-of-distribution, a common assumption in
unsupervised domain adaptation [43, 27, 34].
Loss metrics. We use squared-error for target and auxiliary losses: `(^y;y) = (y ^y)2and
`aux(z;z0)=kz z0k2
2.
Models. We assume all model families ( f,h,gz
out,gy
out) in Section 2 are linear.
LetP=(A?;B?;C?;w;u;Px;Pu)denote the problem setting which satisÔ¨Åes all the assumptions.
C.1 Auxiliary inputs help in-distribution, but can hurt OOD
We show that the aux-inputs model (5) performs better than the baseline model (4) in-distribution
but might perform worse than the baseline model OOD. Intuitively, the target ydepends on both the
inputsx(throughw) and latent variable u(Figure Figure 3). The baseline model only uses xto predict
y; thus it cannot capture the variation in ydue tou. On the other hand, the aux-inputs model uses x
andzto predicty. Sincezis a function of x(throughw) andu,ucan be recovered from xandzby
inverting this relation. The aux-inputs model can then combine uandxto predictybetter.
Let2
u=EuPu[(>
uu)2]denote the (in-distribution) variance of ydue to the latent variables u. The
following proposition shows that if 2
u>0then with enough training examples the aux-inputs model
has lower in-distribution population risk than the baseline model.3
2This is not limiting because bias in zcan be folded into x.
3Since zis typically low-dimensional and xis high-dimensional (e.g. images), we only need a slightly larger
number of examples for the aux-inputs model before it outperforms the baseline.
9Proposition 1. For all problem settings P,P, assuming regularity conditions (bounded x;u,
sub-Gaussian noise , andT=m), and2
u>0, for all>0, there exists Nsuch that for nNnumber
of training points, with probability at least 1 over the training examples, the aux-inputs model
improves over the baseline:
Rid(^fin)<R id(^fbs) (11)
Although using zas input leads to better in-distribution performance, we show that the aux-inputs
model can perform worse than the baseline model in OOD for any number of training examples. The
aux-inputs model learns to predict ^y=^>
x;inx+^>
z;inz, where ^z;inis an approximation to the true
parameterz, that has some error. Out-of-distribution uand hencezmight have much higher variance
thanx, which would magnify the error ^z;in zand lead to worse predictions.
Example 1. There exists a problem setting P,P, such that for every n, there is some test distribution
P0
x;P0
uwith:
E[Rood(^fin)]>E[Rood(^fbs)] (12)
C.2 Pre-training improves risk under arbitrary covariate shift
While using zas inputs can worsen performance relative to the baseline, our Ô¨Årst main result is that
the aux-outputs model outperforms the baseline model under arbitrary covariate shifts.
Theorem 3. For all problem settings P,P, and for all test distributions P0
xandP0
u:
E[Rood(^fout)]E[Rood(^fbs)] (13)
See Appendix E for full proof. Intuitively, pre-training by learning a low rank linear model from xto
zallows us to learn the lower dimensional feature space w=B?x(up to symmetries). The aux-outputs
model learns a linear map from the lower-dimensional wtoy, while the baseline predicts ydirectly
fromx.Without distribution shift , standard techniques show that the aux-outputs model has better risk
sincewis lower dimensional than x. In particular, the in-domain risk only depends on the dimension
but not on the conditioning of the data. In contrast, the worst case risk under distribution shift depends
on the conditioning of the data , which could be worse for wthanx. Our proof shows that the worst
case risk (over all xandu) is still better for the aux-outputs model by ‚Äúzeroing-out‚Äù error directions
when projecting to the low-dimensional feature representation.
D In-N-Out improves risk under arbitrary covariate shift
Setup. For the theory we analyze a slightly modiÔ¨Åed version of In-N-Out where the aux-inputs model
is trained on the features ^hout(x)and we self-train on population unlabeled data. We train an aux-inputs
model ^gin:RkRT!Rgiven by ^gin= argming1
nPn
i=1`(g(^hout(xi);zi);yi). The self-training loss
is:Rst(g)=1
nPn
i=1EzPzjxi[`(g(^hout(xi));^gin(^hout(xi);zi))], and we learn ^g=argmingRst(g). For
inputx;zthe model outputs ^g(^hout(x)). For the theory, we assume all model families are linear.
We show that In-N-Out helps on top of pre-training, as long as the auxiliary features give us lots of
information about yrelative to the noise in-distribution‚Äîin particular, if 2
uis much larger than 2.
Theorem 4. In the linear setting, for all problem settings Pwith2
u>0, test distributions P0
x;P0
u, and
>0, there exists a;b> 0such that for all P, with probability at least 1 over the training examples
and test example x0P0
x, the ratio of the excess risks (for all 2small enough that a b2>0) is:
Rood
in-out R
Roodout R2
a b2(14)
HereR= minfRood(f)is the minimum possible (Bayes-optimal) OOD risk,
Rood
in-out =Ey0P0
y0jx0[`(^g(^hout(x0));y0)]is the risk of the In-N-Out model on test example x0,
andRood
out=Ey0P0
y0jx0[`(^gy
out(^hout(x0));y0)]is the risk of the aux-outputs model on test example x0.
Remark 1. As!0, the excess risk ratio of In-N-Out to Aux-outputs goes to 0, and the In-N-Out
estimator is much better than the aux-outputs estimator.
The proof of the result is in Appendix E, but we give high level intuition here. Since ucan be recovered
fromwandz, we can write y=>
ww+>
zz+. We train an aux-inputs model ^ginfromw;ztoyon
Ô¨Ånite labeled data‚Äîsince the noise 2=E[2]is small this model is very accurate. In the special
case where= 0,^ginpredictsyperfectly from w;z. For each training example wi, we sample many
zPzjwiand pseudolabel the wi;zexamples (very accurately). We then minimize the mean-squared
10	ùë•	ùëß	ùë§	ùë¶	ùë¢ùêµ‚àóùê¥‚àóùê∂‚àóùúÉ"ùúÉ#Figure 3: Graphical model for our theoretical setting, where auxiliary information zis related to targets
ythrough the latent variable wand latent noise u.
error, which is equivalent to predicting the mean ^yiof these pseudolabels from wi. This essentially
averages over latent noise u, so^yiis a denoised version of yi. In the special case where = 0, the
pseudolabel ^yi=>
ww(without the uterm) so we learn the Bayes-opt model w.
The technical challenge is proving that self-training helps under arbitrary covariate shift even when
>0(the aux-inputs model is very accurate but not perfect). The proof reduces to showing that the
max singular value for the In-N-Out error matrix is less than the min-singular value of the aux-outputs
error matrix with high probability. A core part of the argument is to lower bound the min-singular value
of a random matrix (Lemma 4). This uses techniques from random matrix theory (see e.g Chapter
2.7 in Tao [45]), the high level idea is to show that with probability 1 each column of the random
matrix has a (not too small) component orthogonal to all other columns.
E Proof for Sections C and 3
Our theoretical setting assumes all the model families are linear. We begin by specializing the setup
in Section 2 and deÔ¨Åning all the necessary matrices. A word on notation: if unspeciÔ¨Åed, expectations
are taken over all random variables.
Data matrices : We have Ô¨Ånite labeled data in-distribution: nd+minput examples X2Rnd,
where each row XiPxis an example sampled independently. We have an unobserved latent matrix:
U2Rnmwhere each row UiPuis sampled independently. Uis not used by any of the models,
but we will reference Uin our analysis. We have labels Y2Rnand auxiliary data Z2RnT, where
each rowYi;Ziis sampled jointly given input example Xi;Ui, that is:Yi;ZiPy;zjXi;Ui.
E.1 Models and evaluation
Baseline : ordinary least squares estimator that uses xonly, so ^x;ols=argmin0kY X0k2. Given
a test example x;z, the baseline method predicts ^fbs(x;z) =^>
x;olsx, ignoringz. In closed form,
^x;ols=(X>X) 1X>Y.
Aux-inputs : least squares estimator using xand auxiliary zas input: ^x;in;^z;in =
argmin0x;0zkY (X0
x+Z0
z)k2. The input method predicts ^>
x;inx+^>
z;inzfor a test ex-
amplex;z. In closed form, letting XZ=[X;Z], where we append the columns so that XZ2Rn(d+T),
[^x;in;^z;in]=(XZ>XZ) 1XZ>Y
Aux-outputs : pretrains on predicting zfromxon unlabeled data to learn a mapping from xtow, then
learns a regression model on top of this latent embedding w. In the pretraining step: use unlabeled
data to learn the feature-space embedding ^B:
^A;^B=argmin
A;BE
xPx[kABx zk2
2]A2RTk;B2Rkd(15)
The transfer step solves a lower dimensional regression problem from wtoy:^w;out =
argmin0wkY X^B>0
wk2. Given a test example x, the output model predicts ^>
w;out^Bx
11In-N-Out : First learn an output model ^A;^B, and letW=X^B>be the feature matrix. Next, train an
input model on the feature space wso we get:
^w;^z=argmin
^w;^zk(Y (W^w+Z^z))2k (16)
We now use the input model to pseudolabel our in-domain unlabeled examples, and self-train
a model without z on these pseudolabels. Given each point w, we produce a pseudolabel
EzPzjw[ ^w>w+ ^z>z] = ( ^w+A>^z)>w. We now learn a least squares estimator from wto the
pseudolabels which gives us the In-N-Out estimator ^w:
^w= ^w+A>^z (17)
Note that we do not actually have access to A, this is just the closed form for the Ô¨Ånal estimator that
self-training on the pseudolabels gives us. Given a test example x, In-N-Out predicts ^>
w^Bx.
E.2 Auxiliary inputs help in-distribution
The proof of Proposition 1 is fairly standard. We Ô¨Årst give a brief sketch, specify the additional regularity
conditions, and then give the proof. We lower bound the risk of the baseline by 2
u+2since this is
the Bayes-opt risk of using only xbut notzto predicty. We upper bound the risk of the aux-inputs
model which uses x;zto predicty, which is the same as upper bounding the risk in random design
linear regression. For this upper bound we use Theorem 1 in Hsu et al. [20] (note that there are multiple
versions of this paper, and we speciÔ¨Åcally use the Arxiv version, e.g. available at https://arxiv.
org/abs/1106.2363 ). As such, we inherit their regularity conditions. In particular, we assume:
1.x;uare upper bounded almost surely. This is a technical condition, and can be replaced with
sub-Gaussian tail assumptions [20].
2. The noise is sub-Gaussian with variance parameter 2.
3.The latent dimension mand auxiliary dimension Tare equal so that the inputs to the aux-inputs
model have invertible covariance matrix.4
Restatement of Proposition 1. For all problem settings P,P, assuming regularity conditions
(boundedx;u, sub-Gaussian noise , andT=m), and2
u>0, for all>0, there exists Nsuch that
fornNnumber of training points, with probability at least 1 over the training examples, the
aux-inputs model improves over the baseline:
Rid(^fin)<R id(^fbs) (18)
Proof. Lower bound risk of baseline : First, we lower bound the expected risk of the baseline by
2
u+2. Intuitively, this is the irreducible error‚Äîno linear classiÔ¨Åer using only xcan get better risk
than2
u+2because of intrinsic noise in the output y. Letx=B?>wbe the optimal baseline
parameters. We have:
Rid(^fbs)= E
x;y;zPid[(y ^>
x;olsx)2] (19)
=E
x;u;Pid[((>
xx+>
uu+) ^>
x;olsx)2] (20)
=E
xPid[(>
xx ^>
x;olsx)2]+E
uPid[>
uu2]+E
Pid[2] (21)
E
uPid[>
uu2]+E
Pid[2] (22)
=2
u+2(23)
To get Equation 21, we expand the square, use linearity of expectation, and use the fact that x;u; are
independent where u;are mean 0.
Upper bound risk of aux-inputs : On the other hand, we will show that if nis sufÔ¨Åciently large, the
expected risk of the input model is less than 2
u+2.
First we show that we can write y=0>
xx+0>
zz+for some0
x;0
z, that isyis a well-speciÔ¨Åed
linear function of xandzplus some noise. Intuitively this is because yis a linear function of x;u
and sinceC?is invertible we can extract ufromx;z. Formally, we assumed the true model is linear,
that is,y=>
xx+>
uu+. Since we have z=A?B?x+C?uwhereC?is invertible, we can write
u=C? 1(z A?B?x). This gives us:
4xanduare independent, with invertible covariance matrices, and z=A?B?x+C?uwhere C?is full rank,
so by block Gaussian elimination we can see that [x;z]has invertible covariance matrix as well.
12y=>
xx+>
uu+ (24)
=>
xx+>
uC? 1(z A?B?x)+ (25)
=(x B?>A?>(C?>) 1u)>x+(C?>) 1>
uz+ (26)
So setting0
x=x B?>A?>(C?>) 1uand0
z=C?> 1u, we gety=0>
xx+0>
zz+.
As before, we note that the total mean squared error can be decomposed into the Bayes-opt error plus
the excess error:
Rid(^fin)= E
x;y;zPid[(y ^>
x;inx ^>
z;inz)2] (27)
=E
x;z;Pid[((0>
xx+0>
zz+) ^>
x;inx ^>
z;inz)2] (28)
=E
x;zPid[(0>
xx+0>
zz ^>
x;inx ^>
z;inz)2]+E
Pid[2] (29)
=E
x;zPid[(0>
xx+0>
zz ^>
x;inx ^>
z;inz)2]+2(30)
To get Equation 29, we expand the square, use linearity of expectation, and use the fact
thatx; z;  are independent with E[] = 0 . So it sufÔ¨Åces to bound the excess error,
EE=Ex;zPid[(0>
xx+0>
zz ^>
x;inx ^>
z;inz)2].
To bound the excess error, we use Theorem 1 in Hsu et al. [20]where the inputs/covariates are [x;z].
E[[x;z][x;z]>]is invertible because m=T,C?is full rank, and Px;Puhave density everywhere with
density upper bounded so the variance in any direction is positive, and so the population covariance ma-
trix is positive deÔ¨Ånite. This means E[[x;z][x;z]>]has min singular value lower bounded, and we also
have thatx,zare bounded random variables. Therefore, Condition 1 is satisÔ¨Åed for some Ô¨Ånite 0. Con-
dition 2 is satisÔ¨Åed since the noise is sub-Gaussian with mean 0and variance parameter 2. Condition
3 is satisÔ¨Åed with b0=0, since we are working in the setting of well-speciÔ¨Åed linear regression.
To apply Theorem 1 [ 20], we Ô¨Årst choose t= log3
so that 1 3e t1 , and so the statement of
the Theorem holds with probability at least 1 . Since our true model is linear (or as Remark 9 says
that ‚Äúthe linear model is correct‚Äù), approx (x)=0 .
So as per remark 9 [ 20] Equation 11, for some constant c0, we have an upper bound on the excess error
EEwith probability at least 1 :
EE2(d+2p
dt+2t)
n+o(1=n) (31)
Note that the notation in Hsu et al. [20]is different. The learned estimator in ordinary least squares
regression is denoted by ^0, the ground truth parameters by , and the excess error is denoted by
k^0 k. See section 2.1, 2.2 of Hsu et al. [20] for more details.
Sincetis Ô¨Åxed, there exists some constant c(dependent on ) such that for large enough N1ifnN1:
EE2(cd=n) (32)
Note that this is precisely Remark 10 [ 20]. Remark 10 says that k^0 0kis within constant factors
of2d=nfor large enough n. This is the variance term, but the bias term is 0since the linear model
is well-speciÔ¨Åed so approx (x)=0 . As in Propostion 2 [ 20] the total excess error is bounded by 2 times
the sum of the bias and variance term, which gives us the same result.
Putting this (Equation 32) back into Equation 30, we get that with probability at least 1 :
Rid(^fin)2(1+cd=n) (33)
Since2
u>0, we have2<2
u+2. Then for some Nand for allnN, we have:
Rid(^fin)<2
u+2Rid(^fbs) (34)
In particular, we can choose N=max(N0;c2
2ud+1). Which completes the proof.
E.3 Auxiliary inputs can hurt out-of-distribution
Restatement of Example 1. There exists a problem setting P,P, such that for every n, there is some
test distribution P0
x;P0
uwith:
E[Rood(^fin)]>E[Rood(^fbs)] (35)
13Proof. We will have x2R(sod= 1),w=x, andu;z2R2. We setz1=u1+wandz2=u2, in
other words we choose A?= [1;0]andC?=I2is the identity matrix. We set y=x+u1+, with
N(0;2), soyis a function of xandu1but notu2. In other words we choose w=1andu=(1;0).
Pxwill be Uniform [ 1;1], andPuwill be uniform in the unit ball in R2.
LetXZ= [X;Z], which denotes appending XandZby columns so XZ2Rn3withn3. Since
PxandPuhave density, XZhas rank 3 almost surely. This means that XZ>XZis invertible (and
positive semi-deÔ¨Ånite) almost surely. Since PxandPuare bounded, the maximum eigenvalue 0
max
ofXZ>XZis bounded above. The minimum eigenvalue minof(XZ>XZ) 1is precisely 1=0
maxand is therefore positive and bounded below by some c>0almost surely.
We will deÔ¨Åne P0
xandP0
usoon. For now, consider a new test example x0P0
x;u0P0
uwith
z0=[x0;0]+u0andy0=x0+u0
1+0with0N(0;2)andE[u0]=0. For the input model we have:
E[Rood(^fin)]=E[(y0 (^>
x;inx0+^>
z;inz0))2] (36)
=2(1+E[(x0;z0)>(XZ>XZ) 1(x0;z0)]) (37)
2(1+E[mink(x0;z0)k2
2]) (38)
2(1+cE[k(x0;z0)k2
2]) (39)
2(1+cE[z0
22]) (40)
=2(1+cE[u0
22]) (41)
Notice that this lower bound is a function of E[u0
22]which we will make very large.
On the other hand, letting 0
u2=Eu0P0u[(>
uu0)2]=Eu0P0u[u0
12], for the baseline model we have:
E[Rood(^fbs)]=E[(y0 ^>
x;olsx0)2] (42)
=(2+0
u2)+(2+u2)E[x0>(X>X) 1x0]) (43)
So the risk depends on x0andE[u0
12]but not E[u0
22].
So we choose P0
x=Uniform ( 1;1). ForP0
u, we sample the components independently, with
u0
1Uniform ( 1;1), andu0
2Uniform ( R;R). By choosing Rlarge enough, we can make the
lower bound for the input model arbitrarily large without impacting the risk of the baseline model
which gives us:
E[Rood(^fin)]>E[Rood(^fbs)] (44)
E.4 Pre-training improves risk under arbitrary covariate shift
Restatement of Theorem 3. For all problem settings P,P, and for all test distributions P0
xandP0
u:
E[Rood(^fout)]E[Rood(^fbs)] (45)
First we show that pre-training (training a low-rank linear map from xtoz) recovers the unobserved
featuresw. We will then show that learning a regression map from wtoyis better in all directions
than learning a regression map from xtoy.
Our Ô¨Årst lemma shows that we can recover the map from xtowup to identiÔ¨Åability (we will learn
the rowspace of the true linear map from xtow).
Lemma 1. For a pair (x;z), letz=A?B?x+whereA?2RTkandB?2Rkdare the
true parameters with T;dkand2RTis mean-zero noise with bounded variance in each
coordinate. Assume that A?;B?are both rank k. Suppose that E[xx>]is invertible. Let ^A;^Bbe
minimizers of the population risk E[k^A^Bx zk2]of the multiple-output regression problem. Then
spanfB?1;:::;B?kg=spanf^B1;:::;^BkgwhereB?i;^Biare thei-th rows of their respective matrices.
14Proof. We Ô¨Årst consider solving for the product of the weights ^A^B. The population risk can be
decomposed into the risks of the Tcoordinates of the output:
L(C)=E[kCx zk2] (46)
=E[kCx A?B?x k2] (47)
=E[k(C A?B?)xk2]+E[kk2] (48)
=TX
i=1E[((Ci (A?B?)i)x)2]+E[2] (49)
=TX
i=1Li(C) (50)
where E[kk2]is the error of the Bayes optimal estimator, Ci;(A?B?)idenote thei-th row of
the respective matrices, and the i-th risk isLi(C):=E[((Ci (A?B?)i)x)2] +E[2]. From this
decomposition, we see that for every i,
(A?B?)i=argmin
CiLi(Ci) =)A?B?=argmin
CL(C) (51)
such thatA?B?is the unique minimizer of the population risk for the multiple-output regression
problem. The minimizer is unique since the minimizer for each subproblem is unique because E[xx>]
is invertible. Since the variance of is bounded in each coordinate, with inÔ¨Ånite data we achieve the
same minimizer of the multiple-output regression problem. Thus we have
^A^B=argmin
CL(C)=A?B?(52)
so that the product of the learned parameters and the true parameters are equal.
From this, we see that ^A;^Bmust be rank k, or else ^A^B6=A?B?. Since ^Ais rankk, it has a left inverse
^A 1
leftsuch that
^B=QB?(53)
whereQ=^A 1
leftA?2Rkkand^A 1
left=(^A>^A) 1^A>. Since ^A 1
leftis rankkandA?is rankk, we have
that the rank of the product is rank(Q)k. In the other direction, by Sylvester‚Äôs rank inequality,
rank(Q)rank(^A 1
left)+rank(A?) k=k, which implies that Qis full rank (rank k). This implies
the result.
Before proving the theorem, it will be useful to state a version of the Woodbury inversion lemma.
Lemma 2 (Woodbury) .For invertible A2Rdd, invertibleC2Rkk, andD2Rdk,
(A+DCD>) 1=A 1 A 1D(C 1+D>A 1D) 1D>A 1: (54)
Our next lemma shows that for any Ô¨Åxed training examples Xandarbitrary test example x0, the
aux-outputs model will have better expected risk than the baseline where the expectation is taken over
the training labels YjX.
Lemma 3. In the linear setting, Ô¨Åx data matrix Xand consider arbitrary test example x0. Let?=
B?>wbe the optimal (ground truth) linear map from xtoy. The expected excess risk of the aux-outputs
model ^B>^w;out is better than for the baseline ^x;ols, where the expectation is taken over the training
targetsYPYjX(Yshows up implicitly because the estimators ^w;out and^x;ols depend onY):
E[(^>
w;out^Bx0 ?>x0)2]E[(^>
x;olsx0 ?>x0)2] (55)
Proof. Letall=Y X?be the training noise. From standard calculations, the instance-wise risk
of^x;ols for anyxis
E[(^>
x;olsx0 ?>x0)2]=E[(((X>X) 1X>Y)>x0 ?>x0)2] (56)
=E[((?+(X>X) 1X>all)>x0 ?>x0)2] (57)
=E[(((X>X) 1X>all)>x0)2] (58)
=(2+2
u)x0>(X>X) 1x0(59)
By Lemma 1, ^B=QBfor some full rank Q. Thus, learning ^w;out is a regression problem with indepen-
dent mean-zero noise and we can apply the same calculations for the instance-wise risk of ^B>^w;out .
E[(^>
w;out^Bx0 ?>x0)2]=(2+2
u)x0>^B>(^BX>X^B>) 1^Bx0: (60)
15We show that the difference between the inner matrices is positive semi-deÔ¨Ånite, which implies the
result. In particular, we show that
(X>X) 1 ^B>(^BX>X^B>) 1^B<0: (61)
Multiplying the Woodbury inversion lemma on the left and right by A, we have the identity
A(A+DCD>) 1A=A D(C 1+D>A 1D) 1D>: (62)
LettingA=(X>X) 1,C=I, andD=B>, we have
(X>X) 1 ^B>(^BX>X^B>) 1^B=A D(D>A 1D) 1D>(63)
= lim
!1A D(1
I+D>A 1D) 1D>(64)
= lim
!1A(A+DD>) 1A: (65)
where the last step applies Equation (62). The Ô¨Årst limit converges by continuity of the inverse, and
the second limit converges since the elements of the sequence are identical to the Ô¨Årst. For any >0,
A+DD>is a sum of PSD matrices and is thus PSD. Since Ais symmetric and PD, each element
of the limit sequence is PSD. Since the space of PSD matrices is closed, we have that the original
difference of matrices is PSD. This implies the result.
Proof of Theorem 3. Fix training examples Xand test example x0but let the train labels YPYjX
and and test label y0P0
y0jx0be random. In particular, let 0
u2=E[(>
uu0)2]whereu0P0
u, with
E[u0]=0. Then for the baseline OLS estimator, we have:
E[(y0 ^>
x;olsx0)2]=0
u2+2+E[(^>
x;olsx0 ?>x0)2] (66)
For the aux-outputs model, we have:
E[(y0 ^>
w;out^Bx0)2]=0
u2+2+E[(^>
w;out^Bx0 ?>x0)2] (67)
So applying Lemma 3, we get that the risk for the aux-outputs model is better than for the baseline
(the lemma showed it for the excess risk):
E[(y0 ^>
w;out^Bx0)2]E[(y0 ^>
x;olsx0)2] (68)
Since this is true for all Xandx0, it holds when we take the expectation over the training examples
XfromPxand the test example x0fromP0
xwhich gives us the desired result.
E.5 In-N-Out improves risk under arbitrary covariate shift
Restatement of Theorem 4. In the linear setting, for all problem settings Pwith2
u>0, test
distributions P0
x;P0
u, and>0, there exists a;b> 0such that for all P, with probability at least 1 
over the training examples and test example x0P0
x, the ratio of the excess risks (for all 2small
enough that a b2>0) is:
Rood
in-out R
Roodout R2
a b2(69)
HereR= minfRood(f)is the minimum possible (Bayes-optimal) OOD risk,
Rood
in-out =Ey0P0
y0jx0[`(^g(^hout(x0));y0)]is the risk of the In-N-Out model on test example x0,
andRood
out=Ey0P0
y0jx0[`(^gy
out(^hout(x0));y0)]is the risk of the aux-outputs model on test example x0.
We Ô¨Årst show a key lemma that lets us bound the min singular values of a random matrix, which will let
us upper bound the risk of the In-N-Out estimator and lower bound the risk of the pre-training estimator.
DeÔ¨Ånition 1. As usual, the min singular value min(W)of a rectangular matrix W2Rnkwhere
nkrefers to the k-th largest singular value (the remaining n ksingular values are all 0), or in
other words:
min(W)= min
kk2=1kWk2 (70)
Lemma 4. LetPwandPube independent distributions on RkandRmrespectively. Suppose they are
absolutely continuous with respect to the standard Lebesgue measure on RkandRmrespectively (e.g.
this is true if they have density everywhere with density upper bounded). Let W2Rnkwhere each row
Wiis sampled independently WiPw. LetU2Rnmwhere each row Uiis sampled independently
UiPu. Supposenk+m. For all, there exists c()>0such that with probability at least 1 , the
minimum singular values minare lower bounded by c():min(W)>c()andmin([W;U])>c().
16Proof. We note that the matrices WandUare rectangular, e.g. W2Rnkwherenk. We will prove
the lemma for WÔ¨Årst, and the extension to [W;U]will follow.
Note that removing the last n krows ofWcannot increase its min singular value since that
corresponds to projecting the vector W and projection never increases the Euclidean norm. So
WLOG we suppose Wonly consists of its Ô¨Årst krows and soW2Rkk.
Now, consider any row Wi. We will use a volume argument to show that with probability at least
1 
d, this rowWihas a non-trivial component perpendicular to all the other rows. Since all rows are
independently and identically sampled, without loss of generality suppose i=1. Fix the other rows
w2;:::;wk, sincew1is independent of these other rows, the conditional distribution of w1is the same
as the marginal of w1.w2;:::;wkform ak 1dimensional subspace SinRk. Lettingd(w;S)denote
the Euclidean distance of a vector wfrom the subspace S, deÔ¨Åne the event S=fw1:d(w1;S)g.
SincePwis absolutely continuous, P(S)!0as!0, so for some small c()>0,P(Sc())<
d.
So with probability at least 1 =d,d(w1;S)>c().
By union bound, with probability at least 1 , this is true for every row wi‚Äîcondition on the event
that this is true. By representing each row vector as the sum of the component perpendicular to the
subspace of the other vectors, and a component along the subspace, and applying Pythagoras theorem
and expanding we get:
min
kk2=1kWk= min
kk2=1k>Wkc() (71)
Which completes the proof for min(W).
For[W;U], we note that PxandPuare independent, and the product measure is absolutely continuous.
Since each row of [W;U]is identically and independently sampled just like with W, we can apply
the exact same argument as above (though for a different constant c(), we take the min of these two
as ourc()in the lemma statement).
Recall that the In-n-Out estimator was obtained by Ô¨Åtting a model from w;ztoy, and then using that
to produce pseudolabels on (inÔ¨Ånite) unlabeled data, and then self-training a model from wtoyon
these pseudolabels. For the linear setting, we deÔ¨Åned the In-N-Out estimator ^win Equation 17. Our
next Lemma gives an alternate closed form of the In-N-Out estimator in terms of the representation
matrixW=X^Band the latent matrix U.
Lemma 5. In the linear setting, letting W=X^B>we can write the In-n-Out estimator in closed form
as:
^w=[Ikk;0kT]
W>
U>
(W;U) 1
W>
U>
Y (72)
Proof. We recall the deÔ¨Ånition of the In-N-Out estimator, where we Ô¨Årst train a classiÔ¨Åer from W;Z
toY.
^w;^z=argmin
^w;^zk(Y (W^w+Z^z))2k (73)
Denote the minimum value of Equation 73 by p. Note that ^w;^zmay not be unique, and we pick any
solution to the argmin (although our proof will reveal that the resulting ^wis in fact unique). We then
use this to produce pseudolabels and self-train, on inÔ¨Ånite data, which gives us the In-N-Out estimator:
^w= ^w+A>^z (74)
We will now consider the following alternative estimator:
^0
w;^0u=argmin
^0w;^0uk(Y (W^0
w+U^0u))2k (75)
Denote the minimum value of Equation 75 by q. We claim that ^0
w=^w.
We will show that the In-N-Out estimator ^wminimizes the alternative minimization problem in
Equation 75 by showing that p=q. We will then show that the solution to Equation 75 is unique,
which implies that ^0
w=^w.
We note that C?>2RmTwhereTmis full-rank, so there exists a right-inverse C0withC?>C0=
Imm. SinceZ=WA?>+UC?>, this gives us: U=(Z WA?>)C0=ZC0+W( A?>C0).
17So this means that a solution to the alternative problem in Equation 75 can be converted into a solution
for the original in Equation 73 with the same function value:
min
^0w;^0uk(Y (W^0
w+U^0u))2k (76)
= min
^0w;^0uk(Y (W^0
w+(ZC0+W( A?>C0))^0u))2k (77)
= min
^0w;^0uk(Y (W(^0
w A?>C0^0u)+Z(C0^0u)))2k (78)
This implies that pq.
We now show that a solution to the original problem in Equation 73 can be converted into a solution
for the alternative in Equation 75 with the same function value:
min
^w;^zk(Y (W^w+Z^z))2k (79)
= min
^w;^zk(Y (W^w+(WA?>+UC?>) ^z))2k (80)
= min
^w;^zk(Y (W( ^w+A?>^z)+U(C?>^z)))2k (81)
This implies that qp, and we showed before that pqsop=q. But since ^w;^zminimizes
the original minimizer in Equation 73, ^w+A?>^z;C?>^zminimize the alternative problem in
Equation 75, where ^w= ^w+A?>^z.
Since [W;U]is full rank, the solution ^0
w;^0uto the alternative estimator Equation 75 is unique. So
this means that ^0
w=^w.
We have shown that ^0
w=^w‚Äîthis completes the proof because solving Equation 75 for ^0
wgives
us the closed form in Equation 72.
Next we show a technical lemma that says that if a random vector u2Rnhas bounded density every-
where, then for any vwith high probability the dot product (u>v)2cannot be too small relative to kvk2
2.
Lemma 6. Suppose a random vector u2Rnhas density everywhere, with bounded density. For every
, there exists some c()such that for all v, with probability at least 1 overu,(u>v)2c()kvk2
2.
Proof. First, we choose some B0such thatP(kuk2B0)=2, such aB0exists for every probability
measure.
Suppose that the density is upper bounded by B1. Let the area of the n 1dimensional sphere with
radiusB0beA0. Consider any n 1dimensional subspace S, and letS=fu0:d(u0;S)gwhere
d(u0;S)denotes the Euclidean distance from u0toS.P(u2S)A0B1+=2for allS. By choosing
sufÔ¨Åciently small >0, we can ensure that P(u2S)for allS.
Now consider arbitrary vand letS(v)be then 1-dimensional subspace perpendicular to v. We
haveP(u2S(v)). But this means that (u>v)22kvk2
2with probability at least 1 , which
completes the proof.
By deÔ¨Ånition of our linear multi-task model, we recall that y=>
ww+>
uu+, wherew=B?x. We
do not have access to B?, but we assumed that B?is full rank. We learned ^Bwhich has the same
rowspace asB?(Lemma 1). This means that for some 0
w, we havey=0>
w^w+>
uu+where ^w=^Bx.
To simplify notation and avoid using 0
wand^weverywhere, we suppose WLOG that ^B=B?(but
formally, we can just replace all the occurrences of wby0
wandwby^w).
Our next lemma lower bounds the test error of the pre-training model.
Lemma 7. In the linear setting, for all problem settings Pwith2
u>0, for all, there exists some
a;b> 0such that with probability at least 1 over the training examples and test example x0P0
xthe risk of the aux-outputs model is lower bounded:
Rood
out R>a b2(82)
18Proof. Recall thatRood
out=Ey0P0
y0jx0[l(^fout(^hout(x0));y0)]. Let0
u2=Eu0P0u[(>
uu0)2]. We have
R=2+0
u2. LetW=XB?>be the feature matrix, where W2Rnk.
Lettingw0=B?x0, for the aux-outputs model, we have:
E
y0P0
y0jx0[l(^fout(^hout(x0));y0)] (83)
=E
y0Py0jx0[(y0 ^>
w;outw0)2] (84)
=(2+0
u2)+(>
ww0 ^>
w;outw0)2(85)
=R+(>
ww0 ^>
w;outw0)2(86)
Let=Y (Ww+Uu)be the noise of Yfor the training examples, which is a random vector with
2Rn. We can now write:
(>
ww0 ^>
w;outw0)2=((+Uu)>W(W>W) 1w0)2(87)
By assumption, W>Wis invertible (almost surely). With probability at least 1 =10all entries of
W>Ware upper bounded and we condition on this. So (W>W) 1has min singular value bounded be-
low. By Lemma 4, Whas min singular value that is bounded below with probability at least 1 =10, we
condition on this being true. So let =W(W>W) 1w0, so for some c0>0, we have:kk2c0kw0k2.
In terms of, we can write Equation 87 as:
(>
ww0 ^>
w;outw0)2=((+Uu)>)2(88)
=(>)2+((Uu)>)2+2(>)((Uu)>) (89)
((Uu)>)2+2(>)((Uu)>) (90)
((Uu)>)2 2j>jk(Uu)>k2kk2) (91)
We can Ô¨Ånd b0such that with at least probability 1 =10,k(Uu)>k2b0, condition on this. We
note that>has variance 2kk2so by Chebyshev for some b1with probability at least 1 ,
j>jb12kk2, condition on this. So we can now bound Equation 91 and get:
(>
ww0 ^>
w;outw0)2((Uu)>)2 2b0b12kk2
2 (92)
Now we apply Lemma 6, where we use the fact that 2
u>0. So given=10, there exists some c1such
that for every with probability at least 1 =10,((Uu)>)2c1kk2
2, giving us:
(>
ww0 ^>
w;outw0)2(c1 2b0b12)kk2
2 (93)
Sincew0has bounded density everywhere, it is non-atomic and we get that there is some c2>0such
that with probability at least 1 =10,kw0k2
2c2
2. But thenkk2
2c0c2, which gives us for some a;b:
(>
ww0 ^>
w;outw0)2(c1 2b0b12)c0c2a b2(94)
Combining this with Equation 86, we get with probability at least 1 :
E
y0P0
y0jx0[l(^fout(^hout(x0));y0)] R>a b2(95)
Which was what we wanted.
Lemma 8. In the linear setting, for all problem settings P, for all, there exists some c>0such
that with probability at least 1 over the training examples and test example x0P0
xthe risk of the
In-N-Out model is upper bounded:
Rood
in-out R<c2(96)
Proof. Recall thatRood
in-out=Ey0P0
y0jx0[l(^fout(^hout(x0));y0)]. Let0
u2=Eu0P0u[(>
uu0)2]. We have
R=2+0
u2. As before, let W=XB?>be the feature matrix, where W2Rnk.
LetWU=[W;U]which denotes concatenating the matrices by column, so that WU2Rn(k+m). By
Lemma 4,WU>WUhas min singular value that is bounded below by c0with probability at least 1 
=10, we condition on this being true. Now, as for the aux-outputs model, letting w0=B?x0, we have:
E
y0Py0jw0[(y0 ^>
ww0)2]=R+(>
ww0 ^>
ww0)2(97)
19For the second term on the RHS: Let R= [Ikk;0km]. Let=Y (Ww+Uu)be the noise of
Yfor the training examples, which is a random vector with 2Rn. We can now write:
(>
ww0 ^>
ww0)2=(w0>R(WU>WU) 1WU>)2(98)
kw0k2is bounded above by some constant B1with probability at least 1 =10which we condition
on. Now taking the expectation over w0and, using the fact that Rpreserves the norm of a vector
we can write:
E
w0;[(w0>R(WU>WU) 1WU>)2] (99)
=2E
w0;[(w0>R[WU>WU] 1RTw0)] (100)
2
c2
0E
w0[kw0k2
2] (101)
B2
12
c2
0(102)
Then, by Markov‚Äôs inequality, with probability at least 1 =10we can upper bound this by10B2
12
c2
0.
In total, that gives us that for some c, with probability at least 1 :
E
y0P0
y0jx0[l(^fout(^hout(x0));y0)] R<c2(103)
The proof of Theorem 4 simply combines Lemma 7 and Lemma 8.
Proof of Theorem 4. For somea;b;c , with probability at least 1 , we have for the aux-outputs model:
Rood
out R>a b2(104)
And for the In-N-Out model:
Rood
in-out R<c2(105)
Taking ratios and dividing by suitable constants we get the desired result.
F Experimental details
Data splits. In all experiments we Ô¨Årst split off the OOD data, then randomly split the rest into
training, validation, and in-distribution test. We use a portion of the training and OOD set as
in-distribution and OOD unlabeled data respectively. The rest of the OOD set is held-out. We run
5 trials, where we regenerate the training/unlabeled split for each trial (keeping held-out splits Ô¨Åxed).
We use a reduced number of labeled examples from each dataset (1%, 5%, 10% of labeled examples
for CelebA, Cropland, and Landcover respectively), with the rest as unlabeled.
F.1 Cropland
All models reported in Table 1 were trained using the Adam optimizer with learning rate 0:001, a
batch size of 256, and 100 epochs unless otherwise speciÔ¨Åed. Our dataset consists of about 7k labeled
examples, 170k unlabeled examples (with 130k in-distribution examples), 7.5k examples each for
validation and in-distribution test, and 4260 OOD test examples (the speciÔ¨Åcation of OOD points is
described in further detail below). Results are reported over 5 trials, and 2f0:5;0:6;0:7;0:8;0:9g
was chosen using the validation set.
Problem Motivation. Developing machine learning models trained on remote sensing data is
currently a popular line of work for practical problems such as typhoon rainfall estimation, monitoring
reservoir water quality, and soil moisture estimation [ 29,32,1]. Models that could use remote sensing
data to accurately forecast crop yields or estimate the density of regions dedicated to growing crops
would be invaluable in important tasks like estimating a developing nation‚Äôs food security [30].
OOD Split. In remote sensing problems it is often the case that certain regions lack labeled data
(e.g., due to a lack of human power to gather the labels on site), so extrapolation to these unlabeled
regions is necessary. To simulate this data regime, we use the provided (lat, lon) pairs of each data
point to split the dataset into labeled (in-distribution) and unlabeled (out-of-distribution) portions.
SpeciÔ¨Åcally, we take all points lying in Iowa, Missouri, and Illinois as our ID points and use all points
within Indiana and Kentucky as our OOD set.
20Shape of auxiliary info. To account for the discrepancy in shapes of the two sources of auxiliary
information (latitude and longitude are two scalar measurements while the 3 vegetation bands form
a35050tensor), we create latitude and longitude ‚Äúbands‚Äù consisting of two 5050matrices that
repeat the latitude and longitude measurement, respectively. Concatenating the vegetation bands and
these two pseudo-bands together gives us an overall auxiliary dimension of 55050.
UNet. Since our auxiliary information takes the form of 5050bands, we need a model architecture
that can reconstruct these bands in order to implement the aux-outputs and the In-N-Out models. With
this in mind, we utilize a similar UNet architecture that Wang et al. [49] use on the same Cropland
dataset. While the UNet was originally proposed by Ronneberger et al. [39]for image segmentation,
it can be easily modiÔ¨Åed to perform image-to-image translation. In particular, we remove the Ô¨Ånal 11
convolutional layer and sigmoid activation that was intended for binary segmentation and replace them
with a single convolutional layer whose output dimension matches that of the auxiliary information.
In our case, the last convolutional layer has an output dimension of 5 to reconstruct the 3 vegetation
bands and (lat,lon) coordinates.
To perform image-level binary classiÔ¨Åcation with the UNet, we also replace the Ô¨Ånal 11convolutional
layer and sigmoid activation, this time with a global average pool and a single linear layer with an
output dimension of 1. During training we apply a sigmoid activation to this linear layer‚Äôs output to
produce a binary class probability, which is then fed into the binary cross entropy loss function.
Aux-inputs model. Since the original RGB input image is 35050, we can simply concatenate
the auxiliary info alongside the original image to produce an input of dimensions 85050to feed
into the UNet.
Aux-outputs model. The modiÔ¨Åcation of the traditional UNet architecture in order to support aux-
iliary outputs for Cropland is described in the above UNet section. We additional add a tanh activation
function to squeeze the model‚Äôs output values to the range [ 1;1](the same range as the images). We
train the model to learn the auxiliary bands via pixel-wise regression using the mean squared error loss.
In-N-Out model. We found that the Ô¨Ånetuning phase of the In-N-Out algorithm experienced wild
Ô¨Çuctuations in loss and would not converge when using the hyperparameters listed at the top of this
section. To encourage the model to converge and Ô¨Åt the training set, we decreased the Adam learning
rate to 0.0001 and doubled the batch size to 512.
Repeated self-training. For the additional round of self-training, we initialize training and
pseudolabel all unlabeled data with the In-N-Out model. Following [ 27], we employ additional
regularization when doing self training by adding dropout with probability 0.8.
F.2 Landcover
Our Landcover dataset comes from NASA‚Äôs MODIS Surface ReÔ¨Çectance product, which is made
up of measurements from around the globe taken by the Terra satellite [ 48]. In each trial, we use about
16k labeled examples from non-African locations, 203k unlabeled examples (with 150k in-distribution
examples), 9266 examples each for validation and in-distribution test, and 4552 OOD test examples.
We trained with SGD + momentum (0.9) on all models for 400 epochs with a cosine learning rate
schedule. We used learning rate 0.1 for all models that were not pre-trained, and learning rate 0.01 for
models that were already pre-trained. Results are reported over 5 trials, and 2f0:1;0:3;0:5;0:7;0:9g
was chosen using the validation set.
1D CNN While Convolutional Neural Networks are most commonly associated with the ground-
breaking success of 2D-CNNs on image-based tasks, the 1-dimensional counterparts have also found
success in various applications [ 25]. Because the measurements from the MODIS satellite are not
images but instead scalar-valued time series data, we can use a 1D CNN with 7 channels, one for each
of the 7 MODIS sensors.
NDVI The normalized difference vegetation index (NDVI) is a remote sensing measurement
indicating the presense of live green vegetation. It has been shown to be a useful predictor in
landcover-related tasks [ 11,10,31], so we choose to include it in our models as well. NDVI can be
computed from the RED and NIR bands of the MODIS sensors via the equation
NDVI =(NIR RED)=(NIR+RED): (106)
We include NDVI along with the 7 other MODIS bands to give us input dimensions of 468.
21ERA5 It is a reasonable hypothesis that having additional climate variables such as soil type or
precipitation could be useful for a model in inferring the underlying landcover class. To this end we
incorporate features from the ERA5 climate dataset as our auxiliary information [ 4]. The speciÔ¨Åc
variables we include are soil type, temperature, precipitation rate, precipitation total, solar radiation,
and cloud cover. For each MODIS point we Ô¨Ånd its nearest ERA5 neighbor based on their latitude
and longitude in order to pair the datasets together.
The ERA5 measurements are monthly averages, which means the readings are at a different
frequency than that of the 8-day MODIS time series. We upsample the ERA5 signal using the
scipy.signal.resample method, which uses the FFT to convert to the frequency domain, adds
extra zeros for upsampling to the desired frequency, and then transforms back into the time domain.
Landcover classes. The Landcover dataset has a total of 16 landcover classes, with a large variance
in the individual class counts. To ensure our model sees enough examples of each class, we Ô¨Åltered
the dataset to include just 6 of the most populous classes: savannas , woody_savannas , croplands ,
open_shrublands , evergreen_broadleaf_forests , and grasslands .
Aux-inputs model. We concatenate the resampled ERA5 readings with the MODIS and NDVI
measurements to obtain an input dimension of 4614.
Aux-outputs model. Rather than predicting the entire ERA5 time series as an auxiliary output,
we instead average the 6 climate variables over the time dimension and predict those 6 means as the
auxiliary outputs. We use a smaller learning rate of 0.01 for this pre-trainined model.
In-N-Out and Repeated self-training. The In-N-Out model initializes its weights from the
aux-outputs model and gets pseudolabeled ID unlabeled data from the aux-inputs model. As with
aux-outputs, we use a smaller learning rate of 0.01 for this pre-training model.
For the additional round of self-training, we initialize training and pseudolabel all unlabeled data with
the In-N-Out model. Following [ 27], we employ additional regularization when doing self training
by adding dropout with probability 0.5. We found that with dropout, we need a higher learning rate
(0.1) to effectively Ô¨Åt the training set.
F.3 CelebA
For the results in Table 1, we used 7 auxiliary binary attributes included in the
CelebA dataset: ['Bald', 'Bangs', 'Mustache', 'Smiling', '5_o_Clock_Shadow',
'Oval_Face', 'Heavy_Makeup'] . These attributes tend to be fairly robust to our distribution shift
(not hat vs. hat) ‚Äî if the person has a 5 o‚Äôclock shadow, the person is likely a man. We use a subset
of the CelebA dataset with 2000 labeled examples, 30k in-distribution unlabeled examples, 3000
OOD unlabeled examples, and 1000 validation, in-distribution test, and OOD test examples each.
The backbone for all models is a ResNet-18 [ 16] which takes a CelebA image downsized to 6464
and outputs a binary gender prediction. All models are trained for 25 epochs using SGD with cosine
learning rate decay, initial learning rate 0.1, and early stopped with an in-distribution validation set.
The gender ratios in the in-distribution and OOD set are balanced to 50-50.
Aux-inputs model. We incorporate the auxiliary inputs by Ô¨Årst training a baseline model ^fbs
from images to output logit, then training a logistic regression model on the concatenated features
[^fbs(x);z]wherezare the auxiliary inputs. We sweep over L2 regularization hyperparameters C=
[0:1;0:5;1:0;5:0;10:0;20:0;50:0]and choose the best with respect to an in-distribution validation set.
Aux-outputs model. During pretraining, the model trains on the 7-way binary classiÔ¨Åcation task
of predicting the auxiliary information. Then, the model is Ô¨Ånetuned on the gender classiÔ¨Åcation task
without auxiliary information.
In-N-Out and repeated self-training. For In-N-Out models with repeated self-training, we
pseudolabeled all the unlabeled data using the In-N-Out model and did one round of additional
self-training. Following [ 27], we employ additional regularization when doing self training by adding
dropout with probability 0.8. We also reduced the learning rate to 0.05 to improve the training dynamics.
Adding auxiliary inputs one-by-one. In Figure 5, we generate a random sequence of 15
auxiliary inputs and add them one-by-one to the model, retraining with every new conÔ¨Åguration.
We use the following auxiliary information: 'Young' , 'Straight_Hair' , 'Narrow_Eyes' ,
'Mouth_Slightly_Open' , 'Blond_Hair' , '5_o_Clock_Shadow' , 'Big_Nose' , 'Oval_Face' ,
'Chubby' , 'Attractive' , 'Blurry' , 'Goatee' , 'Heavy_Makeup' , 'Wearing_Necklace' , and
'Bushy_Eyebrows' .
2290 91 92 93 94
In-distribution accuracy74767880OOD accuracyAux-input
BaselineFigure 4: Correlation ( r2= 0:52) between in-distribution accuracy and out-of-distribution (OOD)
accuracy when adding 1 to 15 random auxiliary features as input in CelebA.
90.0 90.5 91.0 91.5 92.0 92.5
In-distribution accuracy7475767778OOD accuracy
Figure 5: In-distribution vs. OOD accuracy when
sequentially adding a random set of 15 auxiliary in-
puts one-by-one. While more auxiliary inputs gen-
erally improves both in-distribution and OOD ac-
curacy, some in-distribution gains can hurt OOD.ID Test Acc OOD Test Acc
Only in-distribution 69.73 0.51 57.731.58
Only OOD 69.92 0.41 59.281.01
Both 70.07 0.46 59.840.98
Table 2: Ablation study on the use of in-
distribution vs. OOD unlabeled data in
pre-training models on Landcover, where unla-
beled sample size is standardized (much smaller
than Table 1). Using OOD unlabeled examples are
important for gains in OOD accuracy (%). Results
are shown with 90% error intervals over 5 trials.
Correlation between in-distribution and OOD accuracy. In Figure 4, we sample 100 random
sets of auxiliary inputs of sizes 1 to 15 and train 100 different aux-inputs models using these auxiliary
inputs. We plot the in-distribution and OOD accuracy for each model, showing that there is a signiÔ¨Åcant
correlation between in-distribution and OOD accuracy in CelebA, supporting results on standard
datasets [37, 53, 41]. Each point in the plot is an averaged result over 5 trials.
G Additional Experiments
G.1 Choice of auxiliary inputs matters
We Ô¨Ånd that the choice of auxiliary inputs affects the tradeoff between ID and OOD performance
signiÔ¨Åcantly, and thus is important to consider for problems with distribution shift. While Figure 4
shows that auxiliary inputs tend to simultaneously improve ID and OOD accuracy in CelebA, our theory
suggests that in the worst case, there should be auxiliary inputs that worsen OOD accuracy. Indeed,
Figure 5 shows that when taking a random set of 15 auxiliary inputs and adding them sequentially as
auxiliary inputs, there are instances where an extra auxiliary input improves in-distribution but hurts
OOD accuracy. In cropland prediction, we compare using location coordinates and vegetation data as
auxiliary inputs with only using vegetation data. The model with locations achieves the best ID perfor-
mance, improving almost 1% in-distribution over the baseline with only RGB. Without locations, the
ID accuracy is similar to the baseline but the OOD accuracy improves by 1.5%. In this problem, location
coordinates help with in-distribution interpolation, but the model fails to extrapolate on locations.
G.2 OOD unlabeled data is important for pretraining
We compare the role of in-distribution vs. OOD unlabeled data in pretraining. Table 2 shows the results
of using only in-distribution vs. only OOD vs. a balanced mix of unlabeled examples for pretraining
on the Landcover dataset, where unlabeled sample size is standardized across the models (by reducing
to the size of the smallest set, resulting in 4x less unlabeled data). Using only in-distribution unlabeled
examples does not improve OOD accuracy, while having only OOD unlabeled examples does well
23both in-distribution and OOD since it also has access to the labeled in-distribution data. For the same
experiment in cropland prediction, the differences were not statistically signiÔ¨Åcant, perhaps due to
the smaller geographic shift (across states in cropland vs. continents in landcover).
24