AQ-PINNs: Attention-Enhanced Quantum
Physics-Informed Neural Networks for Carbon-Efficient
Climate Modeling
Siddhant Dutta1, Nouhaila Innan2,3, Sadok Ben Yahia4, Muhammad Shafique2,3
1SVKM’s Dwarkadas J. Sanghvi College of Engineering, India
2eBRAIN Lab, Division of Engineering,
New York University Abu Dhabi (NYUAD), Abu Dhabi, UAE
3Center for Quantum and Topological Systems (CQTS),
NYUAD Research Institute, NYUAD, Abu Dhabi, UAE
4The Maersk Mc-Kinney Moller Institute,
University of Southern Denmark, Alsion 2, 6400- Sønderborg, Denmark
siddhant.dutta180@svkmmumbai.onmicrosoft.com, nouhaila.innan@nyu.edu,
say@mmmi.sdu.dk, muhammad.shafique@nyu.edu
Abstract
The growing computational demands of artificial intelligence (AI) in addressing
climate change raise significant concerns about inefficiencies and environmental
impact, as highlighted by the Jevons paradox. We propose an attention-enhanced
quantum physics-informed neural networks model (AQ-PINNs) to tackle these
challenges. This approach integrates quantum computing techniques into physics-
informed neural networks (PINNs) for climate modeling, aiming to enhance pre-
dictive accuracy in fluid dynamics governed by the Navier-Stokes equations while
reducing the computational burden and carbon footprint. By harnessing variational
quantum multi-head self-attention mechanisms, our AQ-PINNs achieve a 51.51%
reduction in model parameters compared to classical multi-head self-attention
methods while maintaining comparable convergence and loss. It also employs
quantum tensor networks to enhance representational capacity, which can lead to
more efficient gradient computations and reduced susceptibility to barren plateaus.
Our AQ-PINNs represent a crucial step towards more sustainable and effective
climate modeling solutions.
1 Introduction
The rapid growth of artificial intelligence (AI) has led to a significant increase in energy consumption
and carbon emissions, primarily due to the escalating computational requirements of increasingly
complex and accurate models. This trend, often called the AI arms race , has seen energy consumption
doubling approximately every 3.4 months since 2012 [1]. For instance, the training of GPT-3, with
its 175 billion parameters, has been estimated to emit over an amount of electricity equivalent to
500 metric tons of CO 2, a figure comparable to the annual emissions of approximately 60 passenger
vehicles [2]. In response to this unsustainable trajectory, the research community has begun focusing
onGreen AI methodologies and tools like CodeCarbon [3], Carbontracker [4], and eco2AI to quantify
and mitigate the environmental impact of AI models throughout their lifecycle [5]. However, the
Jevons paradox presents a crucial challenge: improvements in energy efficiency [6], such as those
offered by Green AI, can paradoxically lead to an overall increase in energy consumption. This is
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.because greater efficiency often leads to broader adoption and use of AI technologies, potentially
exacerbating the very environmental impact they aim to mitigate.
Quantum machine learning (QML) benefits from quantum tensor networks by significantly reducing
the number of model parameters and leveraging the mathematical essence of Hilbert space to achieve
more compact representations [7 –9]. Quantum tensor networks, such as matrix product states (QMPS),
tree tensor networks (QTTN), and multiscale entanglement renormalization ansatz (QMERA) [10],
offer various approaches to represent complex quantum states efficiently. For example, QMPS and
QTTN can encode quantum states with fewer parameters compared to fully connected quantum
circuits, thereby reducing the model complexity while maintaining high accuracy compared to their
classical counterparts. In a specific case involving the MNIST dataset, quantum tensor networks
trained with fewer parameters achieved high accuracy compared to a classical neural network [11,12].
These tensor network architectures enable more efficient data processing and parameterization by
exploiting specific entanglement patterns and reducing redundant information.
In this work, addressing the significant environmental implications, we emphasize the urgent need
to develop AI models that achieve high performance while minimizing energy consumption. We
propose a novel architecture called attention-enhanced quantum physics-informed neural networks
(AQ-PINNs), which integrates quantum computing principles with energy-efficient AI frameworks.
By employing quantum multi-head self-attention mechanisms alongside quantum tensor networks,
AQ-PINNs aim to reduce the parameter space, thus decreasing the computational resources required
for training and inference compared to classical models. Our goal is to advance climate modeling
capabilities and contribute to sustainable climate change projections, all while maintaining high
model performance.
2 Methodology
2.1 Data
Computational fluid dynamics (CFD) is a vital tool for understanding and predicting fluid flow in
various applications, including climate modeling [13]. The dataset used in this study is derived from
numerical solutions of the incompressible Navier-Stokes equations [14]. The data is stored in the
.mat file and contains the following components:
•Xstar∈R5000×2: Spatial coordinates (x, y)for 5000 points, representing a grid similar to
those used in climate models.
•Ustar∈R5000×2×200: Velocity field (u, v), crucial for modeling atmospheric and oceanic
currents in climate systems.
•t∈R200×1: Time points ranging from 0 to 19.9 seconds, scalable to represent longer
climate timescales.
•pstar∈R5000×200: Pressure field, essential for understanding weather patterns and climate
dynamics.
The raw data is flattened and reorganized from its original 3D structure into 1D arrays, enabling
efficient sampling and manipulation. The training set is created by randomly selecting a subset of
the flattened data, ensuring a diverse and representative sample of the entire dataset. Specifically,
“N_train = 30000” training points are chosen without replacement. The entire grid of spatial
coordinates is utilized as a test set, with time set to a uniform value for visualization consistency. The
incompressible Navier-Stokes equations governing this system key to climate modeling are:
∂u
∂t+ (u· ∇)u=−1
ρ∇p+ν∇2u, with ∇ ·u= 0,where u= (u, v)is the velocity field
(representing wind or ocean currents in climate contexts), pis the pressure, ρis the fluid density,
andνis the kinematic viscosity. In our two-dimensional setting, analogous to simplified climate
models [15], these equations expand to:
∂u
∂t+u∂u
∂x+v∂u
∂y=−1
ρ∂p
∂x+ν∂2u
∂x2+∂2u
∂y2
and∂v
∂t+u∂v
∂x+v∂v
∂y=−1
ρ∂p
∂y+ν∂2v
∂x2+∂2v
∂y2
,
(1)
∂u
∂x+∂v
∂y= 0. (2)
2Figure 1: Residuals in the Navier-Stokes equations are computed precisely via automated differen-
tiation . The loss function L(γ)is differentiable for both the state variables z(γ)and the trainable
parameters γin AQ-PINNs. The predictions of AQ-PINNs can be understood from the diagrammatic
representation in the right half.
The continuity Eq. 2 ensures mass conservation, which is critical for modeling the conservation
of air and water masses in climate systems. The vorticity ω, important for understanding cyclone
formation [16], and other climate phenomena [17], is defined as: ω=∇ ×u=∂v
∂x−∂u
∂y.These
equations collectively describe fluid behavior analogous to large-scale atmospheric and oceanic
flows in climate systems [18]. The spatial domain Xstarrepresents a discretized grid similar to
those in climate models. The velocity field Ustarprovides fluid motion data comparable to wind or
ocean current measurements, while pstargives pressure distributions akin to atmospheric pressure
patterns. The temporal evolution over 200 time steps allows for analyzing both short-term weather-
like phenomena and longer-term climate-like behaviors, demonstrating the dataset’s relevance to
multiscale climate modeling challenges.
2.2 Architecture
Physics-informed neural networks (PINNs) are a class of neural networks that integrate physical laws,
typically expressed as partial differential equations (PDEs), directly into the learning process. They
incorporate physical constraints to solve forward and inverse problems across various scientific and
engineering disciplines. The key idea behind PINNs is to embed a system’s governing equations into
the neural network’s loss function [19–21].
The AQ-PINNs architecture consists of several key components, as shown in Fig. 1. The input
projection maps the input vector x∈R3, representing spatiotemporal coordinates (x, y, t ), into a new
space using a linear transformation. The core of AQ-PINNs is the Quantum Multi-head Self-Attention
(QMSA) mechanism. This mechanism encodes classical data into quantum states, computes attention
scores, and aggregates the results. Each input xifrom the linear layer is encoded into a quantum
state using a data loader operator U†(xi), where the unitary is represented as an angle embedding
followed by circuits such as QMPS, QTTN, or QMERA for different benchmarks [22–25].
For each input xi, the key Ki, query Qi, and value Vijare computed as:
Ki=⟨xi|K†(θK)Z0K(θK)|xi⟩, Q i=⟨xi|Q†(θQ)Z0Q(θQ)|xi⟩,
V ij=⟨xi|V†(θV)ZjV(θV)|xi⟩, (3)
where Z0andZjrepresent spin measurements of the qubit in the z-direction [23]. The attention
matrix Ais then computed using the key and query vectors: Aij=−(Qi−Kj)2.The final output is
obtained by applying the softmax function to the normalized attention matrixA√
dhand multiplying it
by the value matrix Vyielding to SoftMax
A√
dh
·V.Two nonlinear transformations are applied
using the tanh activation function, allowing the model to learn nonlinear mappings while maintaining
3differentiability for gradient-based optimization followed by a final output projection which generates
the predicted physical quantities (ψ,ssh, u). The AQ-PINNs model is trained using a composite loss
function that combines data-driven and physics-driven components:
•Data Loss: Ldata=E
(u−utrain)2+ (v−vtrain)2+ (ssh−ptrain)2
, ensuring fidelity to
the training data.
•Physics-Informed Loss: Lphys=E
(fx−0)2+ (fy−0)2+ (c−0)2
, enforcing adher-
ence to the Navier-Stokes equations, where fx,fy, and care the residuals computed using
automatic differentiation [17].
To optimize our training process, we utilize the L-BFGS method with a learning rate of 6.5E−1,
determined through the Super-Convergence technique. This approach involves gradually increasing
the learning rate and identifying the value that results in the most rapid decrease in the loss function,
ensuring swift and stable optimization [26]. L-BFGS is a quasi-Newton method for solving second-
order differential equations, such as the Navier-Stokes equations. By approximating the Hessian
matrix, L-BFGS efficiently handles the intricate optimization landscape inherent in such equations,
leading to quicker and more precise convergence [20, 27].
3 Results
The proposed AQ-PINNs demonstrate a substantial reduction in model parameters while preserving
and enhancing model performance in some instances. As illustrated in Table 1, the AQ-PINNs
variants, utilizing distinct quantum tensor networks such as QMPS, QTTN, and QMERA, consistently
achieved lower test loss values when compared to the classical attention-enhanced physics-informed
neural networks (A-PINNs). Specifically, the AQ-PINNs models exhibit parameter reductions of up
to 63.29% (QMPS), 55.28% (QTTN), and 51.51% (QMERA), while simultaneously attaining test
losses that are comparable to or better than those of the classical model. This significant reduction in
model parameters is of particular importance in the domain of climate modeling, as it not only results
in decreased computational demands but also contributes to a reduced carbon footprint.
Table 1: Model performance and parameter reduction.
Model Test Loss Achieved Decrease in Model Params
Classical A-PINNs 0.0631 -
AQ-PINNs (QMPS) 0.0609 63.29%
AQ-PINNs (QTTN) 0.0593 55.28%
AQ-PINNs (QMERA) 0.0596 51.51%
4 Conclusion & Future Work
The paper introduces AQ-PINNs - Attention-enhanced Quantum Physics-Informed Neural Net-
works—offering a novel approach to carbon-efficient climate modeling by integrating quantum
computing, physics-informed neural networks, and attention mechanisms targeting Jevons paradox.
Our approach has shown promising results in improving model precision and computational efficiency.
Moving forward, future work will focus on enhancing the interpretability of AQ-PINNs and address
key challenges such as sub-grid process parameterization and extreme weather event prediction,
aiming to refine and expand the model’s capabilities in climate science. This includes developing
advanced techniques for visualizing attention mechanisms, refining feature attribution methods, and
creating more robust counterfactual explanations.
Acknowledgment
This work was supported in part by the NYUAD Center for Quantum and Topological Systems
(CQTS), funded by Tamkeen under the NYUAD Research Institute grant CG008.
4References
[1]Andrew J. Lohn and Micah Musser. AI and compute: How much longer can computing power
drive artificial intelligence progress? Technical report, Center for Security and Emerging
Technology, January 2022.
[2]David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network
training. arXiv preprint arXiv:2104.10350 , 2021.
[3]Benoit Courty, Victor Schmidt, Goyal-Kamal, Marion Coutarel, Boris Feld, Jérémy Lecourt,
Liam Connell, Sab Amine, inimaz, supatomic, Mathilde Léval, Luis Blanche, Alexis Cru-
veiller, ouminasara, Franklin Zhao, Aditya Joshi, Alexis Bogroff, Amine Saboni, and Hugues
de Lavoreille. mlco2/codecarbon: v2.4.1 (v2.4.1). Zenodo, 2024.
[4]Lasse F Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. Carbontracker: Track-
ing and predicting the carbon footprint of training deep learning models. arXiv preprint
arXiv:2007.03051 , 2020.
[5]Semen Andreevich Budennyy, Vladimir Dmitrievich Lazarev, Nikita Nikolaevich Zakharenko,
Aleksei N Korovin, OA Plosskaya, Denis Valer’evich Dimitrov, VS Akhripkin, IV Pavlov,
Ivan Valer’evich Oseledets, Ivan Segundovich Barsola, et al. Eco2ai: carbon emissions tracking
of machine learning models as the first step towards sustainable ai. In Doklady Mathematics ,
volume 106, pages S118–S128. Springer, 2022.
[6]Mario Giampietro and Kozo Mayumi. Unraveling the complexity of the jevons paradox: The
link between innovation, efficiency, and sustainability. Frontiers in Energy Research , 6, 2018.
[7]Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An introduction to quantum machine
learning. Contemporary Physics , 56(2):172–185, 2015.
[8]Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth
Lloyd. Quantum machine learning. Nature , 549(7671):195–202, 2017.
[9]Kamila Zaman, Alberto Marchisio, Muhammad Abdullah Hanif, and Muhammad Shafique. A
survey on quantum machine learning: Current trends, challenges, opportunities, and the road
ahead. arXiv preprint arXiv:2310.10315 , 2023.
[10] William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire.
Towards quantum machine learning with tensor networks. Quantum Science and Technology ,
4(2):024001, jan 2019.
[11] Fanjie Kong, Xiao-Yang Liu, and Ricardo Henao. Quantum tensor network in machine learning:
An application to tiny object classification. arXiv preprint arXiv:2101.03154 , 2021.
[12] Chen-Yu Liu, En-Jui Kuo, Chu-Hsuan Abraham Lin, Sean Chen, Jason Gemsun Young, Yeong-
Jar Chang, and Min-Hsiu Hsieh. Training classical neural networks by quantum machine
learning. arXiv preprint arXiv:2402.16465 , 2024.
[13] Mohd Hafiz Zawawi, A Saleha, A Salwa, NH Hassan, Nazirul Mubin Zahari, Mohd Zakwan
Ramli, and Zakaria Che Muda. A review: Fundamentals of computational fluid dynamics (cfd).
InAIP conference proceedings , volume 2030. AIP Publishing, 2018.
[14] Ramon Codina. Numerical solution of the incompressible navier–stokes equations with coriolis
forces based on the discretization of the total time derivative. Journal of Computational Physics ,
148(2):467–496, 1999.
[15] Martín Jacques-Coper, Valentina Ortiz-Guzmán, and Jorge Zanelli. Simplified two-dimensional
model for global atmospheric dynamics. Physics of Fluids , 34(11), 2022.
[16] Olga S Rozanova, Jui-Ling Yu, and Chin-Kun Hu. Typhoon eye trajectory based on a mathemat-
ical model: Comparing with observational data. Nonlinear Analysis: Real World Applications ,
11(3):1847–1861, 2010.
[17] Ayoub Farkane, Mounir Ghogho, Mustapha Oudani, and Mohamed Boutayeb. EPINN-NSE:
Enhanced physics-informed neural networks for solving navier-stokes equations. arXiv preprint
arXiv:2304.03689 , 2023.
[18] Eric Simonnet, Henk A. Dijkstra, and Michael Ghil. Bifurcation analysis of ocean, atmosphere,
and climate models. In Roger M. Temam and Joseph J. Tribbia, editors, Special Volume:
5Computational Methods for the Atmosphere and the Oceans , volume 14 of Handbook of
Numerical Analysis , pages 187–229. Elsevier, 2009.
[19] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learn-
ing (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint
arXiv:1711.10561 , 2017.
[20] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning
(part ii): Data-driven discovery of nonlinear partial differential equations. arXiv preprint
arXiv:1711.10566 , 2017.
[21] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics , 378:686–707, 2019.
[22] Jacob Biamonte. Lectures on quantum tensor networks. arXiv preprint arXiv:1912.10049 ,
2019.
[23] Eyup B. Unlu et al. Hybrid quantum vision transformers for event classification in high energy
physics. Axioms , 13(3), 2024.
[24] Hui Zhang, Qinglin Zhao, and Chuangtao Chen. A light-weight quantum self-attention model
for classical data classification. Applied Intelligence , 2024.
[25] Siddhant Dutta, Nouhaila Innan, Alberto Marchisio, Sadok Ben Yahia, and Muhammad Shafique.
QADQN: Quantum attention deep Q-network for financial market prediction. arXiv preprint
arXiv:2408.03088 , 2024.
[26] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks
using large learning rates. In Artificial intelligence and machine learning for multi-domain
operations applications , volume 11006, pages 369–386. SPIE, 2019.
[27] Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar
Raissi, and Francesco Piccialli. Scientific machine learning through physics–informed neural
networks: Where we are and what’s next. Journal of Scientific Computing , 92(3):88, 2022.
6