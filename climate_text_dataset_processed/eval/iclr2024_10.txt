Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2024
CALIBRATING BAYESIAN UN ET++ FOR SUB-
SEASONAL FORECASTING
B¨us ¸ra Asan1∗,Abdullah Akg ¨ul2,Alper ¨Unal3,Melih Kandemir2, and G¨ozde ¨Unal1
1Department of Computer Engineering, Istanbul Technical University
2Department of Mathematics and Computer Science, University of Southern Denmark
3Eurasia Institute of Earth Sciences, Istanbul Technical University
ABSTRACT
Seasonal forecasting is a crucial task when it comes to detecting the extreme heat
and colds that occur due to climate change. Confidence in the predictions should
be reliable since a small increase in the temperatures in a year has a big im-
pact on the world. Calibration of the neural networks provides a way to ensure
our confidence in the predictions. However, calibrating regression models is an
under-researched topic, especially in forecasters. We calibrate a UNet++ based ar-
chitecture, which was shown to outperform physics-based models in temperature
anomalies. We show that with a slight trade-off between prediction error and cal-
ibration error, it is possible to get more reliable and sharper forecasts. We believe
that calibration should be an important part of safety-critical machine learning
applications such as weather forecasters.
1 I NTRODUCTION
Seasonal forecasting is a crucial task when it comes to foreseeing the effects of climate change,
especially in making predictions and decisions based on these effects. Generating accurate seasonal
and sub-seasonal forecasts demands substantial resources, such as the curation of Coupled Model
Intercomparison Projects (CMIP) datasets. These datasets combine outputs from over a hundred
climate models worldwide, facilitating top-tier climate simulations. Leveraging the vast data reser-
voirs from CMIP6 (Eyring et al., 2016), the latest phase of CMIP, there are ongoing efforts to harness
deep learning methodologies for enhanced climate forecasting. For instance, Luo et al. (2022) use
Bayesian Neural Networks (BNN) with CMIP6 for climate prediction in the North Atlantic, and
Anochi et al. (2021) use CMIP6 to assess precipitation. Andersson et al. (2021) forecast the change
in Arctic sea ice area with the same dataset. In this work, we also utilize the CMIP6 dataset to
produce well-calibrated and sharp forecasts which are crucial for climate sciences (Gneiting et al.,
2007).
We expand the capabilities of the forecast model introduced in Unal et al. (2023) using the cali-
bration approach proposed by Kuleshov et al. (2018). This model is shown to achieve better perfor-
mance than physics-based methods on sub-seasonal forecasting, especially at predicting temperature
anomalies that indicate extreme hot and cold temperatures that are crucial for climate change.
In this work, we calibrate the Bayesian version of the forecaster in a regression setting. We show that
the BNNs produce the most calibrated and sharp forecasts. We compare the performance of BNNs,
Monte Carlo Dropout (MC-Dropout) Gal & Ghahramani (2016), and Deep Ensemble Lakshmi-
narayanan et al. (2017) methods for assessing climate forecast uncertainty and sharpness, exploring
the potential for improved reliability through calibration. Our contributions can be listed as follows:
1. We apply calibration to a sub-seasonal forecaster that is able to predict extreme events
better than simulations. We show that calibrating deep learning models should be a crucial
step while applying deep learning to climate sciences.
∗Corresponding author: asan18@itu.edu.tr
1Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2024
2. We show that well-calibrated forecasters not only produce better confidence intervals but
may also improve the sharpness of the forecasts.
3. This method may be generalized to any other application in climate sciences that gives
critical importance to the reliability of the results such as extreme events, precipitation, and
natural disasters such as earthquakes, floods, and drought.
2 M ETHODOLOGY
We formulate the problem as predicting the monthly average air temperature at 2 meters above
the earth surface ( 2m) for each coordinate in a 2D temperature grid which we will name as the
temperature map. Our aim is to construct a reliable confidence interval for each coordinate since
Bayesian methods often produce uncalibrated results Kuleshov et al. (2018).
We first train the model on CMIP6 climate simulations, then fine-tune it with ERA5 reanalysis data
based on real climate measurements. We denote the 2D temperature map at time tasxt. Train set
D={Xt, Yt}T
t=1consists of stacked monthly time-series temperature maps as the input. The input
Xtrefers to xt−1:t−k−mwhich denotes the range of the stacked months and Ytcorresponds to xt.
The periodical month selection process from the given range is described in Unal et al. (2023) and
the same setting is used to make a fair comparison.
2.1 B AYESIAN UN ET++FOR TEMPERATURE PREDICTION
For temperature forecasting, we convert UNet++ into a BNN (Goan & Fookes, 2020). BNNs are
highly regarded for their capability in quantifying uncertainty, offering robust insights into predictive
models Kristiadi et al. (2020). Thus, we converted the final three layers of the UNet++ architecture
into Bayesian convolutional layers, where we model the weights θof the neural network as a Gaus-
sian distribution. Letter, we maximize evidence lower bound Kingma & Welling (2022).
2.2 U NCERTAINTY ANALYSIS
Confidence Intervals are used for measuring the uncertainty. Quantiles are calculated from the
predictions and checked whether the correct portion of the predictions actually conforms to those
intervals.
Calibration in neural networks refers that if the confidence interval is chosen as 95%, then the
intervals should capture around 95% of the observed outcomes Yt. To measure calibration, we count
observations that stay below the predicted upper bound for the quantile pof the sample t, then
normalize with the size of the dataset. A neural network is said to be calibrated if it satisfies the
following
1
TTX
t=1( 1{Yt≤F−1
t(p)})−→p, (1)
asT→ ∞ (Gneiting et al., 2007), where Ftrefers to the Cumulative Distribution Function (CDF)
of the output of the neural network Hfor the input Xt.
Calibrated Regression. We need to match empirical and predicted CDFs according to Equation 1.
Therefore, the training partition of the ERA5 dataset is used to construct a calibration dataset to
map the predicted CDF to the empirical CDF. We train a new regressor R: [0,1]→[0,1]on the
calibration dataset. Thus, we expect R◦Fkto be calibrated. CDFs are monotonically increasing
functions, hence the choice of Ris an Isotonic Regressor (Niculescu-Mizil & Caruana, 2005).
From the training partition S={Xt−1:t−k−m, Yt}T′
t=1of ERA5, calibration dataset C={ct, yt}T′
t=1
is constructed where ctrefers to Ft(Yt)andytrefers to ˆP(Ft(Yt))using the dataset generation
method in Kuleshov et al. (2018). ˆPis formulated as
ˆP=1
T|{Yt|Ft(Yt)< p, t = 1, ..., T}| (2)
where |A|refers to the cardinality of the set A. It calculates empirical CDF from the predicted CDF
by normalizing the count of output Ytstaying below pthquantile of Ft.
2Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2024
Table 1: Metrics (lower the better) for calibrated and uncalibrated versions of the models.
Uncalibrated Calibrated
Metrics CE MAE Sharpness CE MAE Sharpness
UNet++ N/A 0.975 N/A N/A N/A N/A
Bayesian UNet++ 0.023 2.237 0.291 0.015 (↓34.8%) 2.298 (↑2.7%) 0.274 (↓6.9%)
Dropout (40%) 0.131 0.993 0.853 0.035 (↓73.2%) 0.990 (↓0.3%) 0.847 (↓0.7%)
Deep Ensemble 0.086 1.548 0.789 0.024 (↓70.0%) 1.366 (↓11.8%) 0.799 (↑1.3%)
As a result, the estimated P(Y≤F−1
X(p))by the regressor Rprovides the calibrated probability
that a random Yfalls into the credible interval so that we can adjust the predicted probability to the
empirical probability.
2.3 T RAINING & E VALUATION
We train our model using the 2mtemperature variable from 9 ensembles of the CMIP6 dataset.
1700 samples are separated for training and 100 for validation. 400 samples from the ERA5 dataset
are used for fine-tuning and the construction of the calibration dataset. 116 samples from ERA5 are
used in the evaluation of all methods as in Unal et al. (2023).
Metrics. To measure accuracy, Mean Absolute Error (MAE) is used. MAE for calibrated models in
Table 1 is recalculated using the mid-quantile values from the calibrated forecaster.
Sharpness (Gneiting et al., 2007) is a metric which is widely used in climate forecasting. It measures
the concentration of the forecasts as
sharpness (F1, F2, ..., F T) =1
TTX
t=1var(Ft). (3)
0 10 20 30 40 50 60 70 80
Time Steps8910111213T emperature
65.38% of the test points within 50% interval
0 10 20 30 40 50 60 70 80
Time Steps8910111213T emperature
56.41% of the test points within 50% interval
0 10 20 30 40 50 60 70 80
Time Steps68101214T emperature
98.72% of the test points within 90% interval
0 10 20 30 40 50 60 70 80
Time Steps891011121314T emperature
89.74% of the test points within 90% interval
Figure 1: 50% confidence interval (Top) and 90% confidence interval (Bottom) of the Bayesian
UNet++ for a sample in the North West Coast of America are given. The mean coverage percentages
for confidence intervals are 63% and91% for the calibrated, and 66% and98% for the uncalibrated
models. Thus, we choose a representative sample. Uncalibrated confidence interval plots are shown
on the left (green), and calibrated plots are on the right (blue). Grey dots refer to the average
temperature values for each month in the given time period (2016-2021). The percentage of the
values falling within the intervals aligns more closely with the expected confidence levels, both at
50% and90% in the calibrated model’s plot.
3Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2024
Calibration error (CE) proposed by Kuleshov et al. (2018) is used for assessing the quality of the
calibration of the forecasts as
CE(F1, Y1, ..., F T, YT) =1
mmX
i=1wj(pj−ˆpj) (4)
where mrefers to the number of confidence levels 0≤p1≤...≤pj≤...≤pm≤1andˆpjis
empirical frequency. In this setting, wjis chosen as 1.
3 R ESULTS
We use the experimental settings of Unal et al. (2023). Table 1 illustrates the impact of calibration
on uncertainty quantification methods. The Bayesian model demonstrates the highest sharpness and
calibration as we expected. However, there exists a trade-off between MAE and CE, with Bayesian
demonstrating the lowest CE, followed by Ensemble and Dropout. Apart from the reduction in CE,
we observe a decrease in MAE for Dropout and Deep Ensemble which suggests that calibration not
only improves the accuracy of the network performance but also enhances the capture probability
percentages of confidence intervals around point estimates. MAE is calculated using the actual 50%
quantile values predicted by the Isotonic Regressor.
Figure 1 demonstrates that for the calibrated case, roughly 90% of the 90% confidence intervals
capture the true temperature values in the test dataset. We also observe the same result for 50%
interval. Thus, the proposed calibration produced results in line with the expected proportion of
confidence intervals capturing the true outcome at the given confidence level, suggesting that the
model is well-calibrated.
0.0 0.2 0.4 0.6 0.8 1.0
Predicted Confidence Level0.00.20.40.60.81.0Observed Confidence Level
Calibration Plot
uncalibrated
calibrated
Figure 2: Calibration plot suggested
by Kuleshov et al. (2018) given for a
sample in the grid in Figure 1 to eval-
uate the calibration of the forecasts.
Each predicted confidence level is plot-
ted against its corresponding expected
confidence level. Predictions illustrate
the frequency of observing an outcome
Ytat each level. We expect calibrated
models to be closer to y=x.CE is visualized in Figure 2. Equation 4 is applied to the
values calculated for the calibration plot for each quantile,
and the mean is used as the CE of that sample. After the
calibration, our model converges to the y=xline which
indicates that the predicted confidences for the samples
are closer to expected confidences, especially for quan-
tiles larger than 50%.
4 C ONCLUSION
We proposed a method to enhance the sharpness and re-
liability of weather forecasts by calibrating them using
a CDF-based calibration approach. This involved trans-
forming the final layers of UNet++ to Bayesian. Peri-
odically stacked multi-dimensional time-series data used
as input. As we designed the output of the network to
produce a CDF, we trained an isotonic regressor to cali-
brate the confidence intervals. We benchmarked the cali-
brated and uncalibrated results of three uncertainty quan-
tification methods. Furthermore, we show that calibrat-
ing Dropout and Deep Ensemble might increase the accu-
racy of the network along with improving the uncertainty
quantification.
This work emphasizes the significance of calibrating neu-
ral networks while suggesting potential improvements for
forecast reliability. Various fields in climate sciences can
benefit from calibration since uncertainties arise from in-
complete modeling of the earth and the inherent complex-
ity of climate systems. While our focus was on temper-
ature forecasting, this approach can be extended to pre-
dicting other essential climate variables such as precipitation, pressure, and wind components.
4Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2024
REFERENCES
Tom R Andersson, J Scott Hosking, Mar ´ıa P´erez-Ortiz, Brooks Paige, Andrew Elliott, Chris Russell,
Stephen Law, Daniel C Jones, Jeremy Wilkinson, Tony Phillips, et al. Seasonal arctic sea ice
forecasting with probabilistic deep learning. Nature Communications , 12(1):5124, 2021.
Juliana Aparecida Anochi, Vin ´ıcius Albuquerque de Almeida, and Haroldo Fraga de Campos Velho.
Machine learning for climate precipitation prediction modeling over south america. Remote Sens-
ing, 13(13):2468, 2021.
V . Eyring, S. Bony, G. A. Meehl, C. A. Senior, B. Stevens, R. J. Stouffer, and K. E. Taylor. Overview
of the coupled model intercomparison project phase 6 (cmip6) experimental design and organiza-
tion. Geoscientific Model Development , 9:1937–1958, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning , pp. 1050–1059.
PMLR, 2016.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration
and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 69(2):
243–268, 2007.
Ethan Goan and Clinton Fookes. Bayesian neural networks: An introduction and survey. In Case
Studies in Applied Bayesian Data Science , pp. 45–87. Springer International Publishing, 2020.
doi: 10.1007/978-3-030-42553-1 3.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes
overconfidence in relu networks, 2020.
V olodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In International Conference on Machine Learning , pp. 2796–2804.
PMLR, 2018.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. Advances in Neural Information Processing
Systems , 30, 2017.
Xihaier Luo, Balasubramanya T Nadiga, Ji Hwan Park, Yihui Ren, Wei Xu, and Shinjae Yoo. A
bayesian deep learning approach to near-term climate prediction. Journal of Advances in Model-
ing Earth Systems , 14(10):e2022MS003058, 2022.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-
ing. In International Conference on Machine Learning , pp. 625–632, 2005.
Alper Unal, Busra Asan, Ismail Sezen, Bugra Yesilkaynak, Yusuf Aydin, Mehmet Ilicak, and Gozde
Unal. Climate model-driven seasonal forecasting approach with deep learning. Environmental
Data Science , 2:e29, 2023. doi: 10.1017/eds.2023.24.
5