An Interpretable Model of Climate Change
Using Correlative Learning
Charles Anderson & Jason Stock
Computer Science
Colorado State University
{anderson,stock}@colostate.edu
Abstract
Determining changes in global temperature and precipitation that may indicate
climate change is complicated by annual variations. One approach for finding
potential climate change indicators is to train a model that predicts the year from
annual means of global temperatures and precipitations. Such data is available
from the CMIP6 ensemble of simulations. Here a two-hidden-layer neural network
trained on this data successfully predicts the year. Differences among temperature
and precipitation patterns for which the model predicts specific years reveal changes
through time. To find these optimal patterns, a new way of interpreting what the
neural network has learned is explored. Alopex, a stochastic correlative learning
algorithm, is used to find optimal temperature and precipitation maps that best
predict a given year. These maps are compared over multiple years to show how
temperature and precipitations patterns indicative of each year change over time.
1 Introduction
Deep networks have been used successfully to model many complex relationships in data from a wide
variety of domains. Both practice and theory suggest that large, deep networks may generalize better
to untrained data samples. This leads to important questions regarding the interpretability of such
large networks to understand the limitations and trustworthiness of these models. In many studies
involving measurements of the natural world, domain experts are most familiar with simple statistical
analysis, such as linear regression. Therefore, to simplify the interpretability and trust of models,
models that are simple extensions of linear models should be the first step.
Here we follow this suggestion in an attempt to find indicators of climate change using a simple
two-hidden layer neural network trained on global temperatures and precipitations from the sixth
phase of the Coupled Model Intercomparison Project [ 1] (CMIP6). The potential of discovering
indicators of climate change from the previous CMIP5 data was demonstrated by Barnes, et al.,
[2,3] who found that linear models map global temperature data to year quite well and a simplified
interpretation of the models was performed by analyzing the resulting linear model weights. In
follow-on work, they recast the regression problem as a classification task in order to use Layerwise
Relevance Propagation (LRP) to identify spatial patterns significant to classifying specific decades [ 4].
Here we return to the regression approach and extend it by including temperature and precipitation
CMIP6 data and modeling it with a neural network having two hidden layers. A correlative learning
algorithm is then applied to interpret what our neural network has learned about how temperatures
and precipitations change over the years.
In Section 2, a brief summary of local and global methods for interpreting what a neural network has
learned are reviewed. Alopex is described as a global method. This is followed by a demonstration of
the Alopex method by using it to interpret what a neural network has learned when trained on the
MNIST data. This same approach is then applied in Section 3 to a neural network trained to predict
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.Figure 1: Input images generated by Alopex that maximize the likelihood of each digit. The images
are averages of the final images from 20 repetitions of the Alopex algorithm.
the year from global temperature and precipitation data, revealing intriguing changes in this data over
time.
2 Interpretation of Neural Network Models
Interpretability methods aim to convey information about a network’s learning process and decision
making. These methods largely fall in two categories, namely local and global. Local explanations
provide insight on individual predictions. For example, saliency maps show certain pixels for a given
sample with a positive saliency that increase accuracy in the prediction [ 5]. Further, methods such as
LRP [ 6], DeepLift [ 7], and Gradient*Input [ 8] aim to provide pixel-wise relevance. In many cases
these methods are mathematically equivalent (i.e., network’s with ReLU activations or zero valued
baselines) [9] and are cumbersome to reason about over a large set of data.
Global methods describe the general behavior of a network where explanations are made on a class
of predictions or network components. Most common for neural networks are methods to understand
high-level concepts (e.g., color or texture sensitivity) [ 10], visualizing maximal neuron or layer
activations [ 11], or optimizing an input to maximize the probability of an output [ 5]. In this work, we
focus on finding an optimal input that generalizes to individual outputs.
Both local and global methods that are gradient-free do exist (e.g., computing surrogate models as
done with LIME [ 12] or Shapley value-based feature importance [ 13]). However, the majority of
these methods rely on backpropagating gradients through the network (including [ 11,5]). Here we
introduce Alopex as a global interpretaton method that does not rely on gradients. Specifically, we
leverage local correlations between changes in input features and changes in the global error function
to produce a global view on a class of predictions.
Algorithm 1 Alopex
1:f(x)is neural network model
2:T←1.0
3:K←100,000
4:δ←0.0002
5:m0←0
6:t←digit or year
7:x0←vector of constant midrange values
8:fork∈ {1, ..., K}do
9: yk←f(xk)
10: ek←loss(yk, t)
11: ck←(ek−ek−1)(xk−xk−1)
12: pk←1/(1 +e−ck/T)
13: dk←δ, w.p.pk
−δ,w.p. 1−pk
14: xk+1←xk+dk+λmk
15: mk+1←dk+λmk
16: Tk+1←0.9998Tk
17:end for
18:return xk+1The Alopex [ 14] algorithm was developed by E.
Harth to investigate the receptive field of neu-
rons in the frog optic tectum. The intensities
of individual pixels in an image presented to
the frog were randomly initialized and updated
during a number of steps. In each step the cor-
relation between changes in a pixel’s intensity
and changes in the neuron’s firing rate deter-
mined which way to adjust each intensity, which
was increased if the correlation was positive and
decreased if it was negative. This simple cor-
relative learning process was adapted by K.P.
Unnikrishnan, et al., [ 15] to train feedforward
and recurrent neural networks. In a very similar
manner to Harth’s original work, we use Alopex
to incrementally adjust the input values to our
neural network using the correlations between
input changes and changes in the loss of our
model. This process converges on images that
maximally predict a certain class in a classifi-
cation problem, or a certain output value in a
regression problem. The Alopex algorithm is
summarized in Algorithm 1, where xis the input
to our neural network model, f(x)is the output of the model and loss(yk, t)is the loss for predicted
value ykand correct target value t.
21850 1900 1950 2000 2050 2100
Actual Year185019001950200020502100Predicted Year
Train
Validation
T esta. b.
Figure 2: a. Model structure with 40 tanh units in each of two hidden layers and a linear output unit.
b. Predicted versus actual year for all samples, which were partitioned into train, validation, and test
subsets by years.
Alopex is demonstrated by applying it to a simple network trained on the MNIST digits. The network
contains two layers of 20 convolutional units each with kernel size 5 x 5 and stride of 1 x 1. The
network is trained using Møller’s Scaled Conjugate Gradient Algorithm [ 16] with cross-entropy
loss. For each digit, Alopex is applied 20 times starting with different initial pixel values. The
mean of the 20 resulting images are shown in Figure 1. All 20 images for each digit were classified
correctly. Since Alopex starts with randomly-initialized pixel values, it is not dependent on particular
image samples. Thus, this global method converges on images that are most confidently predicted
as coming from the correct class. White, or high intensity, pixels strongly correlate with the digit
class, while black pixels negatively correlate with the digit class. Pixels near the edges are medium
intensity showing their values have little correlation with the digit class. Many questions arise when
studying these images and they are being investigated in on-going work. Here we use MNIST just as
a demonstration.
3 Results on Climate Change Modeling
The CMIP6 [ 1] data was produced by 35 models of earth’s atmosphere from which simulated global
temperature and precipitation maps can be obtained for years 1850 to 2100. This data has a spatial
resolution of 120 latitude and 240 longitude values. Not surprisingly, there are numerous correlations
among temperatures and precipitations at multiple spatial locations, which was dealt with in prior
work by performing ridge regression to limit the magnitude of weights in the first layer of the neural
network models [ 2]. Here we use an alternative approach, Principal Components Analysis (PCA), to
represent the data with independent factors and to decrease the data dimensionality by projecting
the2×120×240or57,600dimensional annual samples to the first 250 singular vectors, i.e., the
ones capturing most of the variance in the data. The choice of 250 singular vectors was made by
comparing the RMSE in predicted year for the validation data set when using the different quantities
of top singular vectors from 1 to 1000.
A two-layer fully-connected neural network was trained on the CMIP6 data as follows. Each hidden
layer contained 20 tanh units. PCA was applied separately to the temperature and precipitation data
and the resulting 250-dimensional vector was input to the first hidden layer. The output layer of
the neural network was a single linear unit trained to predict the year. Figure 2a shows the model
structure and Figure 2b shows the results of training the network using the Scaled Conjugate Gradient
algorithm [ 16] to minimize the mean square error in the predicted year. This plot suggests that years
after about 2000 are easier to predict, possibly due to human-caused forcing functions included in the
CMIP6 models.
Similar to the above demonstration of Alopex on the MNIST data, we applied Alopex to the CMIP6
model by searching for input temperature and precipitation maps that best predicted particular years.
Alopex was initialized with PCA projected values near their median values with uniformly-distributed
noise added. This was repeated 20 times. For each result the PCA projection was inverted to produce
temperature and precipitation maps and the means of the resulting maps were calculated. The
resulting maps for year 1850 were subtracted from the maps for following years to highlight changes
3Figure 3: Patterns in global temperature and precipitation that optimally relate to particular years.
Colors correspond to differences from the temperature and precipitation maps found for year 1850.
The left column of maps show areas of higher temperatures than those for 1850 in red and lower
temperatures in blue. The right column of maps show areas of higher precipitation levels than those
in year 1850 in blue and lower values in brown.
from 1850. Figure 3 illustrates this result for target years 1900, 2000, and 2100. Results for other
years are shown in Figure 6.
Several interesting patterns can be discerned in these images. Year 2000 shows considerable warming
in the Antarctic region and western United states. Year 2100 shows warmer temperatures in many
areas. The precipitation maps on the right side show drier regions along the Pacific equator in Years
1900 and 2000. In Year 2100 wetter areas north and south of the equator in the Pacific Ocean are
apparent. Similar questions arise when considering maps for other years as shown in Supplementary
Figure 6.
4 Conclusion
Results shown here suggest that the correlative learning algorithm, Alopex, may be helpful in inter-
preting neural network models by finding global, approximately optimal, input patterns corresponding
to particular network output values. It can be similarly used to find global input patterns that cause any
unit in the neural network to respond maximally. The fact that it is a correlative learning algorithm
means that gradients are not required. However, it would be interesting to compare the results
obtained here with other interpretation methods, especially gradient-based global approaches.
The temperature and precipitation maps resulting from the Alopex method reveal interesting changes
over the years. Currently, specialists in climate and atmosphere modeling are being consulted to
assist in determining relationships between these patterns and changes that are expected from current
knowledge of effects on climate due to human activities.
Acknowledgments and Disclosure of Funding
This work is supported by NSF Grant No. 2019758, AI Institute for Research on Trustworthy AI in
Weather, Climate, and Coastal Oceanography (AI2ES) .
References
[1]V . Eyring, S. Bony, G. A. Meehl, C. A. Senior, B. Stevens, R. J. Stouffer, , and K. E. Taylor.
Overview of the coupled model intercomparison project phase 6 (cmip6) experimental design
4and organization. Geosci. Model Dev. , 9:1937–1958, 2016.
[2]E. A. Barnes, J. W. Hurrell, I. Ebert-Uphoff, C. Anderson, and D. Anderson. Viewing forced
climate patterns through an ai lens. Geophysical Research Letters , 46(13):13389–13398, 2019.
[3]Elizabeth A. Barnes, Benjamin Toms, James W. Hurrell, Imme Ebert-Uphoff, Chuck Anderson,
and David Anderson. Indicator patterns of forced change learned by an artificial neural network.
Journal of Advances in Modeling Earth Systems , 12(9):e2020MS002195, 2020.
[4]B. A. Toms, E. A. Barnes, and I. Ebert-Uphoff. Physically interpretable neural networks for the
geosciences: applications to earthsystem variability. Journal of Advances in Modeling Earth
Systems , 12:e2019MS002002, 2020.
[5]Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 ,
2013.
[6]Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert
Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by
layer-wise relevance propagation. PloS one , 10(7):e0130140, 2015.
[7]Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International conference on machine learning , pages
3145–3153. PMLR, 2017.
[8]Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black
box: Learning important features through propagating activation differences. arXiv preprint
arXiv:1605.01713 , 2016.
[9]Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. Towards better under-
standing of gradient-based attribution methods for deep neural networks. arXiv preprint
arXiv:1711.06104 , 2017.
[10] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors
(tcav). In International conference on machine learning , pages 2668–2677. PMLR, 2018.
[11] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision , pages 818–833. Springer, 2014.
[12] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?" explaining
the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining , pages 1135–1144, 2016.
[13] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.
Advances in neural information processing systems , 30, 2017.
[14] E. Harth and E. Tzanakou. Alopex: A stochastic method for determining visual receptive fields.
Vision Research , 14(12):1475–1482, 1974.
[15] K. P. Unnikrishnan and K. P. Venugopal. Alopex: A Correlation-Based Learning Algorithm for
Feedforward and Recurrent Neural Networks. Neural Computation , 6(3):469–490, 05 1994.
[16] M.F. Møller. A scaled conjugate gradient algorithm for fast supervised learning. Neural
networks , 6(4):525–533, 1993.
5Appendix
This appendix includes additional figures illustrating the convergence of the Alopex algorithm for the
MNIST digits (Figure 4) and for two years of the CMIP6 data (Figure 5). It also includes temperature
and precipitation maps that most confidently predict years 1875, 1900, 1925, . . ., 2100 as differences
from the maps of 1850 (Figure 6).
Figure 4: Input images generated by Alopex that maximize the likelihood of each digit. Above each
image are graphs of the loss versus iterations of the Alopex algorithm. The images are averages from
20 repetitions of the Alopex algorithm. The loss of each iteration is plotted above each image, with
different colors for different repetitions.
Figure 5: Examples of Alopex applied to the CMIP6 neural network to generate temperature and
precipitation maps that most confidently predict Year 1850 (top row) and 2100 (bottom row). On the
left are plots of the mean square error in predicted year verus steps of the Alopex algorithm. Different
colors are for 20 different initializations of Alopex.
6Figure 6: Patterns in global temperature and precipitation that optimally relate to a sequence of years.
Colors correspond to differences from the mean temperatures and precipitations. The left column of
maps show areas of higher temperatures than the mean in red and lower temperatures in blue. The
right column of maps show areas of higher precipitation than the mean in blue and lower values in
brown.
7