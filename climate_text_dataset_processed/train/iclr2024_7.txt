Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
SCALING TRANSFORMERS FOR SKILLFUL AND RELI-
ABLE MEDIUM -RANGE WEATHER FORECASTING
Tung Nguyen
UCLARohan Shah
UCLA, CMUHritik Bansal
UCLATroy Arcomano
Argonne National Laboratory
Romit Maulik
Argonne National Laboratory, Penn State UniversityVeerabhadra Kotamarthi
Argonne National Laboratory
Ian Foster
Argonne National LaboratorySandeep Madireddy
Argonne National LaboratoryAditya Grover
UCLA
ABSTRACT
Weather forecasting is a fundamental problem for anticipating and mitigating the
impacts of climate change. Recently, data-driven approaches for weather forecast-
ing based on deep learning have shown great promise, achieving accuracies that
are competitive with operational systems. However, those methods often employ
complex, customized architectures without sufficient ablation analysis, making it
difficult to understand what truly contributes to their success. Here we introduce
Stormer, a simple transformer model that achieves state-of-the-art performance
on weather forecasting with minimal changes to the standard transformer back-
bone. We identify the key components of Stormer through careful empirical anal-
yses, including weather-specific embedding, randomized dynamics forecast, and
pressure-weighted loss. At the core of Stormer is a randomized forecasting ob-
jective that trains the model to forecast the weather dynamics over varying time
intervals. During inference, this allows us to produce multiple forecasts for a tar-
get lead time and combine them to obtain better forecast accuracy. On Weather-
Bench 2, Stormer performs competitively at short to medium-range forecasts and
outperforms current methods beyond 7 days, while requiring orders-of-magnitude
less training data and compute. Additionally, we demonstrate Stormer’s favorable
scaling properties, showing consistent improvements in forecast accuracy with in-
creases in model size and training tokens.
1 M ETHODOLOGY
We introduce Stormer, an effective deep learning model for weather forecasting. We focus on the
simplicity of the architecture, and aim to show that such an architecture can achieve competitive
forecast performances with a well-designed framework. We first present the overall training and
inference procedure of Stormer, and proceed to describe the model architecture we implement in
practice. We provide an in-depth discussion about the different design choices of Stormer in Ap-
pendix D and empirically demonstrate the importance of each component in Appendix F.2.
1.1 T RAINING
We train Stormer to forecast the weather dynamics ∆δt=Xδt−X0, which is the difference between
two consecutive weather conditions, X0andXδt, across the time interval δt. A common practice in
previous works (Keisler, 2022; Lam et al., 2023) is to use a small fixed value of δtsuch as 6 hours.
However, as we show in Figure 6a, while small intervals tend to work well for short lead times,
larger intervals excel at longer lead times (beyond 7 days) due to less error accumulation. Therefore,
having a model that can produce forecasts at different intervals and combine them in an effective
manner has the potential to improve the performance of single-interval models. This motivates
ourrandomized dynamics forecasting objective , which trains Stormer to forecast the dynamics at
1Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
random intervals δtby conditioning on δt:
L(θ) =Eδt∼P(δt),(X0,Xδt)∼D||fθ(X0, δt)−∆δt||2
2, (1)
in which P(δt)is the distribution of the random interval. In our experiments, unless otherwise spec-
ified, P(δt)is a uniform distribution over three values δt∼ U{ 6,12,24}. These three time intervals
play an important role in atmospheric dynamics. The 6 and 12-hour values help to encourage the
model to learn and resolve the diurnal cycle (day-night cycle), one of the most important oscillations
in the atmosphere driving short-term dynamics (e.g., temperature over the course of a day). The 24-
hour value filters the effects of the diurnal cycle and allows the model to learn longer, synoptic-scale
dynamics which are particularly important for medium-range weather forecasting (Holton, 2004).
1.1.1 P RESSURE -WEIGHTED LOSS
Due to the large number of variables being predicted, we use a physics-based weighting function to
weigh variables near the surface higher. Since each variable lies on a specific pressure level, we can
use pressure as a proxy for the density of the atmosphere at each level. This weighting allows the
model to prioritize near-surface variables, which are important for weather forecasting and have the
most societal impact. The final objective function that we use for training is:
L(θ) =E
1
V HWVX
v=1HX
i=1WX
j=1w(v)L(i)(b∆vij
δt−∆vij
δt)2
. (2)
The expectation is over δt, X 0, and Xδtwhich we omit for notational simplicity. In this equation,
w(v)is the weight of variable v, and L(i)is the latitude-weighting factor.
1.1.2 M ULTI -STEP FINETUNING
To produce forecasts at a lead time beyond the training intervals, we roll out the model several times.
Since the model’s forecasts are fed back as input, the forecast error accumulates as we roll out more
steps. To alleviate this issue, we finetune the model on a multi-step loss function. Specifically, for
each gradient step, we roll out the model Ktimes, and average the objective (2) over the Ksteps.
The multi-step loss is thus:
L(θ) =E
1
KV HWKX
k=1VX
v=1HX
i=1WX
j=1w(v)L(i)(b∆vij
kδt−∆vij
kδt)2
. (3)
In practice, we implement a three-phase training procedure for Stormer. In the first phase, we train
the model to perform single step forecasting, which is equivalent to optimizing the objective in (2).
In the second and third phases, we finetune the trained model from the preceding phase with K=4
andK=8, respectively. We use the same sampled value of the interval δtfor all Ksteps. We tried
randomizing δtat each rollout step, but found that doing so destabilized training as the loss value at
each step is of different magnitudes, hurting the final performance of the model.
1.2 I NFERENCE
At test time, Stormer can produce forecasts at any time interval δtused during training. Thus the
model can generate multiple forecasts for a target lead time Tby creating different combinations of
δtthat sum to T. We consider two inference strategies for generating forecasts:
Homogeneous We only consider homogeneous combinations of δt, i.e., combinations with just one
value of δt. For example, for T= 24 we consider [6, 6, 6, 6], [12, 12], and [24].
BestminnWe generate ndifferent, possibly heterogeneous combinations of δt, evaluate each
combination on the validation set, and pick mcombinations with the lowest losses for testing.
2Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
1.3 M ODEL ARCHITECTURE
We instantiate the framework presented in Section 1.1 with a simple Transformer (Vaswani et al.,
2017)-based architecture. Due to the similarity of weather forecasting to various dense prediction
tasks in computer vision, one might consider applying Vision Transformer (ViT) (Dosovitskiy et al.,
2020) for this task. However, weather data is distinct from natural images, primarily due to its
significantly higher number of input channels. These channels represent atmospheric variables with
intricate physical relationships. For example, the wind fields in the atmosphere are closely related to
the gradient and shape of the geopotential field, while the wind field redistributes moisture and heat
around the globe. Effectively modeling these interactions is critical to forecast accuracy.
1.4 W EATHER -SPECIFIC EMBEDDING
The standard patch embedding module in ViT, which uses a linear layer for embedding all input
channels within a patch into a vector, may not sufficiently capture the complex interactions among
input atmospheric variables. Therefore, we adopt for our architecture a weather-specific embedding
module, consisting of two components, variable tokenization andvariable aggregation .
Variable tokenization Given an input of shape V×H×W, variable tokenization linearly embeds
each variable independently to a sequence of shape (H/p)×(W/p)×D, in which pis the patch
size and Dis the hidden dimension. We then concatenate the output of all variables, resulting in a
sequence of shape (H/p)×(W/p)×V×D.
Variable aggregation We employ a single-layer cross-attention mechanism with a learnable query
vector to aggregate information across variables. This module operates over the variable dimension
on the output from the tokenization stage to produce a sequence of shape (H/p)×(W/p)×Dwhich
is then fed to the transformer backbone. This module offers two primary advantages. First, it reduces
the sequence length by a factor of V, significantly alleviating the computational cost as we use trans-
former to process the sequence. Second, unlike standard patch embedding, the cross-attention layer
allows the models to learn non-linear relationships among input variables, enhancing the model’s
capacity to capture complex physical interactions. We present the complete implementation details
of the weather-specific embedding in Appendix E.
1.4.1 S TORMER TRANSFORMER BLOCK
Following weather-specific embedding, the tokens are processed by a stack of transformer
blocks (Vaswani et al., 2017). In addition to the input X0, the block also needs to process the
time interval δt. We do this by replacing the standard layer normalization module used in trans-
former blocks with adaptive layer normalization (adaLN) (Perez et al., 2018). In adaLN, instead
of learning the scale and shift parameters γandβas independent parameters of the network, we
regress them with an one-layer MLP from the embedding of δt. Compared to ClimaX (Nguyen
et al., 2023) which only adds the lead time embedding to the tokens before the first attention layer,
adaLN is applied to every transformer block, thus amplifying the conditioning signal. Figure 6c
shows the consistent improvement of adaLN over the additive lead time embedding used in ClimaX.
Adaptive layer norm was widely used in both GANs (Karras et al., 2019; Brock et al., 2018) and
Diffusion (Dhariwal & Nichol, 2021; Peebles & Xie, 2023) to condition on additional inputs such
as time steps or class labels. Figure 3 illustrates Stormer’s architecture. We refer to (Nguyen et al.,
2023) for illustrations of the weather-specific embedding block.
2 E XPERIMENTS
We compare Stormer withstate-of-the-art weather forecasting methods, and conduct extensive abla-
tion analyses to understand the importance of each component in Stormer. We also study Stormer
scalability by varying model size and the number of training tokens. We conduct all experiments
on WeatherBench 2 (WB2) (Rasp et al., 2023), a standard benchmark that provides training data,
a set of state-of-the-art models, and an evaluation framework for comparing data-driven weather
forecasting methods. In the following results, unless specified otherwise, we use the same training
and evaluation setup for Stormer. We provide complete experiment details in Appendix E.
3Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
2 4 6 8 10 12 140.51.01.52.02.53.03.5
T2m (K)
2 4 6 8 10 12 1412345
U10m (m/s)
2 4 6 8 10 12 1412345
V10m (m/s)
2 4 6 8 10 12 14200400600800RMSE
MSLP (Pa)
2 4 6 8 10 12 1402004006008001000
Z500 (m2/s2)
2 4 6 8 10 12 141234
T850 (K)
2 4 6 8 10 12 140.500.751.001.251.501.752.00
Q700 (g/kg)
2 4 6 8 10 12 14
Lead time (days)1234567
U850 (m/s)
2 4 6 8 10 12 141234567
V850 (m/s)
Climatology Pangu-Weather GraphCast Stormer
Figure 1: Global forecast verification results of Stormer and the baselines. We show the latitude-
weighted RMSE for select variables. Stormer is on par or outperforms each of the benchmark
models for the shown variables. During the later portion of the forecasts, Stormer gains ∼1day of
forecast skill with respect to climatology compared to the next best deep learning model. We note
that Stormer was trained on much lower resolution data (1.40625◦) compared to Pangu-Weather
(0.25◦) and GraphCast (0.25◦).
Data: We train and evaluate Stormer on the ERA5 dataset from WB2, which is the curated version
of the ERA5 reanalysis data provided by the European Center for Medium-Range Weather Forecast-
ing (ECMWF) (Hersbach et al., 2020). In its raw form, ERA5 contains hourly data from 1979 to the
current time at 0.25◦(721×1440 grids) resolution, with different atmospheric variables spanning
137 pressure levels plus the Earth’s surface. WB2 downsamples this data to 6-hourly with 13 pres-
sure levels and provides different spatial resolutions. In this work, we use the 1.40625◦(128×256
grids) data. We use four surface-level variables – 2-meter temperature (T2m), 10-meter U and V
components of wind (U10 and V10), and Mean sea-level pressure (MSLP), and five atmospheric
variables – Geopotential (Z), Temperature (T), U and V components of wind (U and V), and Spe-
cific humidity (Q), each at 13pressure levels {50,100,150,200,250,300,400,500,600,700,850,
925,1000}. We train Stormer on data from 1979 to 2018, validate in 2019, and test in 2020, which
is the common year for testing in WB2.
Results: Figure 1 evaluates different methods on forecasting nine key weather variables at lead times
from 1to14days. For short-range, 1–5day forecasts, Stormer’s accuracy is on par with or exceeds
that of Pangu-Weather, but lags slightly behind GraphCast. At longer lead times, Stormer excels,
consistently outperforming both Pangu-Weather and GraphCast from day 6 onwards by a large
margin . Moreover, the performance gap increases as we increase the lead time. At 14day forecasts,
Stormer performs better than GraphCast by 10%−20% across all 9key variables. Stormer is also the
only model in this comparison that performs better than Climatology at long lead times, while other
methods approach or even do worse than this simple baseline. The model’s superior performance at
long lead times is attributed to the use of randomized dynamics training, which improves forecast
accuracy by averaging out multiple forecasts, especially when individual forecasts begin to diverge.
4Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
Moreover, we also note that Stormer achieves this performance with much less compute and training
data compared to the two deep learning baselines. We train Stormer on 6-hourly data of 1.40625◦
with 13 pressure levels, which is approximately 190 ×less data than Pangu-Weather’s hourly data
at 0.25◦and 90 ×less than that used for GraphCast, which also uses 6-hourly data but at a 0.25◦
resolution with 37 pressure levels. The training of Stormer was completed in under 24 hours on 128
A100 GPUs. In contrast, Pangu-Weather took 60 days to train four models on 192 V100 GPUs,
and GraphCast required 28 days on 32 TPUv4 devices. This training efficiency will facilitate future
works that build upon our proposed framework.
REFERENCES
Marcin Andrychowicz, Lasse Espeholt, Di Li, Samier Merchant, Alex Merose, Fred Zyda, Shreya
Agrawal, and Nal Kalchbrenner. Deep learning for day forecasts from sparse observations. arXiv
preprint arXiv:2306.06079 , 2023.
Peter Bauer, Alan Thorpe, and Gilbert Brunet. The quiet revolution of numerical weather prediction.
Nature , 525(7567):47–55, 2015.
Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-
range global weather forecasting with 3D neural networks. Nature , 619(7970):533–538, 2023.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096 , 2018.
Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming Ma,
Tianning Zhang, Rui Su, Yuanzheng Ci, Bin Li, Xiaokang Yang, and Wanli Ouyang. FengWu:
Pushing the skillful global medium-range weather forecast beyond 10 days lead. arXiv preprint
arXiv:2304.02948 , 2023a.
Lei Chen, Xiaohui Zhong, Feng Zhang, Yuan Cheng, Yinghui Xu, Yuan Qi, and Hao Li. Fuxi:
a cascade machine learning forecasting system for 15-day global weather forecast. npj Cli-
mate and Atmospheric Science , 6(1):190, 2023b. doi: 10.1038/s41612-023-00512-1. URL
https://doi.org/10.1038/s41612-023-00512-1 .
Lei Chen, Xiaohui Zhong, Feng Zhang, Yuan Cheng, Yinghui Xu, Yuan Qi, and Hao Li. FuXi: A
cascade machine learning forecasting system for 15-day global weather forecast. arXiv preprint
arXiv:2306.12873 , 2023c.
Mariana CA Clare, Omar Jamil, and Cyril J Morcrette. Combining distribution-based neural net-
works to predict weather forecast probabilities. Quarterly Journal of the Royal Meteorological
Society , 147(741):4337–4357, 2021.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Ad-
vances in Neural Information Processing Systems , 34:8780–8794, 2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 , 2020.
P. D. Dueben and P. Bauer. Challenges and design choices for global weather
and climate models based on machine learning. Geoscientific Model Devel-
opment , 11(10):3999–4009, 2018. doi: 10.5194/gmd-11-3999-2018. URL
https://gmd.copernicus.org/articles/11/3999/2018/ .
Lasse Espeholt, Shreya Agrawal, Casper Sønderby, Manoj Kumar, Jonathan Heek, Carla Bromberg,
Cenk Gazen, Rob Carver, Marcin Andrychowicz, Jason Hickey, et al. Deep learning for twelve
hour precipitation forecasts. Nature communications , 13(1):1–10, 2022.
William A Falcon. PyTorch lightning. GitHub , 3, 2019.
5Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
Charles R. Harris, K. Jarrod Millman, St ´efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti
Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern ´andez
del R ´ıo, Mark Wiebe, Pearu Peterson, Pierre G ´erard-Marchant, Kevin Sheppard, Tyler Reddy,
Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array program-
ming with NumPy. Nature , 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-
2649-2. URL https://doi.org/10.1038/s41586-020-2649-2 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
770–778, 2016.
Hans Hersbach, Bill Bell, Paul Berrisford, Gionata Biavati, Andr ´as Hor ´anyi, Joaqu ´ın
Mu˜noz Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Iryna Rozum, Dinand Schepers,
Adrian Simmons, Cornel Soci, Dick Dee, and Jean-No ¨el Th ´epaut. ERA5 hourly data on sin-
gle levels from 1979 to present. Copernicus Climate Change Service (C3S) Climate Data Dtore
(CDS) , 10(10.24381), 2018.
Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr ´as Hor ´anyi, Joaqu ´ın Mu ˜noz-Sabater,
Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, , Adrian Simmons, Cornel Soci,
Saleh Abdalla, Xavier Abellan, Gianpaolo Balsamo, Peter Bechtold, Gionata Biavati, Jean Bidlot,
Massimo Bonavita, Giovanna De Chiara, Per Dahlgren, Dick Dee, Michail Diamantakis, Rossana
Dragani, Johannes Flemming, Richard Forbes, Manuel Fuentes, Alan Geer, Leo Haimberger,
Sean Healy, Robin J. Hogan, El ´ıas H ´olm, Marta Janiskov ´a, Sarah Keeley, Patrick Laloyaux,
Philippe Lopez, Cristina Lupu, Gabor Radnoti, Patricia de Rosnay, Iryna Rozum, Freja Vamborg,
Sebastien Villaume, and Jean-No ¨el Th ´epaut. The ERA5 global reanalysis. Quarterly Journal of
the Royal Meteorological Society , 146(730):1999–2049, 2020.
James R. Holton. An Introduction to Dynamic Meteorology . International Geophysics Series. Else-
vier Academic Press, Burlington, MA, 4 edition, 2004. ISBN 9780123540157.
Stephan Hoyer and Joe Hamman. xarray: N-D labeled arrays and datasets in Python. Journal of
Open Research Software , 5(1):10, April 2017. doi: 10.5334/jors.148.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 4401–4410, 2019.
Ryan Keisler. Forecasting global weather with graph neural networks. arXiv preprint
arXiv:2202.07575 , 2022.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortu-
nato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexan-
der Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander
Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global
weather forecasting. Science , 0(0):eadi2336, 2023. doi: 10.1126/science.adi2336. URL
https://www.science.org/doi/abs/10.1126/science.adi2336 .
John M. Lewis. Roots of ensemble forecasting. Monthly Weather Review ,
133(7):1865 – 1885, 2005. doi: https://doi.org/10.1175/MWR2949.1. URL
https://journals.ametsoc.org/view/journals/mwre/133/7/mwr2949.1.xml .
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng
Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition , pp. 12009–12019, 2022.
Peter Lynch. The origins of computer weather prediction and climate modeling. Journal of Compu-
tational Physics , 227(7):3431–3444, 2008.
6Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
Linus Magnusson and Erland K ¨all´en. Factors influencing skill improvements in the ecmwf forecast-
ing system. Monthly Weather Review , 141(9):3142–3153, 2013.
N. Metropolis and S. Ulam. The Monte Carlo method. Journal of the American Statistical Associa-
tion, 44:335, 1949.
Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover. ClimaX:
A foundation model for weather and climate. arXiv preprint arXiv:2301.10343 , 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Ed-
ward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. Advances in Neural Information Processing Systems , 32, 2019.
Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram
Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar. FourCastNet: A global data-
driven high-resolution weather model using adaptive Fourier neural operators. arXiv preprint
arXiv:2202.11214 , 2022.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of
the IEEE/CVF International Conference on Computer Vision , pp. 4195–4205, 2023.
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual
reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 32, 2018.
Stephan Rasp and Nils Thuerey. Data-driven medium-range weather prediction with a resnet pre-
trained on climate simulations: A new model for WeatherBench. Journal of Advances in Modeling
Earth Systems , 13(2):e2020MS002405, 2021.
Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and Nils
Thuerey. WeatherBench: a benchmark data set for data-driven weather forecasting. Journal of
Advances in Modeling Earth Systems , 12(11):e2020MS002203, 2020.
Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russel, Al-
varo Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, Matthew Chantry, Zied Ben
Bouallegue, Peter Dueben, Carla Bromberg, Jared Sisk, Luke Barrington, Aaron Bell, and Fei
Sha. WeatherBench 2: A benchmark for the next generation of data-driven global weather mod-
els.arXiv preprint arXiv:2308.15560 , 2023.
Sebastian Scher. Toward data-driven weather and climate forecasting: Approximating a simple
general circulation model with deep learning. Geophysical Research Letters , 45(22):12–616,
2018.
Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Sal-
imans, Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. Metnet: A neural weather model
for precipitation forecasting. arXiv preprint arXiv:2003.12140 , 2020.
David J Stensrud. Parameterization Schemes: Keys to Understanding Numerical Weather Prediction
Models . Cambridge University Press, 2009.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Informa-
tion Processing Systems , 30, 2017.
NP Wedi, P Bauer, W Denoninck, M Diamantakis, M Hamrud, C Kuhnlein, S Malardel, K Mo-
gensen, G Mozdzynski, and PK Smolarkiewicz. The modelling infrastructure of the Integrated
Forecasting System: Recent advances and future challenges . European Centre for Medium-Range
Weather Forecasts, 2015.
7Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
Jonathan A Weyn, Dale R Durran, and Rich Caruana. Can machines learn to predict weather?
Using deep learning to predict gridded 500-hPa geopotential height from historical weather data.
Journal of Advances in Modeling Earth Systems , 11(8):2680–2693, 2019.
Jonathan A Weyn, Dale R Durran, and Rich Caruana. Improving data-driven global weather pre-
diction using deep convolutional neural networks on a cubed sphere. Journal of Advances in
Modeling Earth Systems , 12(9):e2020MS002109, 2020.
Ross Wightman. PyTorch image models. https://github.com/rwightman/pytorch-image-models ,
2019.
8Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
A I NTRODUCTION
Weather forecasting is a fundamental problem for science and society. With increasing concerns
about climate change, accurate weather forecasting helps prepare and recover from the effects of
natural disasters and extreme weather events, while serving as an important tool for researchers to
better understand the atmosphere. Traditionally, atmospheric scientists have relied on numerical
weather prediction (NWP) models (Bauer et al., 2015). These models utilize systems of differential
equations describing fluid flow and thermodynamics, which can be integrated over time to obtain
future forecasts (Lynch, 2008; Bauer et al., 2015). Despite their widespread use, NWP models suffer
from many challenges, such as parameterization errors of important small-scale physical processes,
including cloud physics and radiation (Stensrud, 2009). Numerical methods also incur high compu-
tation costs due to the complexity of integrating a large system of differential equations, especially
when modeling at fine spatial and temporal resolutions. Furthermore, NWP forecast accuracy does
not improve with more data, as the models rely on the expertise of climate scientists to refine equa-
tions, parameterizations, and algorithms (Magnusson & K ¨all´en, 2013).
To address the challenges of NWP models, there has been an increasing interest in data-driven ap-
proaches based on deep learning for weather forecasting (Dueben & Bauer, 2018; Scher, 2018; Weyn
et al., 2019). The key idea involves training deep neural networks to predict future weather condi-
tions using historical data, such as the ERA5 reanalysis dataset (Hersbach et al., 2018; 2020; Rasp
et al., 2020; 2023). Once trained, these models can produce forecasts in a few seconds, as opposed to
the hours required by typical NWP models. Because of the similar spatial structure between weather
data and natural images, early works in this space attempted to adopt standard vision architectures
such as ResNet (Rasp & Thuerey, 2021; Clare et al., 2021) and UNet (Weyn et al., 2020) for weather
forecasting, but their performances lagged behind those of numerical models. However, significant
improvements have been made in recent years due to better model architectures and training recipes,
and increasing data and compute (Keisler, 2022; Pathak et al., 2022; Nguyen et al., 2023; Bi et al.,
2023; Lam et al., 2023; Chen et al., 2023a;c). Pangu-Weather (Bi et al., 2023), a 3D Earth-Specific
Transformer model trained on 0.25◦data (721 ×1440 grids), was the first model to outperform op-
erational IFS (Wedi et al., 2015). Shortly after, GraphCast (Lam et al., 2023) scaled up the graph
neural network architecture proposed by Keisler (2022) to 0.25◦data and showed improvements
over Pangu-Weather. Despite impressive forecast accuracy, existing methods often involve complex,
highly customized neural network architectures with minimal ablation studies, making it difficult to
identify which components actually contribute to their success. For example, it is unclear what are
the benefits of 3D Earth-Specific Transformer over a standard Transformer, and how critical is the
multi-mesh message-passing in GraphCast to its performance. A deeper understanding, and ideally
a simplification, of these existing approaches is essential for future progress in the field. Further-
more, establishing a common framework would facilitate the development of foundation models for
weather and climate that extend beyond weather forecasting (Nguyen et al., 2023).
In this paper, we show that a simple architecture with a proper training recipe can perform competi-
tively with state-of-the-art methods. We start with a standard vision transformer (ViT) architecture,
and through extensive ablation studies, identify the three key components to the performance of
the model: (1) a weather-specific embedding layer that transforms the input data to a sequence of
tokens by modeling the interactions among atmospheric variables; (2) a randomized dynamics fore-
casting objective that trains the model to predict the weather dynamics at random intervals; and (3)
a pressure-weighted loss that weights variables at different pressure levels in the loss function to ap-
proximate the density at each pressure level. During inference, our proposed randomized dynamics
forecasting objective allows a single model to produce multiple forecasts for a specified lead time by
using different combinations of the intervals for which the model was trained. For example, one can
obtain a 3-day forecast by either rolling out the 6-hour predictions 12 times or 12-hour predictions
6 times. Combining these forecasts leads to significant performance improvements, especially for
long lead times. We evaluate our proposed method, namely Scalable transf ormers for weath erfore-
casting (Stormer), on WeatherBench 2 (Rasp et al., 2023), a widely used benchmark for data-driven
weather forecasting. Experiments show that Stormer achieves competitive forecast accuracy of key
atmospheric variables for 1–7 days and outperforms the state-of-the-art beyond 7 days. Notably,
Stormer achieves this performance by training on more than 5 ×lower-resolution data and orders-
of-magnitude fewer GPU hours compared to the baselines. Finally, our scaling analysis shows that
the performance of Stormer improves consistently with increases in model capacity and data size,
demonstrating the potential for further improvements.
1Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
Figure 2: Different approaches to weather forecasting. Direct and continuous methods output fore-
casts directly, but continuous forecasting is adaptable to various lead times by conditioning on T.
Iterative forecasting generates forecasts at small intervals δt, which are rolled out for the final fore-
cast. Our proposed randomized iterative forecasting combines continuous and iterative methods.
B B ACKGROUND AND PRELIMINARIES
Given a dataset D={Xi}N
i=1of historical weather data, the task of global weather forecasting is
to forecast future weather conditions XT∈RV×H×Wgiven initial conditions X0∈RV×H×W,
in which Tis the target lead time, e.g., 7 days; Vis the number of input and output atmospheric
variables, such as temperature and humidity; and H×Wis the spatial resolution of the data, which
depends on how densely we grid the globe. This formulation is similar to many image-to-image
tasks in computer vision such as segmentation or video frame prediction. However, unlike the RGB
channels in natural images, weather data can contain up to 100s of channels. These channels repre-
sent actual physical variables that can be unbounded in values and follow complex laws governed by
atmospheric physics. Therefore, the ability to model the spatial and temporal correlations between
these variables is crucial to forecasting.
There are three major approaches to data-driven weather forecasting. The first and simplest is direct
forecasting , which trains the model to directly output future weather bXT=fθ(X0)for each target
lead time T. Most early works in the field adopt this approach (Dueben & Bauer, 2018; Scher,
2018; Weyn et al., 2019; Rasp & Thuerey, 2021; Clare et al., 2021; Weyn et al., 2020). Since
the weather is a chaotic system, forecasting the future directly for large Tis challenging, which
may explain the poor performances of these early models. Moreover, direct forecasting requires
training one neural network for each lead time, which can be computationally expensive when the
number of target lead times increases. To avoid the latter issue, continuous forecasting usesTas
an additional input: bXT=fθ(X0, T), allowing a single model to produce forecasts at any target
lead time after training. MetNet (Sønderby et al., 2020; Espeholt et al., 2022; Andrychowicz et al.,
2023) employed the continuous approach for nowcasting at different lead times up to 24hours,
WeatherBench (Rasp & Thuerey, 2021) considered continuous forecasting as one of the baselines,
and ClimaX (Nguyen et al., 2023) used this approach for pretraining. However, since this approach
still attempts to forecast future weather directly, it suffers from the same challenging problem of
forecasting the chaotic weather in one step. Finally, iterative forecasting trains the model to produce
forecasts at a small interval bXδt=fθ(X0), in which δtis typically from 6 to 24 hours. To produce
longer-horizon forecasts, we roll out the model by iteratively feeding its predictions back in as
input. This is a common paradigm in both traditional NWP systems and the two state-of-the-art
deep learning methods, Pangu-Weather and GraphCast. One drawback of this approach is error
accumulation when the number of rollout steps increases, which can be mitigated by a multi-step
loss function (Keisler, 2022; Lam et al., 2023; Chen et al., 2023a;c). In iterative forecasting, one
can forecast either the weather conditions Xδtor the weather dynamics ∆δt=Xδt−X0, and Xδt
can be recovered by adding the predicted dynamics to the initial conditions. In this work, we adopt
the latter approach, which we refer to as iterative dynamics forecasting . We show empirically that
our approach achieves superior performance relative to the former approach in Section F.2. Figure 2
summarizes these different approaches.
2Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
C R ELATED WORK
Deep learning offers a promising approach to weather forecasting due to its fast inference and high
expressivity. Early efforts (Dueben & Bauer, 2018; Scher, 2018; Weyn et al., 2019) attempted train-
ing simple architectures on small weather datasets. To facilitate progress in the field, Weather-
Bench (Rasp et al., 2020) provided standard datasets and benchmarks, leading to subsequent works
that trained Resnet (He et al., 2016) and UNet architectures (Weyn et al., 2020) for weather fore-
casting. These works demonstrated the potential of deep learning but still displayed inferior forecast
accuracy to numerical systems. However, significant improvements have been made in the last few
years. Keisler (2022) proposed a graph neural network (GNN) that performs iterative forecasting
with6-hour intervals, performing comparably with some NWP models. FourCastNet (Pathak et al.,
2022) trained an adaptive Fourier neural operator and was the first neural network to run on 0.25◦
data. Pangu-Weather (Bi et al., 2023), with its 3D Earth-Specific Transformer design, trained on
high-resolution data, surpassed the benchmark IFS model. Following this, GraphCast (Lam et al.,
2023) scaled up Keisler’s GNN architecture to 0.25◦, outperforming Pangu-Weather. FuXi (Chen
et al., 2023b) was a subsequent work that trained a SwinV2 (Liu et al., 2022) on 0.25◦data and
showed improvements over GraphCast at long lead times. However, FuXi requires finetuning mul-
tiple models specialized for different time ranges, increasing model complexity and computation.
FengWu (Chen et al., 2023a) was a concurrent work with FuXi that also focused on improving long-
horizon forecasts, but has not revealed complete details about their model architecture and training.
D D ISCUSSION
D.1 R ANDOMIZED FORECASTING
We train Stormer to forecast the dynamics at random intervals δtby conditioning on δtaccording
to Equation (1). From a practical standpoint, this randomized objective provides two main benefits.
First, by randomizing δt, it enlarges the training data, serving as a form of data augmentation.
Second, it enables a single model, once trained, to generate various forecasts for a specified lead
timeT. This is achieved by creating different combinations of the intervals δtused in training
to sum up to T. For instance, to predict weather conditions 7 days ahead, one could roll out the
12-hour forecasts 14 times or the 24-hour forecasts 7 times. Our experiments demonstrate that the
amalgamation of these forecasts is crucial for achieving good forecast accuracy, particularly for
longer lead times. We note that while both our approach and the continuous approach use the time
interval as an additional input to the model, we perform iterative forecasting as opposed to direct
forecasting of the counterpart. This avoids the challenge of modeling the chaotic weather directly,
as well as offers more flexibility for combining different intervals at test time.
D.2 I NFERENCE
Due to the randomized forecasting objective, Stormer can generate multiple forecasts for a target
lead time Tby creating different combinations of δtthat sum to T. We consider two inference
strategies for generating forecasts, homogeneous andbestminn. The two strategies offer a trade-
off between efficiency and expressivity. The homogeneous strategy only requires running three
combinations for each lead time T, while best minnprovides greater expressivity. Upon determin-
ing these combinations and executing the model rollouts, we obtain the final forecast by averaging
the individual predictions. This achieves a similar effect to ensembling in NWP, where multiple
forecasts are generated by running NWP models with different perturbed versions of the initial con-
dition (Lewis, 2005). As target lead times extend beyond 5–7 days and individual forecasts begin
to diverge due to the chaotic nature of the atmosphere, averaging these forecasts is a Monte Carlo
integration approach to handle this sensitivity to initial conditions and the uncertainty in the analyses
used as initial conditions (Metropolis & Ulam, 1949). We note that our inference strategy is distin-
guished from that used in Pangu-Weather. While Pangu-Weather trains a separate model for each
time interval δt, we train a single model for all δtvalues by conditioning on δt. Additionally, while
Pangu-Weather relies on a single combination of intervals to minimize rollout steps, our method
improves forecast accuracy by averaging multiple forecasts derived from diverse combinations.
3Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
D.3 W EATHER -SPECIFIC EMBEDDING
To capture the complex interactions among input atmospheric variables, we adopt for Stormer a
weather-specific embedding module, consisting of two components, variable tokenization andvari-
able aggregation . Figure 6b compares weather-specific embedding with standard patch embedding,
which shows the superior performance of weather-specific embedding at all lead times from 1 to
10 days. A similar weather-specific embedding module was introduced by ClimaX (Nguyen et al.,
2023) to improve the model’s flexibility when handling diverse data sources with heterogeneous
input variables. We show that this specialized embedding module outperforms the standard patch
embedding even when trained on a single dataset, due to its ability to effectively model interactions
between atmospheric variables through cross-attention.
E E XPERIMENT DETAILS
E.1 B ASELINES
We compare the forecast performance of Stormer with Pangu-Weather (Bi et al., 2023) and Graph-
Cast (Lam et al., 2023), two leading deep learning methods for weather forecasting. Pangu-Weather
employs a 3D Earth-Specific Transformer architecture trained on the same variables as Stormer, but
with hourly data and a higher spatial resolution of 0.25◦. GraphCast is a graph neural network that
was trained on 6-hourly ERA5 data at 0.25◦, using 37 pressure levels for the atmospheric variables,
and two additional variables, total precipitation and vertical wind speed. Both Pangu-Weather and
GraphCast are iterative methods. GraphCast operates at 6-hour intervals, while Pangu-Weather uses
four distinct models for 1-, 3-, 6-, and 24-hour intervals, and combines them to produce forecasts for
specific lead times. We include Climatology as a simple baseline. We also compare Stormer with
IFS HRES, the state-of-the-art numerical forecasting system, and IFS ENS (mean), which is the
ensemble version of IFS. Since WB2 does not provide forecasts of these numerical models beyond
10days, we defer the comparison against these models to Appendix F.1.
E.2 S TORMER ARCHITECTURE
Figure 3 illustrates the architecture of Stormer. The variable tokenization module tokenizes each
variable of the input X0∈RV×H×Wseparately, resulting in a sequence of V×(H/p)×(W/p)
tokens, where pis the patch size. The variable aggregation module then performs cross-attention
over the variable dimension and outputs a sequence of (H/p)×(W/p)tokens. The interval δt
is embedded and fed to the Stormer backbone together with the tokens. The output of the last
Stormer block is then passed through a linear layer and reshaped to produce the prediction ∆δt.
Each Stormer block employs adaptive layer normalization to condition on additional information
from δt. Specifically, the scale and shift parameters (γ1, β1)and(γ2, β2)are output by an MLP
which takes δtembedding as input. This MLP network additionally outputs α1andα2to scale the
output of the attention and fully connected layers, respectively.
For the main comparison in Section 2, we report the results of our largest Stormer model with 24
transformer blocks, 1024 hidden dimensions, and a patch size of 2, which is equivalent to ViT-L
except for the smaller patch size. We vary the model size and patch size in the scaling analysis. For
the remaining experiments, we report the performance of the same model as for the main result, but
with a larger patch size of 4 for faster training.
In all experiments, the variable tokenization module is a standard patch embedding layer usually
used in ViT, and the aggregation module is a single-layer multi-head cross-attention. The first em-
bedding of δtis a linear layer, and the adaLN module in each block employs a 2-layer MLP.
For the main comparison with the current methods, we train a Stormer model with a patch size of 2,
1024 hidden dimensions, and 24Stormer blocks. For the scaling experiments, we vary the hidden
dimensions, number of blocks, and patch size. For the rest of the ablation studies, we use a patch
size of 4, hidden dimension of 1024 , and 24blocks.
4Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
Figure 3: Stormer architecture. The initial condition goes through tokenization and aggregation,
before being fed to a stack of NStormer blocks together with δt. Each Stormer block employs
adaptive layer norm to condition on δt.
E.3 T RAINING AND EVALUATION DETAILS
E.3.1 D ATA NORMALIZATION
Input normalization We compute the mean and standard deviation for each variable in the input
across all spatial positions and all data points in the training set. This means each variable is associ-
ated with a scalar mean and scalar standard deviation. During training, we standardize each variable
by subtracting it from the associated mean and dividing it by the standard deviation.
Output normalization Unlike the input, the output that the model learns to predict is the differ-
ence between two consecutive steps. Therefore, for each variable, we compute the mean and stan-
dard deviation of the difference between two consecutive steps in the training set. What it means to
be ”consecutive” depends on the time interval δt. Ifδt= 6, we collect all pairs in training data that
are 6-hour apart, compute the difference between two data points in each pair, and then compute the
mean and standard deviation of these differences. Since we train Stormer with randomized δt, we
repeat the same process for each value of δt.
E.3.2 T HREE -PHASE TRAINING
As mentioned in Section 1, we train Stormer in three phases with the following objective:
L(θ) =E
1
KV HWKX
k=1VX
v=1HX
i=1WX
j=1w(v)L(i)(b∆vij
kδt−∆vij
kδt)2
, (4)
where the number of rollout steps Kis equal to 1,4, and 8in phase 1,2, and 3, respectively. For
phases 2and3, we finetune the best checkpoint from the preceding phase.
5Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
E.3.3 P RESSURE WEIGHTS
For pressure-level variables, we assign weights proportionally to the pressure level of each variable.
For4surface variables, we assign w= 1 for T2m and w= 0.1for the remaining variables U10,
V10, and MSLP. The surface weights were proposed by GraphCast (Lam et al., 2023) and we did
not perform any additional hyperparameter tuning.
E.3.4 O PTIMIZATION
For the main result in Section 2, we train Stormer in three phases, as described in Section 1.1.2.
For the 1st phase, we train the model for 100epochs. We optimize the model using AdamW (Kingma
& Ba, 2014) with learning rate of 5e−4, parameters (β11 = 0 .9, β2= 0.95)and weight decay of
1e−5. We used a linear warmup schedule for 10epochs, followed by a cosine schedule for 90
epochs.
For the 2nd and 3rd phases, we train the model for 20epochs with a learning rate of 5e−6and
5e−7, respectively. We used a linear warmup schedule for 5epochs, followed by a cosine schedule
for15epochs. Other hyperparameters remain the same.
We perform early stopping for all phases, where the criterion is the validation loss aggregated across
all variables at lead times of 1day,3days, and 5days for phases 1,2, and 3, respectively. We save
the best checkpoint for each phase using the same criterion. For the remaining experiments, we only
train Stormer for the first phase due to computational constraints.
E.3.5 S OFTWARE AND HARDWARE STACK
We use PyTorch (Paszke et al., 2019), Pytorch Lightning (Falcon, 2019), timm (Wightman, 2019),
numpy (Harris et al., 2020) and xarray (Hoyer & Hamman, 2017) for data processing and model
training. We trained Stormer on 128 40 GB A 100devices. We leverage mixed-precision training,
Fully Sharded Data Parallel, and gradient checkpointing to reduce memory.
E.3.6 E VALUATION PROTOCOL
We evaluate Stormer and the baselines on forecasting nine key variables: T2m, U10, V10, MSLP,
Z500, T850, Q700, U850, and V850. These variables are also used to report the headline scores in
WB2. For each variable, we evaluate the forecast accuracy at lead times from 1 to 14 days, using
the latitude-weighted root-mean-square error (RMSE) metric. For the main results, we use best m
inninference for rolling out Stormer as it yields the best result, with m= 32 and n= 128 chosen at
random from all possible combinations.
As different models are trained on different resolutions of data, we follow the practice in WB2 to
regrid the forecasts of all models to the same resolution of 1.40625◦(128×256grid points). We
then calculate evaluation metrics on this shared resolution. Similarly to WB2, we evaluate forecasts
with initial conditions at 00/12UTC for all days in 2020. We provide additional metrics and results
of Stormer in Appendix F. For the remaining experiments, we use homogeneous inference for
efficiency.
F A DDITIONAL RESULTS
F.1 C OMPARISON WITH SOTA MODELS
Figure 4 compares Stormer with both deep learning and numerical methods. We take IFS and IFS
ENS from WB2 which is only available until day 10. Similar to its deep learning counterparts,
Stormer achieves lower RMSE compared to the IFS model for most variables, except for near-
surface temperature (T2m) at initial lead times, and only performs slightly worse than IFS ENS. To
the best of our knowledge, Stormer is the first model trained on 1.40625◦data to surpass IFS.
Additionally, we compare Stormer and the baselines on latitude-weighted ACC, another common
verification metric for weather forecast models. ACC represents the Pearson correlation coefficient
6Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
2 4 6 8 10 12 140.51.01.52.02.53.03.5
T2m (K)
2 4 6 8 10 12 1412345
U10m (m/s)
2 4 6 8 10 12 1412345
V10m (m/s)
2 4 6 8 10 12 14200400600800RMSE
MSLP (Pa)
2 4 6 8 10 12 1402004006008001000
Z500 (m2/s2)
2 4 6 8 10 12 141234
T850 (K)
2 4 6 8 10 12 140.500.751.001.251.501.752.00
Q700 (g/kg)
2 4 6 8 10 12 14
Lead time (days)1234567
U850 (m/s)
2 4 6 8 10 12 141234567
V850 (m/s)
Climatology Pangu-Weather GraphCast Stormer IFS HRES IFS ENS (mean)
Figure 4: Global forecast verification results of Stormer and the baselines from 1- to14-day lead
times. We show the latitude-weighted RMSE for select variables. Stormer is on par or outperforms
each of the benchmark models for the shown variables. During the later portion of the forecasts,
Stormer significantly outperforms the current methods.
between forecast anomalies relative to climatology and ground truth anomalies relative to clima-
tology. ACC ranges from −1to1, where 1indicates perfect correlation, and −1indicates perfect
anti-correlation. We refer to WB2 (Rasp et al., 2023) for the formulation of ACC. Figure 5 shows
that similarly to RMSE, Stormer achieves competitive performance from 1to5days, and outper-
forms the baselines by a large margin beyond 6days.
F.2 A BLATION STUDIES
We analyze the significance of individual elements within Stormer by systematically omitting one
component at a time and observing the difference in performance. First, Figure 6a shows the forecast
performance of different single-interval models, which shows that while a small interval works well
for short lead times, a larger interval excels at long-term forecasts. This motivates our randomized
forecasting objective which trains a model that can produce forecasts at multiple intervals. Figure 6b
and 6c demonstrate the importance of weather-specific embedding and adaptive layer normalization
(adaLN) to the performance of Stormer.
Impact of randomized forecasts: We evaluate the effectiveness of our proposed randomized it-
erative forecasting approach. Figure 7a compares the forecast accuracy on surface temperature of
Stormer and three models trained with different values of δt. Stormer consistently outperforms all
single-interval models at all lead times, and the performance gap widens as the lead time increases.
We attribute this result to the ability of Stormer to produce multiple forecasts and combine them to
improve accuracy. We note that Stormer achieves this improvement with no computational overhead
compared to the single-interval models, as the different models share the same architecture and were
trained for the same duration.
7Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
2 4 6 8 10 12 140.40.60.81.0
T2m (K)
2 4 6 8 10 12 140.20.40.60.81.0
U10m (m/s)
2 4 6 8 10 12 140.20.40.60.81.0
V10m (m/s)
2 4 6 8 10 12 140.20.40.60.81.0ACC
MSLP (Pa)
2 4 6 8 10 12 140.30.40.50.60.70.80.91.0
Z500 (m2/s2)
2 4 6 8 10 12 140.20.40.60.81.0
T850 (K)
2 4 6 8 10 12 140.20.40.60.8
Q700 (g/kg)
2 4 6 8 10 12 14
Lead time (days)0.20.40.60.81.0
U850 (m/s)
2 4 6 8 10 12 140.20.40.60.81.0
V850 (m/s)
Pangu-Weather GraphCast Stormer IFS HRES IFS ENS (mean)
Figure 5: Global forecast verification results of Stormer and the baselines from 1- to14-day lead
times. We show the latitude-weighted ACC for select variables. Stormer is on par or outperforms
each of the benchmark models for the shown variables. During the later portion of the forecasts,
Stormer significantly outperforms the current methods.
2 4 6 8 10
Lead time (days)1.01.52.02.53.0RMSE
T2m (K)
t=6
 t=24
(a) Time interval comparison.
2 4 6 8 10
Lead time (days)0.751.001.251.501.752.002.252.502.75RMSE
T2m (K)
ViT Embedding Weather Embedding (b) Patch embedding comparison.
2 4 6 8 10
Lead time (days)0.751.001.251.501.752.002.252.50RMSE
T2m (K)
Additive Embedding AdaLN (c) Lead time embed comparison.
Figure 6: Preliminary results on surface temperature forecasting that led to the design choices of
Stormer: (a) δt= 6 works well for small lead times, but δt= 24 excels at lead times beyond 7 days.
(b) Weather-specific embedding significantly outperforms standard ViT embedding. (c) Adaptive
layer norm outperforms additive embedding. Similar trends are observed across different output
variables.
Impact of pressure-weighted loss: Figure 7b shows the superior performance of Stormer when
trained with the pressure-weighted loss. Intuitively, the weighting factor prioritizes variables that
are nearer to the surface, as these variables are more important for weather forecasting and climate
science. The pressure-weighted loss was first introduced by GraphCast, and we show that it also
helps with a different architecture.
8Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
2 4 6 8 10
Lead time (days)0.51.01.52.02.53.0RMSE
T2m (K)
t=6
 t=12
 t=24
 Stormer
(a) Impact of randomized forecasting.
2 4 6 8 10
Lead time (days)1.01.52.02.5RMSE
T2m (K)
Unweighted Weighted (b) Impact of weighted loss.
2 4 6 8 10
Lead time (days)0.751.001.251.501.752.002.252.50RMSE
T2m (K)
Xt forecast
 t forecast
 (c) Absolute vs dynamics forecast.
Figure 7: Ablation studies showing the importance of each component in Stormer: (a) Stormer
outperforms single-interval models at all lead times. (b) Pressure-weighted loss improves accuracy
significantly. (c) Dynamics forecasting is consistently better than absolute forecast. Similar trends
are observed across different output variables.
2 4 6 8 100.51.01.52.0
T2m (K)
2 4 6 8 101.01.52.02.53.03.54.0
U10m (m/s)
2 4 6 8 101.01.52.02.53.03.54.0
V10m (m/s)
2 4 6 8 10100200300400500600RMSE
MSLP (Pa)
2 4 6 8 10100200300400500600700
Z500 (m2/s2)
2 4 6 8 100.51.01.52.02.53.0
T850 (K)
2 4 6 8 100.60.81.01.21.41.6
Q700 (g/kg)
2 4 6 8 10
Lead time (days)12345
U850 (m/s)
2 4 6 8 1012345
V850 (m/s)
K=1 K=8
Figure 8: Performance of Stormer without ( K= 1) and with ( K= 8) multi-step fine-tuning.
Dynamics vs. absolute forecasts: We justify our decision to forecast the dynamics ∆δtby compar-
ing with a counterpart that forecasts Xδt. Figure 7c shows that forecasting the changes in weather
conditions (dynamics) is consistently more accurate than predicting complete weather states. One
possible explanation for this result is that it is simpler for the model to predict the changes between
two consecutive weather conditions than the entire state of the weather; thus, the model can focus
on learning the most significant signal, enhancing forecast accuracy.
Impact of multi-step fine-tuning We verify the importance of multi-step fine-tuning by comparing
Stormer after the 1st phase ( K= 1) and after the 3rd phase ( K= 8). Figure 8 shows that multi-step
fine-tuning significantly improves performance at long lead times.
9Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
2 4 6 8 10
Lead time (days)0246810121416RMSE
T2m (K)
Stormer-S Stormer-B Stormer-L
2 4 6 8 10
Lead time (days)0.51.01.52.02.5RMSE
T2m (K)
p=16 p=8 p=4 p=2
Figure 9: Stormer improves consistently with larger models (left) and smaller patch sizes (right).
F.3 S CALING ANALYSIS
We examine the scalability of Stormer with respect to model size and the number of training tokens.
We evaluate three variants of Stormer – Stormer-S, Stormer-B, and Stormer-L, whose parameter
counts are similar to ViT-S, ViT-B, and ViT-L, respectively. To understand the impact of training
token count, we vary the patch size from 2to16. The number of training tokens increases fourfold
whenever the patch size is halved. Figure 9 shows a significant improvement in forecast accuracy
when we increase the model size, and the performance gap widens as we increase the lead time.
Since we do not perform multi-step fine-tuning for these models, minor performance differences
at short intervals may become magnified over time. While multi-step fine-tuning could potentially
reduce this gap, it is unlikely to eliminate it entirely. Reducing the patch size also improves the
performance of the model consistently. From a practical view, smaller patches mean more tokens
and consequently more training data. From a climate perspective, smaller patches capture finer
weather details and processes not evident in larger patches, allowing the model to more effectively
capture physical dynamics that drive weather patterns.
F.4 Q UALITATIVE RESULTS
We visualize forecasts produced by Stormer at lead times from 1days to 14days for 9key variables.
All forecasts are initialized at 0UTC January 26th 2020. Each figure illustrates one lead time, where
each row is for each variable. The first column shows the initial condition, the second column shows
the ground truth at that lead time, the third column shows the forecast, and the last column shows
the bias, which is the difference between the forecast and the ground truth.
10Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
T2mIC (Jan 26th 2020)
 Ground truth
 1-day Forecast
 Error (Forecast - Truth)
U10m
 V10m
 MSLP
 Z500
 Q700
 T850
 U850
 V850
220240260280300
220240260280300
220240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
950975100010251050
950975100010251050
950975100010251050
20
10
01020
hPa
475005000052500550005750060000
475005000052500550005750060000
475005000052500550005750060000
2000
1000
010002000
m2/s2
051015
051015
051015
4
2
024
g/kg
240260280300
240260280300
240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
40
20
02040
40
20
02040
40
20
02040
20
10
01020
m/s
Figure 10: 1-day lead time
11Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
T2mIC (Jan 26th 2020)
 Ground truth
 3-day Forecast
 Error (Forecast - Truth)
U10m
 V10m
 MSLP
 Z500
 Q700
 T850
 U850
 V850
220240260280300
220240260280300
220240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
950975100010251050
950975100010251050
950975100010251050
20
10
01020
hPa
475005000052500550005750060000
475005000052500550005750060000
475005000052500550005750060000
2000
1000
010002000
m2/s2
051015
051015
051015
4
2
024
g/kg
240260280300
240260280300
240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
40
20
02040
40
20
02040
40
20
02040
20
10
01020
m/s
Figure 11: 3-day lead time
12Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
T2mIC (Jan 26th 2020)
 Ground truth
 5-day Forecast
 Error (Forecast - Truth)
U10m
 V10m
 MSLP
 Z500
 Q700
 T850
 U850
 V850
220240260280300
220240260280300
220240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
950975100010251050
950975100010251050
950975100010251050
20
10
01020
hPa
475005000052500550005750060000
475005000052500550005750060000
475005000052500550005750060000
2000
1000
010002000
m2/s2
051015
051015
051015
4
2
024
g/kg
240260280300
240260280300
240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
40
20
02040
40
20
02040
40
20
02040
20
10
01020
m/s
Figure 12: 5-day lead time
13Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
T2mIC (Jan 26th 2020)
 Ground truth
 7-day Forecast
 Error (Forecast - Truth)
U10m
 V10m
 MSLP
 Z500
 Q700
 T850
 U850
 V850
220240260280300
220240260280300
220240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
950975100010251050
950975100010251050
950975100010251050
20
10
01020
hPa
475005000052500550005750060000
475005000052500550005750060000
475005000052500550005750060000
2000
1000
010002000
m2/s2
051015
051015
051015
4
2
024
g/kg
240260280300
240260280300
240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
40
20
02040
40
20
02040
40
20
02040
20
10
01020
m/s
Figure 13: 7-day lead time
14Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
T2mIC (Jan 26th 2020)
 Ground truth
 10-day Forecast
 Error (Forecast - Truth)
U10m
 V10m
 MSLP
 Z500
 Q700
 T850
 U850
 V850
220240260280300
220240260280300
220240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
950975100010251050
950975100010251050
950975100010251050
20
10
01020
hPa
475005000052500550005750060000
475005000052500550005750060000
475005000052500550005750060000
2000
1000
010002000
m2/s2
051015
051015
051015
4
2
024
g/kg
240260280300
240260280300
240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
40
20
02040
40
20
02040
40
20
02040
20
10
01020
m/s
Figure 14: 10-day lead time
15Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024
T2mIC (Jan 26th 2020)
 Ground truth
 14-day Forecast
 Error (Forecast - Truth)
U10m
 V10m
 MSLP
 Z500
 Q700
 T850
 U850
 V850
220240260280300
220240260280300
220240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
950975100010251050
950975100010251050
950975100010251050
20
10
01020
hPa
475005000052500550005750060000
475005000052500550005750060000
475005000052500550005750060000
2000
1000
010002000
m2/s2
051015
051015
051015
4
2
024
g/kg
240260280300
240260280300
240260280300
20
10
01020
K
20
10
01020
20
10
01020
20
10
01020
20
10
01020
m/s
40
20
02040
40
20
02040
40
20
02040
20
10
01020
m/s
Figure 15: 14-day lead time
16