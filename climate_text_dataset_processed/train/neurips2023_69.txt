A Scalable Network-Aware Multi-Agent
Reinforcement Learning Framework for Distributed
Converter-based Microgrid Voltage Control
Han Xu
Department of Electrical Engineering
Tsinghua University
xuhan21@mails.tsinghua.edu.cnGuannan Qu
Department of Electrical and Computer Engineering
Carnegie Mellon University
gqu@andrew.cmu.edu
Abstract
Renewable energy plays a crucial role in mitigating climate change. With the rising
use of distributed energy resources (DERs), microgrids (MGs) have emerged as
a solution to accommodate high DER penetration. However, controlling MGs’
voltage during islanded operation is challenging due to system’s nonlinearity and
stochasticity. Although multi-agent reinforcement learning (MARL) methods have
been applied to distributed MG voltage control, they suffer from bad scalability
and are found difficult to control the MG with a large number of DGs due to
the well-known curse of dimensionality. To address this, we propose a scalable
network-aware reinforcement learning framework which exploits network structure
to truncate the critic’s Q-function to achieve scalability. Our experiments show
effective control of a MG with up to 84 DGs, surpassing the existing maximum of
40 agents in the existing literature. We also compare our framework with state-of-
the-art MARL algorithms to show the superior scalability of our framework.
1 Introduction
Renewable energy plays a vital role in mitigating climate change by reducing greenhouse gas
emissions and promoting energy resilience and efficiency Osman et al. (2023). The rising utilization
of renewable energy has catalyzed the transformation of power grids, leading to an increasing number
of distributed generations (DGs). To accommodate DGs such as photo-voltaic, fuel-cells and wind
turbines, Microgrids (MGs) have emerged as a promising solution Lasseter, Paigi (2004) where DGs
can be connected to MGs through power electronics converters and the renewable energy is flexibly
distributed to the local loads and main grids Pogaku et al. (2007); Banerji et al. (2013). In this way,
MGs can facilitate DGs and high penetration of renewable energy.
However, the MG voltage control during islanded operation presents unique challenges. To maintain
voltage of the MG at predefined nominal values, secondary control methods are necessary Olivares
et al. (2014); Rajesh et al. (2017); Khayat et al. (2020). To handle the large number of DGs in MG,
distributed secondary control methods are needed to make control only based on local and neighbor
information Bidram et al. (2013); Lu et al. (2018). However, the nonlinear dynamics of MGs and the
uncertainty associated with loads and renewable energies make the design of model-based distributed
control methods challenging, thereby negatively impacting the control performance.
Having achieved great success in complex control tasks like game playing Mnih et al. (2013);
Nguyen, La (2019), robot manipulation Kober et al. (2013) , reinforcement learning (RL), a model-
free approach, has been recognized to have great potential for distributed secondary control Chen
et al. (2022b,a); Hossain et al. (2022); Yang et al. (2022); Wang et al. (2023). More specifically, for
control tasks involving multiple DGs as in our setting, multi-agent RL (MARL) is needed. However,
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.Figure 1: Basic components in micorgrid.
the dominant paradigm in MARL is Centralized Training and Distributed Execution (CTDE). In
this framework, the Q-function of the critics needs to take the global observations and actions to
estimate expected rewards and train individual actors. The size of the observation and action spaces
exponentially increase with the number of agents rendering the lack of scalability of this framework
Canese et al. (2021); Qu et al. (2021). Therefore, when the number of DGs (e.g., agents) is large,
it becomes difficult to effectively train the critics whose input size increases exponentially with the
number of agents and the eventual training performance is not ideal. For this reason, the number of
agents in existing literature on inverter control can only reach 40 Chen et al. (2022a).
Contribution. We aim to address this scalability issue and propose a novel scalable network-aware
framework. The highlight of our model is that it leverages network structure to truncate the critic’s
Q-function, and decomposes the global reward to achieve scalability. Our framework is theoretically
grounded, as we show exponential decay property of the Q-function in MG voltage control problem.
Further, experiments confirm that our framework can effectively control an MG containing up to 84
DGs. To our knowledge, this number surpasses the current maximum number of agents—40 reported
in existing literature Chen et al. (2022a). This indicates the potential of our method to efficiently
control MGs that contain numerous DGs.
Apart from voltage control problem, our framework can also be applied to solve distributed frequency
and energy management problems Chen et al. (2022c) to ensure the safe and efficient operation of
MGs with high penetration of renewable energy. We envision our approach to serve as a critical
technical tool to enable greater integration of renewable energy, which is vital for tackling climate
change.
2 Problem Formulation and Methodology
2.1 Problem Formulation
In MGs, there are three basic components: loads, DGs, and transmission lines. Loads and DGs
are connected via transmission lines, as depicted in Figure 1 (a). Loads typically have stochastic
variations. Nondispatchable DGs that generate power based on weather conditions are also modeled
as stochastic loads Hou et al. (2022). The other DGs are controlled to maintain voltage in the MG.
As shown in Figure 1 (c), the control part of DGs includes primary and secondary control Bidram,
Davoudi (2012). The primary control can help stabilize the voltage but cannot alone maintain them to
the nominal values Guerrero et al. (2011). Therefore, secondary control is necessary. In our approach,
we use neural networks that only take local measurements as the distributed secondary control.
2.2 Methodology
2.2.1 Networked MDP formulation
In our approach, we model the MG as a networked Markov Decision Process (MDP) Qu et al.
(2021)where interactions between nodes occur in a localized manner meaning the next state of a
given node idepends solely on its own current state, the states of its neighbors, and its own action. In
2Figure 2: Networked MDP and exponential decay property.
Figure 3: CTDE and scalable network-aware framewrok comparison.
MG, each bus connected with a DG or a load is considered as a node. Intuitively, the MG modeled as
a networked MDP can justified by the delay effect of transmission lines Christopoulos (2022). In
addition, the reasonableness of modeling as a networked MDP can be also justified by analyzing
relationships between state variables within the MG system, as shown in Appendix A.
2.2.2 Scalable network-aware framework
Building in the networked MDP formulation, we demonstrate how to exploit the local property of the
networked MDP to design a scalable network-aware actor-critic framework. By leveraging the local
properties of the networked MDP, we derive the exponential decay property of the Q-function, which
implies that the Q-function’s dependence on distant nodes shrinks exponentially with their distance,
as shown in Figure 2. The Q-function here refers to the individual Q-function used to estimate each
node’s reward. A detailed description can be found in Appendix B. With this property, we propose
the following network aware actor-critic based framework that is scalable to implement.
Critics that take only states and actions of κ-hop neighbors:
Using the exponential decay property, we define a class of truncated Q-functions, which depend only
on the states and actions of the local and κ-hop neighbors. Due to the exponential decay property,
such truncated Q-functions are still good approximations to the original Q-function. Moreover, since
their input dimension does not increase with the number of agents, they exhibit good scalability and
can still be effectively trained even if the MG scale is large.
Actors that are guided by critics of its κ-hop neighbors:
As mentioned above, each critic’s Q-function only estimates individual rewards. To enhance coopera-
tion among agents and improve global welfare, each local actor is guided by the critics of its κ-hop
neighbors to ensure that its policy can increase the shared rewards of itself and its κ-hop neighbors.
Compared with the conventional CTDE framework shown in Figure 3 (a), the proposed framework,
as shown in Figure 3 (b), exploits the network structure and trains all the actors in a distributed way
to achieve better scalability.
3Figure 4: Training results comparison.
3 Results
As depicted in Figure 3, the proposed framework can be combined with a variety of actor-critic
algorithms. In this paper, we combine our proposed framework with the multi-agent Soft Actor-Critic
(MA-SAC) algorithm Haarnoja et al. (2018), a state-of-the-art reinforcement learning algorithm for
continuous control problems. The pseudocode can be found in Appendix C.
To test our framework, we have built a simulation platform based on the dynamic model of the MG.
This platform can generate dynamic models of MGs by reading standard Matpower data Zimmerman
et al. (2011). In the environment, the loads have random changes within 20%, the control period is
0.025s, and the control action is the voltage set point given to the primary control.
To demonstrate the scalability of the framework we propose, we tested three cases: IEEE 14-bus
with 6 DGs (e.g., agents), IEEE 118-bus with 54 DGs, and 118-bus with 84 DGs. In these cases, we
compared it with MA-SAC and MA-TD3 Dankwa, Zheng (2020) algorithms using CTDE framework.
As can be observed in Figure 4, the three algorithms show relatively similar performance in the case
having 6 DGs. However, as the number of DGs increases, the performance of the scalable network-
aware framework significantly outperforms the CTDE algorithms, demonstrating the scalability of
our proposed framework.
4 Conclusions and Future Wrok
In this paper, we proposed a scalable network-aware reinforcement learning framework that utilizes
network structure to truncate the critic’s Q-function, thereby achieving scalability. Our experiments
have demonstrated effective control of an MG with up to 84 DGs, surpassing the existing maximum
of 40 agents reported in the current literature. Moreover, when compared with state-of-the-art MARL
algorithms, our proposed framework has proven its superior scalability. This lays a solid foundation
for future research and practical applications in large-scale microgrid control. In the future, we
will combine our proposed framework with other actor-critic RL algorithms and solve other control
problems in MGs to fully show the potential of our proposed framework.
References
Banerji Ambarnath, Sen Debasmita, Bera Ayan K., Ray Debtanu, Paul Debjyoti, Bhakat Anurag,
Biswas Sujit K. Microgrid: A Review // 2013 IEEE Global Humanitarian Technology Conference:
South Asia Satellite (GHTC-SAS). VIII 2013. 27–35.
Bidram Ali, Davoudi Ali . Hierarchical Structure of Microgrids Control System // IEEE Transactions
on Smart Grid. XII 2012. 3, 4. 1963–1976.
4Bidram Ali, Davoudi Ali, Lewis Frank L., Guerrero Josep M. Distributed Cooperative Secondary
Control of Microgrids Using Feedback Linearization // IEEE Transactions on Power Systems. VIII
2013. 28, 3. 3462–3470.
Canese Lorenzo, Cardarilli Gian Carlo, Di Nunzio Luca, Fazzolari Rocco, Giardino Daniele,
Re Marco, Spanò Sergio . Multi-Agent Reinforcement Learning: A Review of Challenges and
Applications // Applied Sciences. I 2021. 11, 11. 4948.
Chen Dong, Chen Kaian, Li Zhaojian, Chu Tianshu, Yao Rui, Qiu Feng, Lin Kaixiang . PowerNet:
Multi-Agent Deep Reinforcement Learning for Scalable Powergrid Control // IEEE Transactions
on Power Systems. III 2022a. 37, 2. 1007–1017.
Chen Pengcheng, Liu Shichao, Chen Bo, Yu Li . Multi-Agent Reinforcement Learning for Decen-
tralized Resilient Secondary Control of Energy Storage Systems Against DoS Attacks // IEEE
Transactions on Smart Grid. V 2022b. 13, 3. 1739–1750.
Chen Xin, Qu Guannan, Tang Yujie, Low Steven, Li Na . Reinforcement Learning for Selective Key
Applications in Power Systems: Recent Advances and Future Challenges // IEEE Transactions on
Smart Grid. VII 2022c. 13, 4. 2935–2958.
Christopoulos Christos . The Transmission-Line Modeling (TLM) Method in Electromagnetics. VI
2022.
Dankwa Stephen, Zheng Wenfeng . Twin-Delayed DDPG: A Deep Reinforcement Learning Technique
to Model a Continuous Movement of an Intelligent Robot Agent // Proceedings of the 3rd Interna-
tional Conference on Vision, Image and Signal Processing. New York, NY , USA: Association for
Computing Machinery, V 2020. 1–5. (ICVISP 2019).
Guerrero Josep M., Vasquez Juan C., Matas José, de Vicuna Luis García, Castilla Miguel . Hi-
erarchical Control of Droop-Controlled AC and DC Microgrids—A General Approach Toward
Standardization // IEEE Transactions on Industrial Electronics. I 2011. 58, 1. 158–172.
Haarnoja Tuomas, Zhou Aurick, Abbeel Pieter, Levine Sergey . Soft Actor-Critic: Off-Policy Maxi-
mum Entropy Deep Reinforcement Learning with a Stochastic Actor. VIII 2018.
Hossain Rakib, Gautam Mukesh, Lakouraj Mohammad Mansour, Livani Hanif, Benidris Mohammed .
V olt-V AR Optimization in Distribution Networks Using Twin Delayed Deep Reinforcement
Learning // 2022 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference
(ISGT). IV 2022. 1–5.
Hou Xiaochao, Sun Kai, Zhang Ning, Teng Fei, Zhang Xin, Green Tim C. Priority-Driven Self-
Optimizing Power Control Scheme for Interlinking Converters of Hybrid AC/DC Microgrid
Clusters in Decentralized Manner // IEEE Transactions on Power Electronics. V 2022. 37, 5.
5970–5983.
Khayat Yousef, Shafiee Qobad, Heydari Rasool, Naderi Mobin, Dragiˇ cevi´ c Tomislav, Simpson-Porco
John W., Dörfler Florian, Fathi Mohammad, Blaabjerg Frede, Guerrero Josep M., Bevrani Hassan .
On the Secondary Control Architectures of AC Microgrids: An Overview // IEEE Transactions on
Power Electronics. VI 2020. 35, 6. 6482–6500.
Kober Jens, Bagnell J. Andrew, Peters Jan . Reinforcement Learning in Robotics: A Survey // The
International Journal of Robotics Research. IX 2013. 32, 11. 1238–1274.
Lasseter R.H., Paigi P . Microgrid: A Conceptual Solution // 2004 IEEE 35th Annual Power
Electronics Specialists Conference (IEEE Cat. No.04CH37551). 6. VI 2004. 4285–4290 V ol.6.
Lu Xiaoqing, Yu Xinghuo, Lai Jingang, Wang Yaonan, Guerrero Josep M. A Novel Distributed
Secondary Coordination Control Approach for Islanded Microgrids // IEEE Transactions on Smart
Grid. VII 2018. 9, 4. 2726–2740.
Mnih Volodymyr, Kavukcuoglu Koray, Silver David, Graves Alex, Antonoglou Ioannis, Wierstra Daan,
Riedmiller Martin . Playing Atari with Deep Reinforcement Learning. XII 2013.
Nguyen Hai, La Hung . Review of Deep Reinforcement Learning for Robot Manipulation // 2019
Third IEEE International Conference on Robotic Computing (IRC). Naples, Italy: IEEE, II 2019.
590–595.
Olivares Daniel E., Mehrizi-Sani Ali, Etemadi Amir H., Cañizares Claudio A., Iravani Reza, Kazerani
Mehrdad, Hajimiragha Amir H., Gomis-Bellmunt Oriol, Saeedifard Maryam, Palma-Behnke
Rodrigo, Jiménez-Estévez Guillermo A., Hatziargyriou Nikos D. Trends in Microgrid Control //
IEEE Transactions on Smart Grid. VII 2014. 5, 4. 1905–1919.
5Osman Ahmed I., Chen Lin, Yang Mingyu, Msigwa Goodluck, Farghali Mohamed, Fawzy Samer,
Rooney David W., Yap Pow-Seng . Cost, Environmental Impact, and Resilience of Renewable
Energy under a Changing Climate: A Review // Environmental Chemistry Letters. IV 2023. 21, 2.
741–764.
Pogaku Nagaraju, Prodanovic Milan, Green Timothy C. Modeling, Analysis and Testing of Au-
tonomous Operation of an Inverter-Based Microgrid // IEEE Transactions on Power Electronics.
III 2007. 22, 2. 613–625.
Qu Guannan, Wierman Adam, Li Na . Scalable Reinforcement Learning for Multi-Agent Networked
Systems. X 2021.
Rajesh K. S., Dash S. S., Rajagopal Ragam, Sridhar R. A Review on Control of Ac Microgrid //
Renewable and Sustainable Energy Reviews. V 2017. 71. 814–819.
Wang Ruoheng, Bu Siqi, Chung C. Y. Real-Time Joint Regulations of Frequency and V oltage for
TSO-DSO Coordination: A Deep Reinforcement Learning-Based Approach // IEEE Transactions
on Smart Grid. 2023. 1–1.
Yang Qiufan, Yan Linfang, Chen Xia, Chen Yin, Wen Jinyu . A Distributed Dynamic Inertia-droop
Control Strategy Based on Multi-Agent Deep Reinforcement Learning for Multiple Paralleled
VSGs // IEEE Transactions on Power Systems. 2022. 1–15.
Zimmerman Ray Daniel, Murillo-Sanchez Carlos Edmundo, Thomas Robert John . MATPOWER:
Steady-State Operations, Planning, and Analysis Tools for Power Systems Research and Education
// IEEE Transactions on Power Systems. II 2011. 26, 1. 12–19.
6A Networked MDP formulation justification
Figure 5: Relations of state variables.
In order to justify the networked MDP modeling of the MG, the relationships among the state
variables in the microgrid (MG) model were examined. As shown in Figure 5 (a), we considered a
simple system with three DGs. Figure 5 (b) depicts the interactions among the state variables in the
system. The directed arrows in the figure represent that the value of the state variable corresponding
to the starting point of the arrow influences the first order derivative of the state variable at the arrow’s
end point.
When we examine the state variables of DG jand DG kin Figure 5, the minimum distance between
them is four. This implies that a change in the value of any state variable in the system will only
affect derivatives of the fourth order or higher. Consequently, if the system is discretized using any
numerical method with a precision less than the fourth order, it would satisfy the assumption of the
networked MDP, i.e., the interaction between nodes occurs in a local manner. Given the fact that the
popular numerical integration method used for electromagnetic transient simulation is trapezoidal
method, a second-order method, we can conclude the networked MDP modelling is accurate enough.
B Exponential decay property
Figure 6: Graphical proof of exponential decay property.
Before explaining exponential decay property, we formulate the aforementioned networked MDP
in a more formal way. We consider a network of Nagents that are associated with an underlying
undirected graph G= (N,E), where N={1, . . . N }is the set of agents and E ⊂ N × N is the
set of edges. Each agent iis associated with state si∈ Si,ai∈ A i. The global state is denoted
ass= (s1, . . . ,sN)∈ S:=S1× ··· × S Nand similarly the global action a= (a1, . . . ,aN)⊂
A:=A1× ··· × A N. At time t, given current state stand action at, the next individual state st+1is
independently generated and is only dependent on neighbors:
P(st+1|st,at) =NY
i=1P(si,t+1|sNi,t,ai,t), (1)
7where notation Nimeans the neighborhood of i(including iitself) and notation sNi,tmeans the states
of the agents in Ni. In addition, for integer κ≥0, we use Nκ
ito denote the κ-hop neighborhood of i,
i.e. the nodes whose graph distance to i has length less than or equal to κ, andNκ
−i=N/Nκ
i.
In each time step t, each agent ican only observe the local states si,t∈ Si. Then each agent
chooses its own action ai,tusing a stochastic policy defined as a probability density function
πi:Si× A i→[0,∞), i.e., ai,t∼πi(·|si,t). Also use π(at|st) =QN
i=1πi(ai,t|si,t)to denote the
joint policy.
Furthermore, each agent is associated with a stage reward function ri,t=ri(si,t,ai,t)that depends
on the local observations and actions, and the global stage reward is rt=r(st,at) =PN
i=1ri,t.
Define the global Q-function and individual Q-functions as
Qπ(s,a) =E[∞X
t=0γtrt|s0=s,a0=a]
=NX
i=1E[∞X
t=0γtri
t|s0=s,a0=a] :=NX
i=1Qπ
i(s,a),(2)
where γis the discounted factor.
The(c, ρ)-exponential decay property is said to hold if, for any localized policy πi, for any i∈ N ,
sNκ
i∈ SNκ
i,esNκ
−i∈SNκ
−i,aNκ
i∈ ANκ
i,eaNκ
−i∈ ANκ
−iQπ
isatisfies
|Qπ
i(sNκ
i,sNκ
−i,aNκ
i,aNκ
−i)−Qπ
i(sNκ
i,esNκ
−i,aNκ
i,eaNκ
−i)| ≤cρκ+1. (3)
If all rewards are bounded, then (c, ρ)-exponential decay property holds with ρ≤γ. This proof can
obtained by using the local characteristic of the networked MDP. The graphical proof is shown in
Figure 6.
C Soft-actor-critic with scalable network-aware framework
Algorithm 1 SAC with scalable network-aware framework
1:Initialize experience buffer D, and parameters of the actors and critics
2:foreach episode do
3: foreach step tdo
4: foreach agent idoin parallel
5: Choose ai,tbased on πi(·|si,t);
6: Take actions
7: Get reward ri,tand next states si,t+1from the MG environment.;
8: end for
9: D ← D ∪ { st,at, rt,st+1}
10: foreach agent idoin parallel
11: Sample a batch from experience buffer D;
12: Update individual Q-function: θi←θi+σi∇θiJQi(θi);
13: Update individual policy: ϕi←ϕi+σi∇ϕiJπi(ϕi);
14: Softly Update target networks’ parameters;
15: end for
16: end for
17:end for
To train the critics, we need to minimize the following equations:
JQi(θi) =E(sNκ
i,t,aNκ
i,t)∼D[1
2(ˆQθi(sNκ
i,t,aNκ
i,t)−ˆy)2], (4)
with
ˆy=ri
t+γE(sNκ
i,t,aNκ
i,t)∼D[ˆQθi(sNκ
i,t+1,aNκ
i,t+1)−αlog(πϕi(ai,t+1|si,t+1))], (5)
8where ϕiandθiare parameters of actors and critics; θiare parameters of the target neural network of
critics; ˆQis the truncated Q-function.
The target value ˆyonly takes the individual reward, indicating the truncated Q-function ˆQonly
estimates the expected individual reward.
To train the actors, we need to minimize the following equations:
Jπi(ϕi) =E(sNκ
i,t)∼D,a∼πϕ(·|st)[αlog(πϕi(ai,t+1|si,t+1)−X
j∈Nκ
iˆQθj(sNκ
j,t,aNκ
j,t)].(6)
As shown in the above equation, each actor is guided by the critics of its κ-hop neighbors.
9