Towards autonomous large-scale monitoring the health
of urban trees using mobile sensing
Akshit Gupta
Faculty Of Civil Engineering and Geosciences
Delft University of Technology
Delft, The Netherlands
a.gupta-5@tudelft.nlMartine Rutten
Faculty Of Civil Engineering and Geosciences
Delft University of Technology
Delft, The Netherlands
m.m.rutten@tudelft.nl
Remko Uijlenhoet
Faculty Of Civil Engineering and Geosciences
Delft University of Technology
Delft, The Netherlands
R.Uijlenhoet@tudelft.nl
R. Venkatesha Prasad
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
Delft, The Netherlands
R.R.VenkateshaPrasad@tudelft.nl
Abstract
Healthy urban greenery is a fundamental asset to mitigate climate change phe-
nomenons such as extreme heat and air pollution. However, urban trees are often
affected by abiotic and biotic stressors that hamper their functionality, and when-
ever not timely managed, even their survival. The current visual or instrumented
inspection techniques often require a high amount of human labor making frequent
assessments infeasible at a city-wide scale. In this work, we present the GreenScan
Project, a ground-based sensing system designed to provide health assessment of
urban trees at high space-time resolutions, with low costs. The system utilises
thermal and multi-spectral imaging sensors, fused using computer vision models to
estimate two tree health indexes, namely NDVI and CTD. Preliminary evaluation of
the system was performed through data collection experiments in Cambridge, USA.
Overall, this work illustrates the potential of autonomous mobile ground-based tree
health monitoring on city-wide scales at high temporal resolutions with low-costs.
1 Introduction
In urban cities, the tree canopies and vegetation provide a wide range of ecosystem services such as
air filtering, carbon sequestration, reduced energy consumption, and decreased local temperatures
[1,2]. However, urban trees are experiencing an ample amount of abiotic stressors such as the urban
heat island (UHI) effect, and biotic stressors, such as insects and bacteria attacks that are exacerbated
due to climate change [ 3,4,5]. As a result, their functionality, productivity, and survival are of
increasing concern [6].
Currently, the health of trees can be inspected by arborists (a tree doctor) with good quality results,
but at high costs due to the amount of human labor involved [ 7]. Therefore, leading to an assessment
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.that has a low temporal resolution (e.g. once every 2-5 years). Satellite-based imaging can cover large
areas although at a low spatial granularity (depending on the type of sensor) [ 8], with data quality
influenced by external factors such as the availability of clear skies [ 8], and constrained temporal
resolutions except in cases of targeted acquisitions [ 9]. Airborne sensing using Unmanned Aerial
Vehicles (UA Vs) or airplanes leads to an increased spatial granularity [ 8], yet it may not be suitable
in highly urbanized environments due to aviation authority regulations. Further, depending on the
canopy density, both airborne sensing and satellite imagery usually capture the overhead view of the
urban canopy. As a result, vegetation elements such as green walls, short trees, or shrubs present
underneath the tree canopy may be missed [ 10] or misinterpreted [ 11]. These challenges make it
infeasible for governments and municipalities to perform frequent health quality inspections of urban
greenery at large scales. As a result, adverse health conditions are often discovered only after severe
damage, and sometimes, beyond the point of no return. Further, given the lack of tree health data,
intricate relationships of urban trees with other micro-scale ecosystem services is not well understood
from an urban planning perspective.
In recent years, a number of projects have investigated the use of low-cost technological alternatives
to monitor various environmental parameters in cities. For instance, applying computer vision
methods on Google Street View (GSV) images to detect the presence of trees [ 12,13], or using
drive-by sensing strategies to measure air pollution and other urban morphology parameters [ 14].
Drive-by sensing approaches enable collecting data at both higher temporal and spatial resolutions
[15] compared to manual inspection approaches, embedded sensors, or satellite-based sensing. A
subset of these works, set within the domain of opportunistic sensing are developing platforms that
can be deployed and operated without the need of a dedicated infrastructure.
Following on this trend and the critical need for protecting and managing urban trees, in this work, we
investigate the monitoring of the health of urban trees, terrestrially, at high spatio-temporal resolutions
with low costs. The work described as the GreenScan Project , measures the health of urban trees on
a city-wide scale using complementary indexes that represents a tree’s photosynthetic capacity and a
tree’s water stress levels. The system fuses data from low-cost thermal and multispectral imaging
sensors using custom computer vision models to generate two popular health indexes namely NDVI
(Normalised Difference Vegetation Index) and CTD (Canopy Temperature Depression). NDVI is
based on the optical properties of chlorophyll and cell structure in the leaves [ 16], while CTD is
physically related to the overall water stress of the trees (based on soil) [ 17]. A GreenScan system is
suitable for deployment in a citizen-science paradigm by being carried by pedestrians and/or in a
drive-by sensing paradigm by mounting on urban vehicles such as taxis, garbage trucks or bikes. Thus,
allowing autonomous large-scale monitoring the health of urban trees at high spatial and temporal
resolutions for cities and municipalities around the world.
2 Methodology
The block diagram of the entire GreenScan Project architecture is shown in Figure 5. To ensure
real-world deployment across geographical shifts, we developed a custom hardware device as shown
in Figure 1a running custom computer vision model and a white box processing pipeline. The case
was designed such that it is suitable to be attachable to moving vehicles without any alterations
using magnets, as shown in Figure 4. Herewith, we give a brief description of all the major modules,
including the main hardware and the software components.
2.1 Hardware Components
•Thermal Imaging Sensor : A thermal imaging sensor [FLIR Lepton 3.5 (spectrum:
longwave-infrared @8 µm - 14 µm)] to measure true temperature) is connected to the central
single board computer and captures thermal images normalised to a suitable temperature
range with low pixel resolution (160x120).
•Multispectral Imaging Sensor : A multispectral imaging sensor [MAPIR Survey 3W
(spectrum: red@660nm, green@550nm, near-infrared@850nm)] is attached to the central
single board computer and captures Red, Green and Near-infrared (RGN) imaging data with
high pixel resolution (4000x3000).
2(a) All hardware components en-
cased with the 3D printed case
(b) Input RGN Image captured
from MAPIR Survey 3W
(c) Segmentation output from our
Custom Mask R-CNN (instance
segmentation model) trained us-
ing transfer learning
Figure 1
•GNSS Receiver : A Global Navigation Satellite System (GNSS) receiver with support for
GPS, GLONASS, and Galileo is used to find the current location of the system and geo-tag
all the captured images.
•Single Board Computer with/without Edge TPU : A single board computer such as
Raspberry Pi 3 (with/without USB TPU accelerator) or Nvidia Jetson Nano acts as the
central brain of the system integrating all the hardware and software components.
•Power Supply / Solar Panel : A lithium-ion battery (10000 mAh) is used to ensure uninter-
rupted power supply to the system along with support for charging over a solar panel or a
standard power adapter (5V/2A).
2.2 Software Components
A visualization of processing the data from the imaging sensors after each software module is shown
in Figure 3. The major software modules consist of:
•Image Selection : With the co-location (<5m) of the system with particular GPS coordinates
fetched from a tree inventory database, multiple pairs of thermal and multi-spectral images
are captured simultaneously from the hardware components. Amongst the pair of images,
the best frame is selected by utilising the multi-spectral data (high resolution).1
•Image Registration : This involves matching or aligning images taken by two different
sensors into a single coordinate system for further analysis [ 21] by linear translation in both
horizontal, vertical directions and zooming in due to different FOVs (field of views).
•Image Segmentation : The NDVI and CTD of a tree should be calculated only for the leaves
in the tree canopy excluding the wooden parts, such as the trunk and branches. This is solved
using a fusion of custom developed Mask R-CNN and pixel-wise NDVI analysis. Given a
multi-spectral RGN image captured using the multi-spectral imaging sensor, this task was
be broken down into two sub-problems as follows:
–Detect the canopy part of the trees including if the image contains multiple trees:
This is solved using a custom-developed Mask R-CNN model. The Mask R-CNN model
is trained using transfer learning on RGN images and the development is explained
in more detail in Section A. Given an RGN image as input, it segments the instances
of the tree canopies in the RGN image by generating a mask over them as shown in
Figure 1.
1This module was not used in the current data collection experiments and is currently under active research.
Instead, data was collected using a citizen science paradigm and the best frame was selected manually. As of
now, we are exploring two selection approaches for this module. Deterministic selection involving the usage of
features such as the percentage coverage of the tree canopy pixels, sharpness [ 18] and the centering of the tree
trunk in the frame. However, being restricted to a single frame, this deterministic selection can give adverse
results if the best frame for diagnostic is less sharp and hence not selected. Secondly, we are also exploring
Model-based selection such as [ 19]. Further, we are exploring custom LSTM based approaches to utilise more
than one frame for a tree’s diagnosis as well [20].
3–Remove Noise: The non-vegetation elements such as trunks, branches, and sky have
very low NDVI values compared to vegetation elements which have significantly
higher NDVI. Thus, we employ a thresholding based method which first calculates the
individual NDVI of each pixel in the segmentation mask generated by custom Mask
R-CNN and the pixels with NDVI value below a certain threshold are eliminated.
The end result employing the above two-stage approach gives segmentation of only leaves
present in the tree canopy.
•Analysis and Calculation : This module handles the calculation of final NDVI and CTD for
each tree in the field of view of the imaging sensors. CTD is calculated as:
CTD =Tcanopy −Tair (1)
where Tcanopy andTairare average canopy temperature and air temperature respectively in
◦C.
The raw NDVI value for each pixel is calculated from red and near-infrared channels as per
(2) as:
NDV I =NIR −Red
NIR +Red, (2)
where NIR andRed are values of near-infrared channel and visible red channel for each
pixel respectively. Both the CTD and NDVI are subsequently averaged for all the pixel in
the mask of the leaves of the tree canopy.
3 Results
We evaluated this system using a ground truth dataset obtained for the city of Cambridge, USA
through the Cambridge Urban Forest Master Plan [ 11]. This dataset classifies the health conditions of
all trees (47,063 trees) into three categories, namely good, poor, and fair. We collected multi-spectral
(RGN) and thermal images using the GreenScan Project on three separate days in Cambridge, USA
during the month of February 2022. In total, we collected data for 49 trees spread over two species
namely Red Pine and Eastern White Pine trees. The correlation results between the measured NDVI
and CTD with ground truth parameters is shown in Table 1. From Table 1, it is seen that there is a
significant and moderately strong correlation (r=0.54 with p < 0.05) between our measured NDVI
and remote NDVI. Further, it is seen that the CTD has a weak-moderate correlation (r=0.28 with
p < 0.05) with ground truth tree health condition. Further, the distribution of CTD and NDVI with
respect to ground truth health conditions is also shown in Figure 2.
4 Conclusion
In this work, we developed a novel system to measure tree health autonomously from the terrestrial
level in urban cities. The system fuses data from low-cost thermal and multi spectral imaging sensors
using custom computer vision models optimised for efficiency to generate two tree health indexes
namely NDVI and CTD. The custom Mask R-CNN model fine-tuned using transfer learning was
employed to fuse the data collected by the imaging sensors on the edge device. Deployment can be
performed both in a drive-by sensing paradigm on moving vehicles such as taxis and garbage trucks
or in a citizen-science sensing paradigm by humans. Evaluation of the system was performed through
data collection experiments in Cambridge, USA. The tree health analysis revealed moderately-strong
correlation (r=0.54 with p−value < 0.05) between our measured NDVI and the remote NDVI
obtained from the ground truth dataset. The deployment and validation of the system in cities with
different topographies and with geographical domain shifts can help in enhancing the generalization
of the approach using large-scale training and validation datasets. Further, the effect of different
weather conditions with reduction of visibility, and sunlight directly facing the imaging lenses needs
further exploration.
Acknowledgments and Disclosure of Funding
The authors would like to thank Peter Naylor from European Space Agency for his help in mentoring
the image selection module of this work. The current results were collected by Akshit Gupta when he
4All Trees Eastern White Pine Trees Red Pine Trees0.200.250.300.350.400.450.500.550.60Corrected NDVIHealth Condition
Good
Poor
Fair(a) The distribution of NDVI for the trees
All Trees Eastern White Pine Trees Red Pine Trees10
5
051015CTD
Health Condition
Good
Poor
Fair (b) The distribution of CTD for the trees
Figure 2: The distribution of NDVI and CTD for the trees with respect to health
Variables Pearson Correlation Significant at
Measured Ground Truth (r) (p <0.05)
NDVI Remote NDVI 0.54 Yes
CTD Remote NDVI -0.38 No
NDVI Health Condition 0.11 No
CTD Health Condition 0.28 Yes
NDVIArea (m2)
(LiDAR)0.28 Yes
CTDArea (m2)
(LiDAR)-0.15 No
Table 1: The correlation between our measured values and ground truth parameters
was a visiting student at the MIT Senseable City Lab. Part of this work was funded by the TU Delft
Climate Action Programme.
A Development of Custom Mask R-CNN
As the system will operate autonomously, the images are captured in an unsupervised fashion. Thus,
in addition to multiple trees in a single image, other objects such as cars, buildings, grass, and snow
might also be present. Hence, it is imperative to individually identify all the tree canopies in an
image and feed them to the calculation and analysis module. The custom mask R-CNN part of the
image segmentation module solves this by providing instance segmentation of the tree canopies in
the image. To our knowledge, there is no pre-existing model available for instance segmentation of
tree canopies only in standard RGB images. The problem is further complicated as our input is RGN
(Red, Green, Near-infrared) images from the multispectral imaging sensor instead of standard RGB
images. For instance, we found that pre-trained models like Deeplabv3 [ 22], which perform semantic
segmentation of trees and vegetation on standard RGB images, perform poorly on RGN images.
A.1 Training Data and Process
We manually created the dataset using the RGN images collected during the data collection exper-
iments. Each tree canopy in the image was manually annotated using a popular image annotation
tool called LabelMe [ 23]. During annotation, only tree canopies that were completely present in the
image were labelled. After this process, our dataset consisted of 51 annotated RGN images with two
classes namely tree canopies and background.
Transfer learning combined with data augmentation was employed in order to develop a custom
model by using an existing model pre-trained on a different dataset. For this, we used a Mask R-CNN
pre-trained [ 24] on COCO [ 25] with ResNet101 as the backbone. We re-trained only the head layers
(the top layers without the backbone) on our dataset. The batch size was configured as 4 and the
number of epochs was 10. The training was performed on the Google Cloud Platform with a N1
5Figure 3: A visualisation of processing the images at each software module
instance with 13GB memory and 2vCPUs. We also generated synthetic data by augmenting the
original dataset with flips in the horizontal and vertical directions and applying Gaussian blur. This
increased the training dataset size by 50% and acted as a regularizer. The manually annotated dataset
(refer Section A.1) consisting of 51 images was split in the ratio of 70: 30 for training: testing.
Later, the developed Mask R-CNN was optimized to run on the edge at the cost of possible minute
performance reduction. For this, the model built on TensorFlow was converted to TensorFlow-lite
with dynamic range quantization [ 26]. The performance of the custom Mask R-CNN with and
without quantization is shown in Table 2.
6(a)
 (b)
 (c)
Figure 4: (a) Concept casing for the system with magnets; (b) The system attached to the top of a car;
(c) A closeup view of the system attached to the roof of a car
Figure 5: Architecture Diagram of the system
Model AP(IoU =0.5:0.95:0.05)AP(IoU =0.5)AP(IoU =0.75)ModelSize
Custom MR-
CNN0.489 0.938 0.500 255.9 MB
Custom MR-
CNN (Quan-
tized)0.491 0.938 0.500 65 MB
Table 2: Performance of custom R-CNN model (Full and Quantized model)
Species / Health Good Fair Poor
Red Pine (NDVI) 0.37±0.07 0 .28±0.05 0 .28±0.03
Eastern White Pine (NDVI) 0.49±0.08 0.46 0.43±0.12
Red Pine (CTD) 4.63±3.64 2 .89±1.78 6 .99±4.85
Eastern White Pine (CTD) −9.1±1.88 -8.59 −9.1±1.88
Table 3: The mean of measured NDVI and CTD across species and health
7References
[1]E. Gregory McPherson. Accounting for benefits and costs of urban greenspace. Landscape and
Urban Planning , 22(1):41–51, 1992.
[2]Sarah E. Hobbie and Nancy B. Grimm. Nature-based approaches to managing climate change
impacts in cities. Philosophical Transactions of the Royal Society B: Biological Sciences ,
375(1794):20190124, jan 2020.
[3]Nancy Grimm, Stanley Faeth, Nancy Golubiewski, Charles Redman, Jianguo Wu, Xuemei
Bai, and John Briggs. Global change and the ecology of cities. Science (New York, N.Y.) ,
319:756–60, 03 2008.
[4]Sophie A. Nitoslawski, Nadine J. Galle, Cecil C. Konijnendijk van den Bosch, and James W. N.
Steenberg. Smarter ecosystems for smarter cities? a review of trends, technologies, and turning
points for smart urban forestry. Sustainable Cities and Society , 2019.
[5]IPCC. Summary for Policymakers , page 3-32. Cambridge University Press, Cambridge, United
Kingdom and New York, NY , USA, 2021.
[6]Zoey R. Werbin, Leila Heidari, Sarabeth Buckley, Paige Brochu, Lindsey J. Butler, Catherine
Connolly, Lucila Houttuijn Bloemendaal, Tempest D. McCabe, Tara K. Miller, and Lucy R.
Hutyra. A tree-planting decision support tool for urban heat mitigation. PLOS ONE , 15(10):1–
13, 10 2020.
[7]E. Leong, Daniel Burcham, and Yok Fong. A purposeful classification of tree decay detection
tools. Arboricultural Journal , 34, 06 2012.
[8]Sigfredo Fuentes, Eden Tongson, and Claudia Gonzalez Viejo. Urban green infrastructure
monitoring using remote sensing from integrated visible and thermal infrared cameras mounted
on a moving vehicle. Sensors , 21(1), 2021.
[9]Rasmus Houborg and Matthew F. McCabe. High-resolution ndvi from planet’s constellation of
earth observing nano-satellites: A new data source for precision agriculture. Remote Sensing ,
8(9), 2016.
[10] Xiaojiang Li, Chuanrong Zhang, Weidong Li, Robert Ricard, Qingyan Meng, and Weixing
Zhang. Assessing street-level urban greenery using google street view and a modified green
view index. Urban Forestry & Urban Greening , 14(3):675–685, 2015.
[11] Cambridge urban forest master plan preliminary report.
[12] Xiaojiang Li, Chuanrong Zhang, Wei Li, Robert M. Ricard, Qingyan Meng, and Weixing Zhang.
Assessing street-level urban greenery using google street view and a modified green view index.
Urban Forestry & Urban Greening , 14:675–685, 2015.
[13] Ian Seiferling, Nikhil Naik, Carlo Ratti, and Raphael Proulx. Green streets: Quantifying and
mapping urban trees with street-level imagery and computer vision. Landscape and Urban
Planning , 165:93–101, 09 2017.
[14] Amin Anjomshoaa, Fabio Duarte, Daniel Rennings, Thomas J. Matarazzo, Priyanka Desouza,
and Carlo Ratti. City Scanner: Building and Scheduling a Mobile Sensing Platform for Smart
City Services. IEEE Internet of Things Journal , 5(6):4567–4579, 2018.
[15] Kevin P. O’Keeffe, Amin Anjomshoaa, Steven H. Strogatz, Paolo Santi, and Carlo Ratti.
Quantifying the sensing power of vehicle fleets. Proceedings of the National Academy of
Sciences , 116(26):12752–12757, 2019.
[16] Compton J. Tucker. Red and photographic infrared linear combinations for monitoring vegeta-
tion. Remote Sensing of Environment , 8(2):127–150, 1979.
[17] Maria Balota, William A. Payne, Steven R. Evett, and Mark D. Lazar. Canopy temperature
depression sampling to assess grain yield and genotypic differentiation in winter wheat. Crop
Science , 47(4):1518–1529, 2007.
8[18] Cuong T. Vu, Thien D. Phan, and Damon M. Chandler. s3: A spectral and spatial measure
of local perceived sharpness in natural images. IEEE Transactions on Image Processing ,
21(3):934–945, 2012.
[19] Jian Ren, Xiaohui Shen, Zhe Lin, and Radomír M ˇech. Best frame selection in a short video. In
2020 IEEE Winter Conference on Applications of Computer Vision (WACV) , pages 3201–3210,
2020.
[20] Fabio Carrara, Petr Elias, Jan Sedmidubsky, and Pavel Zezula. LSTM-based real-time ac-
tion detection and prediction in human motion streams. Multimedia Tools and Applications ,
78(19):27309–27331, October 2019.
[21] Emna Kamoun. Image registration: From sift to deep learning, Mar 2021.
[22] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation, Dec 2017.
[23] Kentaro Wada. labelme: Image polygonal annotation with python. https://github.com/
wkentaro/labelme , 2018.
[24] Waleed Abdulla. Mask r-cnn for object detection and instance segmentation on keras and
tensorflow. https://github.com/matterport/Mask_RCNN , 2017.
[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll’a r, and C. Lawrence Zitnick. Microsoft COCO:
common objects in context. CoRR , abs/1405.0312, 2014.
[26] Tensorflow. Post-training quantization | tensorflow lite. Available online at: https://www.
tensorflow.org/lite/performance/post_training_quantization , last accessed on
09.30.2023.
9