Ocean Wave Energy: Optimizing Reinforcement
Learning Agents for Effective Deployment
Vineet Gundecha1,Sahand Ghorbanpour1,Ashwin Ramesh Babu1,Avisek Naug1,
Alexandre Pichard2,Mathieu Cocho2,Soumyendu Sarkar1∗
1Hewlett Packard Enterprise
vineet.gundecha, sahand.ghorbanpour, ashwin.ramesh-babu, avisek.naug,
soumyendu.sarkar @hpe.com
2Carnegie Clean Energy
apichard, mcocho @cce.com
Abstract
Fossil fuel energy production is a leading cause of climate change. While wind
and solar energy have made advancements, ocean waves, a more consistent clean
energy source, remain underutilized. Wave Energy Converters (WEC) transform
wave power into electric energy. To be economically viable, modern WECs
need sophisticated real-time controllers that boost energy output and minimize
mechanical stress, thus lowering the overall cost of energy (LCOE). This paper
presents how a Reinforcement Learning (RL) controller can outperform the default
spring-damper controller for complex spread waves in the sea, enhancing wave
energy’s viability. Using the Proximal Policy Optimization (PPO) algorithm with
Transformer variants as function approximators, the RL controllers optimize multi-
generator Wave Energy Converters (WEC) leveraging wave sensor data for multiple
cost-efficiency goals. After successful tests in the EuropeWave2project’s emulator
tank, the platform is planned to deploy. We discuss the challenges of deployment
at the BiMEP site and how we had to tune the RL controller to address that. The
RL controller outperforms the default Spring Damper controller in the BiMEP3
conditions by 22.8% on energy capture. Enhancing wave energy’s economic
viability will expedite the transition to clean energy, reducing carbon emissions
and fostering a healthier climate.
1 Introduction
Ocean waves are a more consistent and reliable source of clean energy than wind and solar. It is
estimated that the total potential of ocean wave energy is around 30,000 TWh/yr, which is about
20% of the 2019 world energy consumption Lewis et al. (2011). Harnessing this energy using Wave
Energy Converters is a complex engineering task.
∗Corresponding author
2EuropeWave: https://www.europewave.eu/
3BiMEP: https://www.bimep.com/en/
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.Figure 1: (Left) Complexity increases from simple one-legged WEC to the multi-legged CETO 6. (Middle)
Spread waves with components from multiple directions. (Right) Distribution of energy along various directions
There are different methodologies for designing WECs. Here, we focus on the CETO design which
is a fully submerged, point absorber type wave energy technology. WECs are typically submerged
beneath the ocean, and are tethered to the ocean bed using mooring legs. These legs are connected
to an electrical generator. As waves roll over, these legs extend and retract. The generator applies
a varying control torque, which dictates how the device moves. The power generated by each leg
is proportional to the product of the leg’s extension velocity and the control force. Maximizing
generated power, requires advanced controllers that can operate in real-time. Traditional, passive
controllers like spring-damper are optimal for simple, unidirectional sinusoidal waves. However,
they sub-optimal for ocean waves that have a spectrum of frequencies with components in multiple
directions - usually called spread waves as depicted in Figure 1. Spread waves introduce complexity
for WECs due to simultaneous waves arriving from various angles, influenced by atmospheric events
in different oceanic regions. Also, passive controllers cannot utilize information about incoming
waves in advance obtained from wave sensors further out in the ocean.
This work uses reinforcement learning to train an agent to maximize power for spread waves at
the BiMEP test site. We design a state space ( Figure 2) that enable the agent to effectively utilize
information from incoming waves. We also design a reward function that enables the agent to
maximize power for a variety of sea states for a specific location while minimizing the yaw rotational
motion detrimental to the device’s longevity. Overall this can significantly reduce the Levelized-Cost-
of-Energy (LCOE).
To our knowledge, this is the first reinforcement learning controller that was successfully tested in a
physical multi-legged WEC.
Related work: RL controllers for WEC have been explored in simplified settings - mostly for point
absorbers with 1 degree of freedom. Anderlini et al. (2016), Anderlini et al. (2017) utilize RL in a
non-real-time scenario to predict change in the damping coefficient of a spring-damper controller
for a particular sea state. Anderlini et al. (2020) apply Soft Actor Critic for real-time control of a 1
legged point absorber. Sarkar et al. (2021), Sarkar et al. (2022a), Sarkar et al. (2022b), Sarkar et al.
(2023), demonstrate using PPO for a multi-legged device, however, did not discuss the deployment
issues with multidirectional spread waves.
Figure 2: Reinforcement Learning controller for Wave Energy Controller
22 Reinforcement Learning for Wave Energy Converters
The state vector for the reinforcement learning (RL) is formulated as shown in Figure 2. The position,
velocity, and acceleration include both the translation and rotational components (6 degrees of
freedom). In addition to the current wave elevation, we feed in the wave elevation 10 and 15 seconds
in the future. This is available through wave sensors that are installed further into the ocean. The
characteristics of the incoming wave is essential for the agent to optimize over a time horizon.
TrXL TrXL-1 
Lx Lx 
I' '\ 
Layer-Nom1 X ' ./ 
y 
I' '\ 
' ./ Posi.'lion-Wise 
A.(lP • 
Posi.'lion-Wise Layer-Nom1 
A.(lP • " I' '\ 
Layer-Nom1 ' ./ 
X 
y 
I' '\ "'"" ,e3a 
' ./ Af.enuon 
MUlb-1e30 • 
A!:tention Layer-Nom1 GTrXL 
Lx 
-( Gating Layer 
X .l 
Position-Wrse 
MLP • 
Layer-Nom1 
• I 
-I Gating Layer 
X YT 
11o1U1wneaa 
Af.enuon 
• 
Layer-Nom1 STrXL (Ours) 
Lx 
Gaiing Layer 
Ga:ing Layer 
Position-'Mse 
MlP 
Layer-Norm 
Gaiing Layer 
Layei-Nom1 
Figure 3: Variations of Transformer adapted for convergence. The overall design has L such blocks. TrXL: A
conventional transformer with multi-head attention and layer normalization, TrXL-I: Layer norm is included
with the input stream, GTrXL: Gating layer to include the residual connection around the attention and MLP
blocks (Parisotto et al. (2020)), STrXL: Residual connections with gating layer aroud the transformer block.
The sequential and periodic nature of waves warrants a neural network architecture that can inherently
handle past observations. Transformers have proven to be a robust and performant architecture
for long term dependencies. Specifically, we use the Skip Transformer XL (STrXL) as shown
in figure 3, where we enhanced the Gated Transformer XL (GTrXL) Parisotto et al. (2020), with
residual connections with a gating layer around the transformer block, which makes it train faster
for reinforcement learning applications. As shown in figure 8, the STrXL outperformed the vanilla
transformer (TrXL) and GTrXL in our experiments. We use a common network with separate heads
for the action and value predictions. The action head outputs the normalized control force for the
three legs.
Sea states are characterized by their time periods (6-16 seconds) and their height (Ht, 1-4 meters).
Depending on the geographic location, each sea state, characterized by the wave height and principal
time period, occurs with a certain probability and exhibits different energy capacities as shown in
Figure 4. To this end, while training, the RL agent we sample a sea state according to the wave
occurrence matrix.
Figure 4: Occurrence of wave types at BiMEP and their power content
Since some sea states carry higher energy than others, the agent tends to favor them. To counter this,
we normalize the generated power by the optimal power for every sea state.
Due to the three-legged architecture of the WEC, it tends to exhibit a spinning motion (yaw) causing
mechanical stress that is detrimental to the operation of the device. In addition to maximizing power,
we would like the agent to also minimize yaw.
We combine these two objectives into a reward function r(t)as below:
r(t) =power (t)
power _opt(tp, ht )−α∗f(yaw (t))∗yaw (t) (1)
Here, power (t)is the power generated at time t,power _opt(tp, ht )is the optimal power for time
period tpand height ht,αis the weighting factor for the yaw penalty, yaw (t)is the yaw at time t,
30 1 2 3 4
Environment steps 1e8150
100
50
050100150200RewardEpisode Reward Mean
0 1 2 3 4
Environment steps 1e815
10
5
0510152025Percentage ImprovementPercentage Improvement in Power
PPO agent
Spring-damperFigure 6: (Left) The mean episode reward over the course of training. (Right) The percentage improvement in
power across all sea states weighted by their probability of occurrence
f(yaw (t))is a non-linear yaw factor which penalizes higher yaw more than lower yaw. Note the
negative sign before the yaw term as we would like to minimize this term
We train the agent using Proximal Policy Optimization (PPO) Schulman et al. (2017) algorithm. We
employ several of recommendations from Parisotto et al. (2020) to stabilize training - policy network
weight initialization, value targets and advantage normalization, and action distribution standard
deviation offset to encourage exploration.
3 Experiments and Results
As training the agent on-line with experience collected on the physical device is infeasible because of
safety constraints, we make use of a simulator. Every episode, a wave of a certain height (1-4 meters),
time-period (6-16 seconds), and principal wave direction (0-30 degrees) is sampled. Episodes can
range between 500-2000 seconds. The simulator operates at a resolution of 0.05 seconds, while the
agent predicts a new action every 0.2 seconds. We use the Ray RLlib package for distributed training.
We normalize the input by tracking the running mean and standard deviation for all the variables
in the state space. The policy network is a 3-layer STrXL, with hidden dimension of 256 and 12
attention heads.
% Power Gain of RL controller over default Spring 
D
a
m
p
e
r
 
f
o
r
 
C
E
T
O
 
6
 
W
E
C
% Power Distribution across diﬀerent wave types 
(
w
e
i
g
h
t
e
d
 
b
y
 
o
c
c
u
r
a
n
c
e
)
Figure 5: RL controller power gain over default Spring Damper controller with tuned the occurrence
of wave types and power distribution for various wave types at the deployment site of BiMEP
On average, the agent requires 400 million steps for convergence, which translates to roughly 900
days of experience. This is depicted in Figure 6. Certain wave states (height and time-period) occur
more frequently than others depending on the geographical location. We would like the agent to
optimize for a particular sea state based on it’s probability of occurrence. To this end, while training
we sample a sea state from the location’s probability occurrence matrix.
Figure 5 shows the power gain achieved for different sea states over the spring-damper controller.
Weighting this by the location’s probability occurrence matrix, we get an average power improvement
of 22.8%. We also achieve an average of 7◦of yaw, which is well under the 20◦limit that causes
mechanical stress. Table 1 shows percentage yaw reduction compared to the spring-damper controller.
To iron out variance, the power gain is calculated for an evaluation episode of 4000 seconds. Figure 7
shows how the RL agent controls the reactive (braking) force for the PTO / generator differently to
generate more power than the default spring damper controller.
40 250 500 750 1000 1250 1500 1750 200075
50
25
0255075Controller force (kN)
0 250 500 750 1000 1250 1500 1750 2000
Time step0255075100125Power (kW)PPO agent
Spring-damperFigure 7: Comparing RL controller with spring-damper. The top figure shows the generated power, the bottom
shows the controller (reactive) force for one of the 3 legs, enabling the RL controller to get higher power peaks.
On-device testing: This RL controller was successfully tested in an emulator tank as part of the
EuropeWave project
0 1 2 3 4 5
Environment steps 1e720
15
10
5
05101520Percentage improvementPercentage improvement in power
GTrXL
STrXL
TrXL
Figure 8: Comparing the proposed transformer architecture STrXL with GTrXL and TrXL. The STrXL exhibits
faster training convergence while outperforming the other architectures in power generation. This curve is for a
wave height of 2 meters and time period of 10 seconds
Tp (s) 11 12 13 14 15 16
Yaw reduction(%) 99.8 98.1 98.1 98.9 98.6 98.6
Table 1: Percentage reduction in yaw for time-periods ranging from 11 to 16 seconds
4 Conclusion and Future Work
We demonstrate the effectiveness of using RL as a control strategy for WECs. Furthermore, we
demonstrate that RL controllers have the capacity to extend their capabilities beyond mere software
simulations, functioning effectively on physical devices and significantly exceeding the performance
of default traditional controllers. We also introduced the challenges in deployment where the real
world waves are more complex and showed how RL agents need to be adapted while considering the
wave types’ distribution in deployment locations. This physical deployment of the WEC to harness
the energy of the ocean waves will contribute to the reduction of energy production from fossil fuels
for a greener planet.
5References
A. Lewis, S. Estefen, J. Huckerby, W. Musial, T. Pontes, Torres-Martinez, Ocean energy. in ipcc
special report on renewable energy sources and climate change mitigation, 2011.
E. Anderlini, D. I. Forehand, P. Stansell, Q. Xiao, M. Abusara, Control of a point absorber using
reinforcement learning, IEEE Transactions on Sustainable Energy 7 (2016) 1681–1690.
E. Anderlini, D. I. Forehand, E. Bannon, M. Abusara, Control of a realistic wave energy converter
model using least-squares policy iteration, IEEE Transactions on Sustainable Energy 8 (2017)
1618–1628.
E. Anderlini, S. Husain, G. G. Parker, M. Abusara, G. Thomas, Towards real-time reinforcement
learning control of a wave energy converter, Journal of Marine Science and Engineering 8 (2020)
845.
S. Sarkar, V . Gundecha, A. Shmakov, S. Ghorbanpour, A. R. Babu, P. Faraboschi, M. Cocho,
A. Pichard, J. Fievez, Multi-objective reinforcement learning controller for multi-generator
industrial wave energy converter, in: NeurIPs Tackling Climate Change with Machine Learning
Workshop, 2021.
S. Sarkar, V . Gundecha, S. Ghorbanpour, A. Shmakov, A. R. Babu, A. Pichard, M. Cocho, Skip
training for multi-agent reinforcement learning controller for industrial wave energy converters, in:
2022 IEEE 18th International Conference on Automation Science and Engineering (CASE), IEEE,
2022a, pp. 212–219.
S. Sarkar, V . Gundecha, A. Shmakov, S. Ghorbanpour, A. R. Babu, P. Faraboschi, M. Cocho,
A. Pichard, J. Fievez, Multi-agent reinforcement learning controller to maximize energy efficiency
for multi-generator industrial wave energy converter, Proceedings of the AAAI Conference on
Artificial Intelligence 36 (2022b) 12135–12144. URL: https://ojs.aaai.org/index.php/
AAAI/article/view/21473 . doi: 10.1609/aaai.v36i11.21473 .
S. Sarkar, V . Gundecha, S. Ghorbanpour, A. Shmakov, A. Ramesh Babu, A. Naug, A. Pichard,
M. Cocho, Function approximation for reinforcement learning controller for energy from spread
waves, in: E. Elkind (Ed.), Proceedings of the Thirty-Second International Joint Conference
on Artificial Intelligence, IJCAI-23, International Joint Conferences on Artificial Intelligence
Organization, 2023, pp. 6201–6209. URL: https://doi.org/10.24963/ijcai.2023/688 .
doi:10.24963/ijcai.2023/688 , aI for Good.
E. Parisotto, F. Song, J. Rae, R. Pascanu, C. Gulcehre, S. Jayakumar, M. Jaderberg, R. L. Kaufman,
A. Clark, S. Noury, et al., Stabilizing transformers for reinforcement learning, in: International
Conference on Machine Learning, PMLR, 2020, pp. 7487–7498.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithms,
arXiv preprint arXiv:1707.06347 (2017).
6