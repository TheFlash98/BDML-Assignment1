Multivariate climate downscaling with latent neural processes
Anna Vaughan1Nicholas D. Lane1Michael Herzog2
Abstract
Statistical downscaling is a vital tool in generat-
ing high resolution projections for climate impact
studies. This study applies convolutional latent
neural processes to multivariate downscaling of
maximum temperature and precipitation. In con-
trast to existing downscaling methods, this model
is shown to produce spatially coherent predictions
at arbitrary locations speciÔ¨Åed at test time, regard-
less of whether training data are available at these
points.
1. Introduction
Generating high resolution climate projections is vital for
assessing risks under different climate scenarios. Unfor-
tunately, the computational requirements of modern earth
system models limit the resolution of simulations. For this
reason, raw model output is routinely post-processed using
statistical methods to downscale results to higher spatial
resolution (Maraun & Widmann, 2018).
Given shortcomings of using traditional statistical tech-
niques for downscaling on many metrics (Widmann et al.,
2019; Guti ¬¥errez et al., 2019; Hertig et al., 2019; Maraun
et al., 2019), there has been considerable work on applying
modern deep learning architectures to this problem. While
bias correction (Cannon, 2011; B ¬®urger et al., 2012) and un-
supervised methods using normalising Ô¨Çows (Groenke et al.,
2020) have been proposed, the majority of studies apply
perfect prognosis models. In these methods the aim is to
learn a transfer function mapping from low resolution pre-
dictors to high resolution outputs. This function is trained
on observational data then applied to the coarse resolution
model output (Maraun & Widmann, 2018). Architectures
previously applied to this task include convolutional neural
networks (Vandal et al., 2017; Wang et al., 2021; Liu et al.,
2020; Ba Àúno-Medina et al., 2020), autoencoders (Vandal
et al., 2019), generative adversarial networks (Chaudhuri &
Robertson, 2020), multilayer perceptrons (MLPs; Cannon,
2008) and long short-term memory networks (Misra et al.,
2018).
In these studies the transfer function maps from a set of
coarse resolution predictors to a Ô¨Åxed set of locations, typi-cally on a grid, where training data are available. The model
is then Ô¨Åtted by minimising a loss function between obser-
vations and model predictions at each point. This raises
the question of how to make predictions at locations where
training data are not available. Wang et al. (2021) used trans-
fer learning to apply a convolutional neural network trained
to map low resolution gridded predictors to high resolution
gridded observations in one region to make skillful predic-
tions in another. Though this allows predictions to be made
at other locations, the resolution of the output grid remains
Ô¨Åxed by the resolution of the training data. An alternative
approach is to learn a transfer function from the low reso-
lution predictors to a stochastic process (i.e a distribution
over spatial functions of the variable being downscaled).
This has advantages that the predicted stochastic process
can be queried at any spatial location at test time, and un-
certainty in the predictions is quantiÔ¨Åed. Vaughan et al.
(2021) applied convolutional conditional neural processes
(convCNP; Gordon et al., 2019) to downscaling temperature
and precipitation at 86 locations in Europe. Unfortunately,
the convCNP model is of little practical use for climate
impact studies as it cannot produce spatially coherent sam-
ples (Dubois et al., 2020). A further limitation is that the
convCNP model is only suitable for univariate downscaling.
In many applications, for example agriculture and wildÔ¨Åre
prediction, it is necessary to accurately model the joint dis-
tribution of multiple variables.
In this study we build on the work of Vaughan et al. (2021)
to develop a new model for multivariate downscaling using a
convolutional latent neural process (convLNP; Foong et al.,
2020) to predict a multivariate stochastic process describing
spatial Ô¨Åelds of temperature and precipitation given low res-
olution predictor Ô¨Åelds. This model jointly downscales daily
maximum temperature and precipitation, quantiÔ¨Åes uncer-
tainty in predictions, generates coherent spatial samples and
can generate predictions at arbitrary locations at test time.
For this work the convLNP model is trained to downscale
gridded low-resolution ERA-5 reanalysis data to station
observations over Germany, with performance evaluated
at held out locations. To our knowledge this is the Ô¨Årst
attempt at multivariate downscaling by learning a transfer
function to a stochastic process. Our Ô¨Åndings indicate that
this model is suitable for generating multivariate projections
of temperature and precipitation suitable for use in climateMultivariate climate downscaling with latent neural processes
Figure 1. convLNP model architecture.
impact studies.
2. Methodology
Idea: Use a latent neural process Garnelo et al. (2018) to
learn a transfer function mapping a set of daily gridded low
resolution climate predictors to a multivariate stochastic pro-
cess indexed by longitude and latitude. Unlike the convCNP
model used by Vaughan et al. (2021), the latent neural pro-
cess is able to model correlations between locations (Dubois
et al., 2020). As translation equivariance is an important
inductive bias in multisite downscaling (Ba Àúno-Medina &
Guti¬¥errez, 2019), we use a convolutional neural process
model (Foong et al., 2020), which incorporates translation
equivariance into the latent neural process.
Experiment: ERA-Interim reanalysis data (Dee et al.,
2011) at 0.75 degree resolution is downscaled to weather
station data from the European Climate and Assessment
Dataset (Klok & Klein Tank, 2009) in a region from 6 to
16 degrees longitude and 47 to 55 degrees latitude (Figure
2a). This region is chosen as Germany has a high density of
station observations and complex topography in the Alps.
Predictors are the 0.75 degree resolution longitude-latitude
grids of 25 variables from the reanalysis dataset. Atmo-
spheric predictors are surface level mean and maximum
temperature, wind and precipitation. Tropospheric predic-
tors are humidity temperature and winds at 850hPa, 700hPa
and 500hPa. Invariant predictors are longitude, latitude,
sub-gridscale orography angle, anisotropy and standard de-
viation and geopotential. Day of year is also included repre-
sented by periodic transforms to capture seasonal variation.
The training set consists of reanalysis predictors from 1979-2002 together with observations from 397 stations (Figure
2a). The held out validation set consists of reanalysis predic-
tors from 2003-2008 and observations from 19 stations held
out from the training set (Figure 2b), assessing the ability of
the model to generalise to unseen times and locations. These
19 validation stations are selected from locations used in the
V ALUE downscaling intercomparison experiment as they
provide high quality observations in regions where statisti-
cal downscaling models are typically less accurate (Maraun
et al., 2015).
ConvLNP architecture Using a convLNP the predictive
distribution of maximum temperature and precipitation y(t)
at T locations x(t)speciÔ¨Åed by longitude, latitude, and ele-
vation coordinates is modelled as
p(fy(t)gT
t=1jfx(t)gT
t=1;C) =Z
p(zjC)Y
tN(y(t); (t)(x(t);z;C);(t)(x(t);z;C))dz
WhereCis the set of low resolution predictors on longitude-
latitude grids, zis a latent variable and N(y; ;)is a mul-
tivariate Gaussian distribution with mean and covariance
.
The convLNP is implemented in two stages: an encoder pa-
rameterizing p(zjC)mapping the low resolution predictors
to a distribution over latent variable zfollowed by a decoder
parameterizing the predictive distribution p(yjx;z)over y
at location xgiven z.
A schematic of the model architecture is shown in Figure 1.
In the encoder, the low resolution predictor grids are passedMultivariate climate downscaling with latent neural processes
through a convolutional network consisting of three residual
blocks. This outputs predictions of (;)parameterizing
an independent Gaussian latent variable at each gridpoint.
A sample from these Gaussian distributions is used as input
to the decoder, and passed through a further three residual
blocks to output predictions of the Ô¨Åve parameters speci-
fying a joint Gaussian distribution of temperature and pre-
cipitation: the mean for temperature and precipitation ( T,
Precip ), the variance for temperature and precipitation
(T,Precip ) and the covariance ( T;Precip ). These grid-
ded predictions are transformed to the required location x
using a set convolution layer (Gordon et al., 2019). Finally,
the Ô¨Åve predicted parameters together with the elevation
atxare passed through a MLP, adjusting for the effect of
subgrid-scale elevation. The model outputs predictions of
the parameters specifying a multivariate Gaussian over tem-
perature and precipitation at location x. For details of the
model architecture and training, see Appendix A.
Baseline: As to our knowledge this is the Ô¨Årst attempt at
multivariate downscaling by learning a transfer function to a
stochastic process, there is no directly comparable baseline
from previous work. We instead construct a simple trans-
fer learning baseline aiming to answer the following two
questions:
1.Is the skill of the convLNP model at unseen locations
any higher than training a single-site model and using
transfer learning to make predictions at other locations?
2.Does the multivariate convLNP predict intervariable
correlations more accurately than applying separate
univariate models?
The baseline model consists of two independent multi-layer
perceptrons for maximum temperature and precipitation,
each of which take spatial predictors from the low resolu-
tion grid together with the elevation at xas input, and output
predictions of maximum temperature or precipitation at that
location. In a comparison of downscaling models for sta-
tions over Europe (Guti ¬¥errez et al., 2019), many of the best
performing models use joined principal components (PCs)
of the predictor Ô¨Åelds as input to capture spatial coherence.
We therefore take the leading 19 joined principal compo-
nents (explaining >95% of the variance) of the predictor
Ô¨Åelds as input to the MLP model. For details of the baseline
architecture and training see Appendix B.
3. Results
Models are evaluated on multiple metrics. Marginal as-
pects are compared using mean absolute error (MAE) and
pearson (spearman) correlation for maximum temperature
(precipitation) between model predictions and observations.Table 1. Comparison of results for the convLNP and baseline mod-
els on the validation set.
METRIC CONV LNP B ASELINE
MAXIMUM TEMPERATURE
MAE (C) 1.95 2.33
PEARSON 0.96 0.96
CMD 1.29 E-4 6.43 E-4
DOF BIAS 0.02 0.16
PRECIPITATION
MAE (C) 2.51 2.64
SPEARMAN 0.64 0.64
CMD 0.02 0.06
DOF BIAS 1.79 2.24
MULTIVARIATE
CORRELATION BIAS 0.08 0.15
These metrics are averaged over the 19 validation stations.
Following (Widmann et al., 2019), two metrics are calcu-
lated speciÔ¨Åcally to evaluate how well spatial correlations
are reproduced. The correlation matrix distance (CMD;
Herdin, 2005) measures the similarity of two correlation
matrices, with a value of zero indicating that the correlation
structure is identical up to a scaling factor and a value of
one indicating that the correlation structures are very differ-
ent. The second spatial metric, bias in degrees of freedom
(DOF) compares the predicted to observed spatial degrees
of freedom (Widmann et al., 2019). Finally, predictions of
inter-variable correlations between maximum temperature
and precipitation are assessed by the absolute bias in spear-
man correlation between the variables (Maraun & Widmann,
2018) in model predictions compared to observations.
Results for each model on the validation set are shown
in Table 1. The convLNP model outperforms or equals
the performance of the baseline transfer learning model
on all metrics. For maximum temperature, the MAE is
1.95C for the convLNP compared to 2.33C for the baseline.
Pearson correlations are 0.96 for both models. Similarly for
precipitation, the MAE is lower for the convLNP model at
2.51mm compared to 2.64mm for the baseline, with equal
spearman correlations of 0.64.
Of particular interest in this study is how well the models
capture the spatial distribution of the downscaled variables.
For both maximum temperature and precipitation, the con-
vLNP outperforms the baseline on the CMD and DOF bias
metrics. Further insight into the ability of the model to re-
produce spatial Ô¨Åelds is gained by querying the predicted
stochastic process at 0.05 degree resolution over the domain.
Figure 2 shows an example of convLNP predictions for
maximum temperature (d,e,f) and precipitation (g,h,i) com-
pared to the low resolution reanalysis predictors of theseMultivariate climate downscaling with latent neural processes
Figure 2. convLNP model predictions for 12/01/2003, showing (a) training stations and elevation (b) validation stations and elevation,
(c) predicted maximum temperature-precipitation covariance, and low resolution reanalysis, predicted mean and predicted variance for
maximum temperature (d,e,f) and precipitation (g,h,i).
Ô¨Åelds. For both variables, large scale features are consistent
with the low resolution input. For maximum temperature,
predicted variance (model uncertainty) is highest in regions
with complex topography and areas where training data are
sparse, as expected. For precipitation, higher uncertainty is
seen towards the edges of the area of predicted precipitation,
especially in elevated regions. Locations at greater elevation
are predicted to be colder and receive higher precipitation.
The Ô¨Ånal metric to consider is how well the model pre-
dicts intervariable correlations. The bias in intervariable
correlations is substantially lower for the convLNP model
compared to the baseline. Predicted covariance is largest in
regions with complex topography (Figure 2c).4. Conclusion
We have presented a new approach for multivariate climate
downscaling using a convLNP. This model outperforms a
transfer learning baseline, and reproduces spatially realistic
high resolution Ô¨Åelds of maximum temperature and precipi-
tation with accurate intervariable correlations.
The convLNP model has a number of advantages over exist-
ing machine learning downscaling methods:
Application to new domains the convLNP can be
queried at new locations at test time and generate spatially
coherent predictions.
Uncertainty quantiÔ¨Åcation the model robustly quantiÔ¨Åes
uncertainty in predictions.Multivariate climate downscaling with latent neural processes
Multivariate downscaling the model predicts a joint dis-
tribution of multiple variables, making it suitable for appli-
cation to impact studies relying on accurate inter-variable
correlations.
Multiple areas remain for future work. Further veriÔ¨Åcation is
required with an ensemble of baselines, particularly quanti-
fying how well extreme values are represented and exploring
whether predictions are physically consistent. The convLNP
model will then be applied to climate impact studies where
multivariate projections are required, with an initial focus
on local wildÔ¨Åre risk projections.References
BaÀúno-Medina, J. and Guti ¬¥errez, J. M. The importance of in-
ductive bias in convolutional models for statistical down-
scaling. In Proceedings of the 9th International Workshop
on Climate Informatics: CI , 2019.
BaÀúno-Medina, J., Manzanas, R., and Guti ¬¥errez, J. M. Con-
Ô¨Åguration and intercomparison of deep learning neural
models for statistical downscaling. GeoscientiÔ¨Åc Model
Development , 13(4):2109‚Äì2124, 2020.
B¬®urger, G., Murdock, T., Werner, A., Sobie, S., and Cannon,
A. Downscaling extremes‚Äîan intercomparison of mul-
tiple statistical methods for present climate. Journal of
Climate , 25(12):4366‚Äì4388, 2012.
Cannon, A. J. Probabilistic multisite precipitation downscal-
ing by an expanded bernoulli‚Äìgamma density network.
Journal of Hydrometeorology , 9(6):1284‚Äì1300, 2008.
Cannon, A. J. Quantile regression neural networks: Im-
plementation in r and application to precipitation down-
scaling. Computers & geosciences , 37(9):1277‚Äì1284,
2011.
Chaudhuri, C. and Robertson, C. Cligan: A structurally sen-
sitive convolutional neural network model for statistical
downscaling of precipitation from multi-model ensem-
bles. Water , 12(12):3353, 2020.
Chollet, F. Xception: Deep learning with depthwise separa-
ble convolutions. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pp. 1251‚Äì
1258, 2017.
Dee, D. P., Uppala, S. M., Simmons, A., Berrisford, P., Poli,
P., Kobayashi, S., Andrae, U., Balmaseda, M., Balsamo,
G., Bauer, d. P., et al. The era-interim reanalysis: ConÔ¨Åg-
uration and performance of the data assimilation system.
Quarterly Journal of the royal meteorological society ,
137(656):553‚Äì597, 2011.
Dubois, Y ., Gordon, J., and Foong, A. Y . Neural
process family. http://yanndubs.github.io/
Neural-Process-Family/ , September 2020.
Foong, A. Y ., Bruinsma, W. P., Gordon, J., Dubois, Y .,
Requeima, J., and Turner, R. E. Meta-learning stationary
stochastic process prediction with convolutional neural
processes. arXiv preprint arXiv:2007.01332 , 2020.
Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F.,
Rezende, D. J., Eslami, S., and Teh, Y . W. Neural pro-
cesses. arXiv preprint arXiv:1807.01622 , 2018.
Gordon, J., Bruinsma, W. P., Foong, A. Y ., Requeima, J.,
Dubois, Y ., and Turner, R. E. Convolutional conditional
neural processes. arXiv preprint arXiv:1910.13556 , 2019.Multivariate climate downscaling with latent neural processes
Groenke, B., Madaus, L., and Monteleoni, C. Climalign:
Unsupervised statistical downscaling of climate variables
via normalizing Ô¨Çows. In Proceedings of the 10th Inter-
national Conference on Climate Informatics , pp. 60‚Äì66,
2020.
Guti¬¥errez, J. M., Maraun, D., Widmann, M., Huth, R., Her-
tig, E., Benestad, R., R ¬®ossler, O., Wibig, J., Wilcke, R.,
Kotlarski, S., et al. An intercomparison of a large en-
semble of statistical downscaling methods over europe:
Results from the value perfect predictor cross-validation
experiment. International journal of climatology , 39(9):
3750‚Äì3785, 2019.
Herdin, M., Czink, N., Ozcelik, H., and Bonek, E. Correla-
tion matrix distance, a meaningful measure for evaluation
of non-stationary mimo channels. In 2005 IEEE 61st Ve-
hicular Technology Conference , volume 1, pp. 136‚Äì140.
IEEE, 2005.
Hertig, E., Maraun, D., Bartholy, J., Pongracz, R., Vrac,
M., Mares, I., Guti ¬¥errez, J. M., Wibig, J., Casanueva, A.,
and Soares, P. M. Comparison of statistical downscaling
methods with respect to extreme events over europe: Vali-
dation results from the perfect predictor experiment of the
cost action value. International Journal of Climatology ,
39(9):3846‚Äì3867, 2019.
Klok, E. and Klein Tank, A. Updated and extended euro-
pean dataset of daily climate observations. International
Journal of Climatology: A Journal of the Royal Meteoro-
logical Society , 29(8):1182‚Äì1191, 2009.
Liu, Y ., Ganguly, A. R., and Dy, J. Climate downscaling
using ynet: A deep convolutional network with skip con-
nections and fusion. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Dis-
covery & Data Mining , pp. 3145‚Äì3153, 2020.
Maraun, D. and Widmann, M. Statistical downscaling and
bias correction for climate research . Cambridge Univer-
sity Press, 2018.
Maraun, D., Widmann, M., Guti ¬¥errez, J. M., Kotlarski, S.,
Chandler, R. E., Hertig, E., Wibig, J., Huth, R., and
Wilcke, R. A. Value: A framework to validate down-
scaling approaches for climate change studies. Earth‚Äôs
Future , 3(1):1‚Äì14, 2015.
Maraun, D., Huth, R., Guti ¬¥errez, J. M., Mart ¬¥ƒ±n, D. S.,
Dubrovsky, M., Fischer, A., Hertig, E., Soares, P. M.,
Bartholy, J., Pongr ¬¥acz, R., et al. The value perfect pre-
dictor experiment: evaluation of temporal variability. In-
ternational Journal of Climatology , 39(9):3786‚Äì3818,
2019.Misra, S., Sarkar, S., and Mitra, P. Statistical downscaling
of precipitation using long short-term memory recurrent
neural networks. Theoretical and applied climatology ,
134(3):1179‚Äì1196, 2018.
Vandal, T., Kodra, E., Ganguly, S., Michaelis, A., Nemani,
R., and Ganguly, A. R. Deepsd: Generating high reso-
lution climate change projections through single image
super-resolution. In Proceedings of the 23rd acm sigkdd
international conference on knowledge discovery and
data mining , pp. 1663‚Äì1672, 2017.
Vandal, T., Kodra, E., and Ganguly, A. R. Intercomparison
of machine learning methods for statistical downscaling:
the case of daily and extreme precipitation. Theoretical
and Applied Climatology , 137(1):557‚Äì570, 2019.
Vaughan, A., Tebbutt, W., Hosking, J. S., and Turner, R. E.
Convolutional conditional neural processes for local cli-
mate downscaling. GeoscientiÔ¨Åc Model Development
Discussions , pp. 1‚Äì25, 2021.
Wang, F., Tian, D., Lowe, L., Kalin, L., and Lehrter, J. Deep
learning for daily precipitation and temperature downscal-
ing. Water Resources Research , pp. e2020WR029308,
2021.
Widmann, M., Bedia, J., Guti ¬¥errez, J. M., Bosshard, T., Her-
tig, E., Maraun, D., Casado, M. J., Ramos, P., Cardoso,
R. M., Soares, P. M., et al. Validation of spatial variability
in downscaling results from the value perfect predictor
experiment. International Journal of Climatology , 39(9):
3819‚Äì3845, 2019.
A. convLNP model
A.1. Architecture
The encoder consists of three residual blocks with depth
separable convolutions (Chollet, 2017). Each block consists
of two layers of ReLU nonlinearities followed by a 2D con-
volutional layer with a kernel size of 3 and 128 intermediate
channels. The latent variable zhas 64 channels.
The decoder consists of a further three residual blocks with
architecture identical to above. This is followed by a set
convolution (Gordon et al., 2019), where the outputs of the
parameters on the low resolution grid are used as weights
for an exponentiated-quadratic kernel with learnable length
scale to make predictions at target location x.
Resulting predictions of parameters are then concatenated
with the elevation coordinate and passed through a MLP
with four hidden layers, each with 64 neurons, and ReLU
activations.Multivariate climate downscaling with latent neural processes
A.2. Training
The convLNP is trained for 100 epochs using Adam, with a
learning rate of 510 4. As the log-predictive likelihood
is not analytically tractable, we instead minimise the neural
process maximum likelihood objective (Foong et al., 2020),
deÔ¨Åned by
^LNPML =log(1
LLX
l=1TY
t=1p(y)(t)jx)(t);zl)
Where zlp(zjC). The number of samples Lfrom the
latent variable is set to 24.
B. Baseline model
The baseline MLP has 4 hidden layers each with 64 units
and ReLU nonlinearities. Both the maximum temeprature
and precipitation models are trained for 100 epochs using
Adam with a learning rate of 110 3minimising the mean
squared error.