Enhancing Reinforcement Learning-Based Control of
Wave Energy Converters Using Predictive Wave
Modeling
Vineet Gundecha1, Arie Paap2, Mathieu Cocho2, Sahand Ghorbanpour1,
Alexandre Pichard2, Ashwin Ramesh Babu1, Soumyendu Sarkar1∗
1Hewlett Packard Enterprise,2Carnegie Clean Energy
{vineet.gundecha, sahand.ghorbanpour, ashwin.ramesh-babu,
soumyendu.sarkar} @hpe.com
{apaap, mcocho, apichard} @carnegiece.com
Abstract
Ocean wave energy is a reliable form of clean, renewable energy that has been
under-explored compared to solar and wind. Wave Energy Converters (WEC)
are devices that convert wave energy to electricity. To achieve a competitive
Levelized Cost of Energy (LCOE), WECs require complex controllers to maximize
the absorbed energy. Traditional engineering controllers, like spring-damper,
cannot anticipate incoming waves, missing vital information that could lead to
higher energy capture. Reinforcement Learning (RL) based controllers can instead
optimize for long-term gains by being informed about the future waves. Prior
works have utilized incoming wave information, achieving significant gains in
energy capture. However, this has only been done via simulated waves (perfect
prediction), making them impractical in real-life deployment. In this work, we
develop a neural network based model for wave prediction. While prior works use
auto-regressive techniques, we predict waves using information available on-device
like position, acceleration, etc. We show that replacing the simulated waves with
the wave predictor model can still maintain the gain in energy capture achieved by
the RL controller in simulations.
1 Introduction
Burning fossil fuel is a leading cause of climate change responsible for 75% of global greenhouse gas
emissions UN (2023). To move away from fossil fuels, we need renewable and clean energy sources
like wind, solar, and wave energy. Ocean waves are a reliable source of energy.
Wave Energy Converters can harness this energy to produce electricity. These devices can either
be submerged a few meters under the ocean, or can be deployed as floating devices. In this work,
we focus on the CETO configuration Wikipedia contributors (2024). CETOs are submerged under
the water and tethered to the ocean floor with mooring legs. The purpose of the mooring legs is
two-fold: ensure CETO doesn’t drift with waves, but also capture power from the waves. As waves
roll over the device, they apply forces to it. In order to allow relative motion between the seabed
and the device, the the mooring legs have to extend to retract. This process is controlled by each
leg’s respective Power Take Off (PTO). Onboard the device, the mooring legs terminate onto a drum,
which is connected to an electrical generator. The generator, by varying the torque it applies to the
∗Corresponding author. †These authors contributed equally.
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.drum, controls how much and how fast its mooring leg extends or retracts, which dictates how the BA
moves. The power generated by each leg is proportional to the product between the leg’s extension /
retraction velocity, and the tension in the leg. The power generated by CETO is the sum of each leg’s
generated power. Passive controllers rely on hourly to half-hourly average values of the wave height
and time period (Tp) to perform this task, which means that the controller cannot anticipate and react
to optimise the devices’ response to individual waves.
Figure 1: The CETO 6 configuration of WEC.Deep Reinforcement Learning has been widely
and successfully used for continuous control
tasks. RL controllers have already been devel-
oped for wave energy converters (Sarkar et al.,
2023, 2022a,b,c, 2021), (Anderlini et al., 2016),
(Anderlini et al., 2020). In this work, we follow
the design in (Sarkar et al., 2022a), which uses a
RL controller trained with Proximal Policy Opti-
mization Schulman et al. (2017) with the GTrXL
Parisotto et al. (2020) transformer architecture
as the policy network.
Our contribution is as follows:
•We analyze the change in performance
with and without knowledge of incom-
ing waves.
•We then develop a wave predictor
model.
•We show that using the predictions
from the model causes negligible loss
in the RL controller performance.
2 RL Control with Wave Predictions
2.1 Reinforcement Learning for WEC
A standard reinforcement learning setup consists of an agent interacting with an environment in
discrete timesteps. At each step, the agent receives an observation, takes an action, and receives a
scalar reward. The agent’s behaviour is defined by a policy. The goal in reinforcement learning is to
maximize the sum of rewards over a time horizon. In the WEC environment, the observation space
consists of variables that define the movement of the device in 6 degrees of freedom, and information
about the waves. The action space is the controller force applied on the three legs. The reward is
the sum of instantaneous power generated in the three legs. The RL agent learns to take actions to
maximize the generated power. The RL agent is typically trained in a simulator where it learns from
thousands of episodes over different wave states.
Along with the position, velocity, acceleration of the device, and the extension the legs, (Sarkar et al.,
2022a) also feed the incoming wave elevation 3sec and 5sec into the future as one of the inputs to the
RL agent. While information of the incoming waves are available in the simulator, this is not readily
available on-device during deployment.
2.2 Wave predictions
We first analyze how useful knowledge of incoming waves is by training the RL controller without
feeding it to the agent. Figure 2 shows the RL agent training performance with and without the future
waves. Not using the future waves, causes around a 10% drop in absorbed energy, signifying that
knowledge of the incoming wave contributes significantly to the controller performance.
Peña-Sanchez et al. (2020) give a comprehensive comparison of using auto-regressive models to
forecast surface wave elevation. For CETO which is submerged 2 meters below the ocean surface,
even the instantaneous wave elevation is not readily available. The movement of WEC is essentially
a function of the force imparted by the waves flowing over it. This time-series of the position and
its derivatives can be used to predict the current and the future wave elevation. LSTM’s are an
2Figure 2: Percentage improvement in generated energy of the RL controller over the spring-damper controller
with and without future waves. When the agent has access to the incoming waves, it achieves around 20%
improvement as compared to 10% without it. The curves here are genereated while training the RL controller
established recurrent neural network architecture which can process multi-variate time-series data.
We train a LSTM network to predict wave the current wave elevation, and the wave elevation 3 and 5
seconds into the future.
2.3 Data collection and training
Waves in the ocean are characterized by a significant wave height (Hs) and time period (Tp). De-
pending on the geographical location, the wave height typically ranges from 1-4 meters and the time
period ranges from 6-16 seconds. We collect 16 runs of 500 seconds of each wave state by running
the default spring-damper controller in the simulator. The input variables to the LSTM Hochreiter
and Schmidhuber (1997) network are the position, velocity, acceleration of the device, and the total
force on each of the three legs. We feed in a time-series of these variables corresponding to past 128
seconds. The network is a one-layer LSTM with 512 hidden units, trained with the mean squared
error loss, with a batchsize of 64 and learning rate of 1e-4 using the AdamW Loshchilov and Hutter
(2017) optimizer. We reduce the learning rate by 0.1 whenever the validation loss plateaus. Figure 3
shows the prediction results at 0, 3 and 5 seconds. While the network prediction is very close to the
ground truth for 0 and 3 seconds, it deviates as move further into the future at 5sec. However, for
controlling the WEC just the phase of the prediction is also very helpful.
3 Results
We test the wave predictor by feeding it’s predictions into the RL agent that is trained on simulated
(perfect) waves as described earlier. Table 1 contains the percentage gain in power over the spring-
damper achieved by the RL agent when using the predicted waves as input. We see that there’s
under 2% drop in performance over the simulated waves. In Figure 4, the RL agent’s action and the
generated power using predicted waves closely follows the same using simulated waves.
Tp (s) W/o future waves (%) With simulated waves (%) With predicted waves(%) Change
6 -4.9 10.27 9.1 1.17
8 9.5 19.8 18.5 1.3
10 20.2 42.2 39.1 3.1
12 27.1 43.65 41.64 2.01
Table 1: Percentage change in generated power using the RL controller over spring-damper for wave height of
1m and time period ranging from 6-12 seconds
3Figure 3: LSTM predictions v/s ground truth simulated waves. Top row represents the instantaneous wave
elevation while the middle and bottom represent wave elevation 3sec and 5sec into the future
Figure 4: Top: Comparing the RL controller action (controller force) using simulated waves vs predicted waves,
bottom: the instantaneous generated power
4 Conclusion and Future Work
We develop a neural network based wave predictor and the demonstrate the effectiveness of using
a wave predictor model on the performance of the RL controller. The LSTM wave predictor can
competently predict incoming waves, causing only a small loss in performance. We look forward
to first testing this in an emulator tank, and then deploying it in the ocean. In the future, we would
like to explore the option of training the RL controller directly without explicitly feeding in the wave
predictions, but using an auxiliary loss that can direct the agent to utilize future wave information.
4References
UN, fossilfuel, Wikipedia (2023). URL: https://www.un.org/en/climatechange/science/
causes-effects-climate-change .
Wikipedia contributors, Ceto — Wikipedia, the free encyclopedia, 2024. URL: https://
en.wikipedia.org/w/index.php?title=CETO&oldid=1237520279 , [Online; accessed 29-
August-2024].
S. Sarkar, V . Gundecha, S. Ghorbanpour, A. Shmakov, A. Ramesh Babu, A. Naug, A. Pichard,
M. Cocho, Function approximation for reinforcement learning controller for energy from spread
waves, in: E. Elkind (Ed.), Proceedings of the Thirty-Second International Joint Conference
on Artificial Intelligence, IJCAI-23, International Joint Conferences on Artificial Intelligence
Organization, 2023, pp. 6201–6209. URL: https://doi.org/10.24963/ijcai.2023/688 .
doi:10.24963/ijcai.2023/688 , aI for Good.
S. Sarkar, V . Gundecha, A. Shmakov, S. Ghorbanpour, A. R. Babu, P. Faraboschi, M. Cocho,
A. Pichard, J. Fievez, Multi-agent reinforcement learning controller to maximize energy efficiency
for multi-generator industrial wave energy converter, Proceedings of the AAAI Conference on
Artificial Intelligence 36 (2022a) 12135–12144. URL: https://ojs.aaai.org/index.php/
AAAI/article/view/21473 . doi: 10.1609/aaai.v36i11.21473 .
S. Sarkar, V . Gundecha, S. Ghorbanpour, A. Shmakov, A. R. Babu, A. Pichard, M. Cocho, Skip
training for multi-agent reinforcement learning controller for industrial wave energy converters, in:
2022 IEEE 18th International Conference on Automation Science and Engineering (CASE), IEEE,
2022b, pp. 212–219.
S. Sarkar, V . Gundecha, S. Ghorbanpour, A. Shmakov, A. R. Babu, A. Pichard, M. Cocho, Function
approximations for reinforcement learning controller for wave energy converters (2022c).
S. Sarkar, V . Gundecha, A. Shmakov, S. Ghorbanpour, A. R. Babu, P. Faraboschi, M. Cocho,
A. Pichard, J. Fievez, Multi-objective reinforcement learning controller for multi-generator
industrial wave energy converter, in: NeurIPs Tackling Climate Change with Machine Learning
Workshop, 2021.
E. Anderlini, D. I. Forehand, P. Stansell, Q. Xiao, M. Abusara, Control of a point absorber using
reinforcement learning, IEEE Transactions on Sustainable Energy 7 (2016) 1681–1690.
E. Anderlini, S. Husain, G. G. Parker, M. Abusara, G. Thomas, Towards real-time reinforcement
learning control of a wave energy converter, Journal of Marine Science and Engineering 8 (2020)
845.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithms,
arXiv preprint arXiv:1707.06347 (2017).
E. Parisotto, F. Song, J. Rae, R. Pascanu, C. Gulcehre, S. Jayakumar, M. Jaderberg, R. L. Kaufman,
A. Clark, S. Noury, et al., Stabilizing transformers for reinforcement learning, in: International
Conference on Machine Learning, PMLR, 2020, pp. 7487–7498.
Y . Peña-Sanchez, A. Mérigaud, J. V . Ringwood, Short-term forecasting of sea surface elevation
for wave energy applications: The autoregressive model revisited, IEEE Journal of Oceanic
Engineering 45 (2020) 462–471. doi: 10.1109/JOE.2018.2875575 .
S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (1997) 1735–1780. URL:
https://doi.org/10.1162/neco.1997.9.8.1735 . doi: 10.1162/neco.1997.9.8.1735 .
I. Loshchilov, F. Hutter, Fixing weight decay regularization in adam, CoRR abs/1711.05101 (2017).
URL: http://arxiv.org/abs/1711.05101 .arXiv:1711.05101 .
5