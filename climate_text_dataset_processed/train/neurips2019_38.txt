A Deep Learning-based Framework for the Detection
of Schools of Herring in Echograms
Alireza Rezvanifar* 1Tunai Porto Marques* 1Melissa Cote1Alexandra Branzan Albu1Alex Slonimer2
Thomas Tolhurst2Kaan Ersahin2Todd Mudge2St´ephane Gauthier3
Abstract
Tracking the abundance of underwater species
is crucial for understanding the effects of cli-
mate change on marine ecosystems. Biolo-
gists typically monitor underwater sites with
echosounders and visualize data as 2D images
(echograms); they interpret these data manually
or semi-automatically, which is time-consuming
and prone to inconsistencies. This paper proposes
a deep learning framework for the automatic de-
tection of schools of herring from echograms. Ex-
periments demonstrated that our approach outper-
forms a traditional machine learning algorithm
that uses hand-crafted features. Our framework
could easily be expanded to detect more species
of interest to sustainable ﬁsheries.
1. Introduction
The study of acoustic backscatter is commonly utilized by
biologists as a means to monitor underwater sites in a thor-
ough and non-invasive manner. Different species possess
different acoustic properties, thus producing different re-
sponses. Acoustic survey data are typically interpreted
via time-consuming manual or semi-automatic approaches
prone to inconsistencies, using expensive softwares such
as Echoview (https://www.echoview.com/). In this paper,
we apply deep learning methods for automatically detecting
speciﬁc biological targets in acoustic survey data. Our goals
are to improve data processing and interpretation for species
abundance tracking and environmental monitoring, and to
provide a tool that reliably supports analyses of the effects
of climate change. We present a case study on schools of
herring.
*Equal contribution1University of Victoria, Victoria, Canada
2ASL Environmental Sciences, Victoria, Canada3Fisheries and
Oceans Canada, Victoria, Canada. Correspondence to: Alexandra
Branzan Albu <aalbu@uvic.ca >.
Tackling Climate Change with Machine Learning Workshop at
the33rdConference on Neural Information Processing Systems
(NeurIPS 2019), Vancouver, Canada.Acoustic data are obtained via multifrequency echosounders,
such as the Acoustic Zooplankton Fish Proﬁler (AZFP) [ 1],
and visualized in 2D, with depth along the y-axis and time
along the x-axis, producing an echogram. A color code
represents the echo amplitude, usually in the form of volu-
metric backscatter strength. Visual structures indicate geo-
logical and biological elements. Acoustic-scattering models
provide a link between echograms and the size/abundance
distribution of the animals [ 2]. Echograms are traditionally
analyzed using statistical characteristics of the aggregations
of organisms [ 3]. Feature-based classiﬁcation methods com-
monly favor a classical machine learning [ 4,5,6,7,8,9]
paradigm and use hand-crafted features. Deep learning,
which has been shown to be very effective at various tasks
in computer vision such as object detection and recogni-
tion, has yet to permeate echogram analysis. Only a few
methods deal with identifying ﬁsh from echograms utilizing
deep learning and are quite limited in the nature of their
experiments [ 10] or data [ 11]. A few additional works have
applied deep learning to the detection of mobile marine life
in sonar data [ 12,13,14], which typically have a higher
resolution than echograms.
Our contributions are two-fold. Firstly, we propose a novel
application of deep learning techniques for ﬁsh detection
from echograms. Secondly, our framework automates acous-
tic survey analyses; an automatic approach will reduce pro-
cessing times, required man-power, and inconsistencies in
the results. Our approach also has the potential to be scaled
to handle additional underwater species. Biologists collect
large quantities of echograms so the amount of data is not a
limitation for the training of a deep learning-based frame-
work; however, the annotation process is a tedious task.
We show that, even using a smaller annotated dataset, deep
learning-based solutions can outperform a classical machine
learning method. Our promising results indicate that with
the annotation of more data, deep learning can be efﬁciently
applied in underwater acoustic analysis.
2. Technical Approach
Thanks to the emergence of convolutional neural networks
(CNNs) and the existence of large annotated datasets likeA Deep Learning-based Framework for the Detection of Schools of Herring in Echograms
Figure 1. ROI extractor pipeline and sample processed echograms at each stage.
MS COCO [ 15], PASCAL VOC [ 16], and ImageNet [ 17],
object detection is now well addressed for natural scenes.
CNNs have been utilizing different architectures to ﬁnd
regions of interest (ROIs) in images and to classify them.
While earlier networks like R-CNN [ 18] and Fast R-CNN
[19] were using selective search [ 20] for ﬁnding ROIs, more
recent ones like Faster R-CNN [ 21] use dedicated networks
(i.e., Region Proposal Networks (RPNs)) to that end. Unfor-
tunately, this approach is not feasible in applications outside
the natural scene scope, including ours, where there is in-
sufﬁcient box-annotated data for training RPNs. We thus
designed an ROI extractor based on high level information
on the intensity and morphology of schools of herring. We
introduce the proposed ROI extractor in Section 2.2 and
then describe the deep learning-based classiﬁcation phase
in Section 2.3. Our data are described in Section 2.1.
2.1. Dataset
Our dataset is composed of 100 multifrequency echograms
generated from data collected with an AZFP device de-
ployed on the surface of the water looking downward, in the
Discovery Passage off Vancouver Island, British Columbia,
Canada in 2015. The AZFP transducer was calibrated in
four frequency channels (67, 125, 200, and 455 KHz), gen-
erating a total of four echograms. We use all four echograms
as input, as they may convey complementary information.
Each echogram resolution is 1200 pixels in the x-axis (rep-
resenting one hour of measurements) by 571 pixels in the
y-axis (correspoding to a depth range of 50 meters). The
dataset is divided into 70 echograms for training (which
itself is divided in 80% training, 20% validation) and 30
echograms for testing.
2.2. ROI Extractor
Two high level features common to all schools of herring
are presumed: 1) a strong intensity core visible in most
echograms, 2) a vertical elongated shape. Figure 1 shows
the ROI extractor pipeline and sample intermediate outputs
after each step. We ﬁrst apply a median ﬁlter along the
time direction of the echograms to smooth out the spiky
variations and remove some of the undesirable signals suchas small ﬁsh. The denoised echograms are then binarized
using adaptive thresholding [ 22], and processed via morpho-
logical opening and closing to remove small isthmuses and
protrusions and ﬁll in small holes. We create a binary score
matrix, indicating the strength of the potential ROIs, by sum-
ming the four processed binarized echograms and retaining
the pixels that have a value greater than 2, thus retrieving
objects present in at least three frequency channels. Con-
nected components are then ﬁltered for size and orientation:
we discard very small regions (less than 50 pixels) as well
as regions with an orientation smaller than 60 degrees with
respect to the horizontal axis, as those cannot be schools of
herring. The remaining connected components constitute
the ﬁnal ROIs. The ROI bounding boxes are passed to the
classiﬁcation stage (Section 2.3) for further analysis.
We do not seek a high precision for ROIs at this stage, as
they will be further reﬁned in the next step. However, we do
seek a high recall; if a school of herring is not encompassed
by an ROI, it cannot be detected in subsequent steps.
2.3. Classiﬁcation
The classiﬁcation stage determines whether each ROI repre-
sents a school of herring or background. We compare three
deep learning-based architectures, which automatically ex-
tract features (without any contextual information) from
ROIs and classify them: ResNet [ 23], DenseNet [ 24] and
InceptionV3 [25].
Each echogram might contain multiple instances of schools
of herring ( positive samples) and background ( negative sam-
ples). To obtain meaningful and realistic negative samples
(avoiding the use of random crops), we used the ROI extrac-
tor outputs for the 70 training echograms as follows. The
ROI bounding boxes are compared with the positive samples
from the ground truth and the intersection-over-union (IoU),
i.e. ratio between areas of overlap and union, is calculated.
ROI bounding boxes with less than 0:4of IoU score are used
as negative samples and the others as positive samples. We
follow the 1:2 ratio between positive and negative samples
used in the popular work of Girshick et al. [19].A Deep Learning-based Framework for the Detection of Schools of Herring in Echograms
Figure 2. Results of the detection framework using DenseNet. Black: regions classiﬁed as background; green: ground truth bounding
boxes; red: regions classiﬁed as schools of herring. All detections are correct in the ﬁrst two samples (left and center), while one ROI
bounding box was incorrectly classiﬁed in the right-most echogram.
3. Results and Discussion
The two main components of the framework (ROI extrac-
tor and classiﬁer) are evaluated separately using precision,
recall, and F1-score metrics.
3.1. ROI Extractor Evaluation
Since we are dealing with bounding boxes, we make use of
an IoU threshold between the ROIs and the ground truth to
determine true positives, false positives, and false negatives.
We consider a true positive when an ROI bounding box
has an IoU with the ground truth higher than the threshold.
Results for the 100 echograms are shown in Table 1.
Table 1. Precision, recall and F1-score of the ROI extractor for
different IoU thresholds.
IoU Threshold Precision Recall F1-Score
0.0 0.173 0.931 0.292
0.2 0.171 0.917 0.288
0.4 0.155 0.834 0.262
The most relevant metric here is the recall, as ROIs are
further classiﬁed in the next stage and the classiﬁer can-
not recover a school missed by the ROI extractor. Table 1
shows that the performance increases as the IoU threshold
is lowered. The small difference in recall (less than 10 p.p.)
between IoU thresholds 0.0 and 0.4 shows that most of the
ROI bounding boxes have a signiﬁcant overlap with the
ground truth. Ideally, the incorrect bounding boxes will be
identiﬁed as background in the next step, classiﬁcation.
3.2. Framework Evaluation
This section assesses the complete framework as the out-
put of the ROI extractor are used as input to the classiﬁers.
Table 2 shows the classiﬁcation results for each tested net-
work architecture over the test set (30 echograms) for an
IoU threshold of 0.4, while highlighting the best results in
bold. We compare our results with a baseline method based
on traditional hand-crafted features engineered for ﬁsheries
research (ratio between minor to major axis, mean inten-
sity, eccentricity, and circularity of the region) and classical
machine learning (support vector machine (SVM) classiﬁer
[26] with a linear kernel).
The DenseNet architecture achieves the best balance be-tween metrics, and outperforms the baseline method. Figure
2 shows sample results of the framework using DenseNet.
In most of the echograms (as exampliﬁed by the left and cen-
ter images of Figure 2), the ROI extractor was able to detect
bounding boxes around the positive samples, followed by a
correct classiﬁcation by the DenseNet. However, the classi-
ﬁer also incorrectly classiﬁed some samples, as illustrated
in the right-most image of Figure 2. Experiments where the
IoU threshold was set to 0.0 in the classiﬁcation phase saw
a recall by the framework that reached the maximum value
of 0.93 (bounded by the ROI extractor), attesting to the efﬁ-
ciency of the trained classiﬁer. However, the usage of such a
low IoU threshold is not recommended, given that any ROI
bounding box that at least touches a positive sample could
be considered as a school of herring candidate. Higher IoU
thresholds guarantee that the detections are more reliable.
Table 2. Precision, recall and F1-score of the full pipeline using
different CNN-based architectures for IoU thresholds of 0.4.
Architecture Precision Recall F1-Score
ResNet50 [23] 0.77 0.85 0.81
DenseNet201 [24] 0.78 0.85 0.82
InceptionNet [25] 0.81 0.81 0.81
Baseline (SVM) [26] 0.51 0.78 0.62
4. Conclusion
We propose a novel framework that assists the study of
species’ abundance in underwater sites using acoustic data.
Our ﬁrst case study is focused on the detection of schools
of herring in echograms using two main components: a
novel ROI extractor and a deep learning-based image classi-
ﬁer. The framework achieved a good performance for IoU
thresholds (i.e., 0.4) which guaranteed meaningful detec-
tions. This framework will assist in the interpretation of
large amounts of raw acoustic data. Although this proof-of-
concept focuses on schools of herring, it is scalable to other
aquatic species that can be monitored using echosounders
(e.g., zooplankton, salmon). The future substitution of our
ROI extractor by generic ROI calculation systems (e.g.,
RPNs [ 21]) will simplify the expansion of detectable classes
to the gathering of new visual samples. The ability to mea-
sure the abundance of such subjects is paramount for the
study of the effects of water temperature shifts caused by
climate change-related phenomena.A Deep Learning-based Framework for the Detection of Schools of Herring in Echograms
References
[1]David Lemon, Paul Johnston, Jan Buermans, Eduardo
Loos, Gary Borstad, and Leslie Brown. Multiple-
frequency moored sonar for continuous observations
of zooplankton and ﬁsh. In 2012 Oceans , pages 1–6.
IEEE, 2012.
[2]Mark V Trevorrow. The use of moored inverted echo
sounders for monitoring meso-zooplankton and ﬁsh
near the ocean surface. Canadian Journal of Fisheries
and Aquatic Sciences , 62(5):1004–1018, 2005.
[3]Timothy K Stanton. 30 years of advances in active
bioacoustics: A personal perspective. Methods in
Oceanography , 1:49–77, 2012.
[4]P LeFeuvre, GA Rose, R Gosine, R Hale, W Pear-
son, and R Khan. Acoustic species identiﬁcation in
the Northwest Atlantic using digital image processing.
Fisheries Research , 47(2-3):137–147, 2000.
[5]Ariel G Cabreira, Mart ´ın Tripode, and Adri ´an Madiro-
las. Artiﬁcial neural networks for ﬁsh-species identiﬁ-
cation. ICES Journal of Marine Science , 66(6):1119–
1129, 2009.
[6]Aymen Charef, Seiji Ohshimo, Ichiro Aoki, and
Natheer Al Absi. Classiﬁcation of ﬁsh schools based
on evaluation of acoustic descriptor characteristics.
Fisheries Science , 76(1):1–11, 2010.
[7]Hugo Robotham, Paul Bosch, Juan Carlos Guti ´errez-
Estrada, Jorge Castillo, and Inmaculada Pulido-Calvo.
Acoustic identiﬁcation of small pelagic ﬁsh species in
Chile using support vector machines and neural net-
works. Fisheries Research , 102(1-2):115–122, 2010.
[8]St´ephane Gauthier, Johannes Oeffner, and Richard L
ODriscoll. Species composition and acoustic signa-
tures of mesopelagic organisms in a subtropical conver-
gence zone, the New Zealand Chatham Rise. Marine
Ecology Progress Series , 503:23–40, 2014.
[9]Niall G Fallon, Sophie Fielding, and Paul G Fernan-
des. Classiﬁcation of Southern Ocean krill and iceﬁsh
echoes using random forests. ICES Journal of Marine
Science , 73(8):1998–2008, 2016.
[10] Yudai Hirama, Soichiro Yokoyama, Tomohisa Ya-
mashita, Hidenori Kawamura, Keiji Suzuki, and
Masaaki Wada. Discriminating ﬁsh species by an Echo
sounder in a set-net using a CNN. In 21st Asia Paciﬁc
Symposium on Intelligent and Evolutionary Systems
(IES) , pages 112–115. IEEE, 2017.[11] Yue Shang and Jianlong Li. Study on echo features and
classiﬁcation methods of ﬁsh species. In 10th Interna-
tional Conference on Wireless Communications and
Signal Processing (WCSP) , pages 1–6. IEEE, 2018.
[12] L. Liu, H. Lu, Z. Cao, and Y . Xiao. Counting ﬁsh in
sonar images. In 25th IEEE International Conference
on Image Processing (ICIP) , pages 3189–3193. IEEE,
2018.
[13] Geoff French, Michal Mackiewicz, Mark Fisher, Mike
Challiss, Peter Knight, Brian Robinson, and Angus
Bloomﬁeld. Jellymonitor: automated detection of jel-
lyﬁsh in sonar images using neural networks. In 14th
IEEE International Conference on Signal Processing
(ICSP) , pages 406–412. IEEE, 2018.
[14] Dmitry Glukhov, Rykhard Bohush, Juho M ¨aki¨o, and
Tatjana Hlukhava. A joint application of fuzzy logic
approximation and a deep learning neural network to
build ﬁsh concentration maps based on sonar data. In
2nd International Workshop on Computer Modeling
and Intelligent Systems (CMIS) , pages 133–142, 2019.
[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C Lawrence Zitnick. Microsoft coco: Common objects
in context. In European Conference on Computer
Vision , pages 740–755. Springer, 2014.
[16] Mark Everingham, Luc Van Gool, Christopher KI
Williams, John Winn, and Andrew Zisserman. The
Pascal Visual Object Classes (VOC) challenge. Inter-
national Journal of Computer Cision , 88(2):303–338,
2010.
[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information
Processing Systems , pages 1097–1105, 2012.
[18] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jiten-
dra Malik. Rich feature hierarchies for accurate object
detection and semantic segmentation. In IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR) , pages 580–587, 2014.
[19] Ross Girshick. Fast r-cnn. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) ,
pages 1440–1448, 2015.
[20] Jasper RR Uijlings, Koen EA Van De Sande, Theo
Gevers, and Arnold WM Smeulders. Selective search
for object recognition. International Journal of Com-
puter Vision , 104(2):154–171, 2013.A Deep Learning-based Framework for the Detection of Schools of Herring in Echograms
[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. Faster r-cnn: Towards real-time object detection
with region proposal networks. In Advances in Neural
Information Processing Systems , pages 91–99, 2015.
[22] Derek Bradley and Gerhard Roth. Adaptive thresh-
olding using the integral image. Journal of Graphics
Tools , 12(2):13–21, 2007.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
InIEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 770–778, 2016.
[24] Gao Huang, Zhuang Liu, Laurens Van Der Maaten,
and Kilian Q Weinberger. Densely connected convo-
lutional networks. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 4700–
4708, 2017.
[25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. In IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 2818–2826, 2016.
[26] Corinna Cortes and Vladimir Vapnik. Support-vector
networks. Machine Learning , 20(3):273–297, 1995.