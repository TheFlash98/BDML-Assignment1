Intermediate and Future Frame Prediction of Geostationary Satellite Imagery
With Warp and Refine Network
Minseok Seo1, Yeji Choi1*, Hyungon Ryu2, Heesun Park3Hyungkun Bae1, Hyesook Lee3,
Wanseok Seo2,
1SI Analytics,2NVIDIA,3National Institute of Meteorological Sciences (NIMS)
*yejichoi@si-analytics.ai
Abstract
Geostationary satellite imagery has applications in climate
and weather forecasting, planning natural energy resources,
and predicting extreme weather events. For precise and ac-
curate prediction, higher spatial and temporal resolution of
geostationary satellite imagery is important. Although recent
geostationary satellite resolution has improved, the long-term
analysis of climate applications is limited to using multiple
satellites from the past to the present due to the different res-
olutions. To solve this problem, we proposed warp and refine
network (WR-Net). WR-Net is divided into an optical flow
warp component and a warp image refinement component.
We used the TV-L1 algorithm instead of deep learning-based
approaches to extract the optical flow warp component. The
deep-learning-based model is trained on the human-centric
view of the RGB channel and does not work on geostation-
ary satellites, which is gray-scale one-channel imagery. The
refinement network refines the warped image through a multi-
temporal fusion layer. We evaluated WR-Net by interpolation
of temporal resolution at 4 min intervals to 2 min intervals in
large-scale GK2A geostationary meteorological satellite im-
agery. Furthermore, we applied WR-Net to the future frame
prediction task and showed that the explicit use of optical
flow can help future frame prediction.
Introduction
Weather prediction using geostationary satellite imagery in-
directly contributes to the earth‚Äôs environment in various
fields such as planning natural energy resources (Pathak
et al. 2022) and extreme weather event prediction.
Physically-based numerical weather prediction models have
traditionally been used to analyze and forecast weather and
climate, but recent developments in deep learning-based
models have made data-driven methods possible to use for
weather prediction. In the era of the climate crisis, accurate
weather prediction is essential to prepare the sudden disas-
ters and to maximize the use of renewable energy such as
wind and solar power. Therefore, the spatial and temporal
resolution of forecasting results is important to appropriate
countermeasures. Recently, a geostationary-satellite resolu-
tion has been improved with 1-2 minute intervals with a
*corresponding author
Copyright ¬© 2022, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.pixel size of 1-2 km. However, to integrate multiple satel-
lites from the past to the present for long-term analysis for
climate applications, there is a limit to high-resolution anal-
ysis because most of the past geostationary satellites have a
lower resolution of 10 to 15 minutes.
To solve this problem, Vandal and Nemani (Vandal and
Nemani 2021) proposed a temporal interpolation of geosta-
tionary satellite imagery with optical flow approach based on
Super SloMo (Jiang et al. 2018). In the Super SloMo based
method, when images ItandIt+1are given, the interpola-
tion frame Iiis predicted through optical flow estimation
and warp between them. This method achieved state-of-the-
art, but does not extract the optical flow properly when a
sample that has not been learned in the training phase is in-
put and is vulnerable to intensity changes such as brightness
changes.
To solve this problem, video frame interpolation meth-
ods using pre-trained weights for PWC-Net (Sun et al. 2018)
and FlowNet (Dosovitskiy et al. 2015) have been proposed.
However, PWC-Net and FlowNet trained on RGB channels
of human-centric view do not work in geostationary satellite
imagery.
In this paper, we propose Warp and Refine Network (WR-
Net). WR-Net is divided into optical flow estimation and
warp component and refinement component. In the optical
flow estimation and warp component, since deep learning-
based supervised optical flow estimation methods cannot be
used, we estimate and warp the optical flow through the to-
tal variation-L1 algorithm, a non-parametric approach. Af-
ter that, the refinement network refines the intensity change
through a multi-temporal fusion of the warp frame with It
andIt+1to refine the intensity change, such as the bright-
ness change. We verified the WR-Net in large-scale GK2A
geostationary meteorological satellite imagery, and as a re-
sult achieved state-of-the-art.
Furthermore, we extend WR-Net to future frame predic-
tion. Our motivation is that optical flow includes information
such as movement direction and speed, and this will be ef-
fective not only for video frame interpolation but also for fu-
ture frame prediction. We predicted the frame after 90 min
through WR-Net and evaluated it qualitatively. As a result
of qualitative analysis, it was shown that the explicit use of
optical flow is helpful for future frame prediction. Our con-
tributions can be summarized as follows:IR10.5 (ùêº1)
IR10.5 (ùêº2)Optical Flow 
Estimation
warp
Pseudo IR10.5 ( ùë∞ùíë)
Final IR10.5 ( ùë∞ùíë)Multi -
Temporal 
Fusion 
LayerQùë¢ùëíùëüùë¶
ùêæùëíùë¶ùëâùëéùëôùë¢ùëí
Encoder DecoderRefinement 
NetworkFigure 1: An overview of our proposed WR-Net. WR-Net consists of an optical flow warp component and a refinement com-
ponent. The optical flow is extracted through the TV-L1 algorithm. The image warp with the extracted optical flow is refined
through a multi-temporal fusion network and a refinement network.
‚Ä¢ We propose WR-Net to extract optical flow using TV-L1
algorithm and warp and refine it.
‚Ä¢ Our WR-Net achieves state-of-the-art in geostationary
satellite imagery video frame interpolation.
‚Ä¢ To the best of our knowledge, for the first time, opti-
cal flow was explicitly applied to deep learning-based
weather forecasting.
Method
In this section, we divide our proposed Warp-and-Refine
Networks (WR-Net) into three components and describe
them in detail. The first component, optical flow warp, es-
timates the optical flow between the Itimage and the It+1
image, and warps the ItorIt+1image with the estimated
optical flow to predict intermediate or future frames. There-
fore, we propose and use a multi-temporal fusion layer to
correct Iinter, Ifuture by referring to the intensity values of
highly correlated parts in ItandIt+1images. The multi-
temporal fusion feature is input to the U-Net based Refine
Network to refine the intensity value.
Optical Flow Warp
The optical flow warp method is a recent learning approach
in the field of video frame interpolation. However, as shown
in Fig. 2-(a), the existing optical flow estimation network
is designed and trained on the human-centric view of RGB
images, so the performance of geostationary satellite im-
agery is greatly reduced. Therefore, we estimate the opti-
cal flow by using the GPU accelerated total variation-L1
(TVL1) (Wedel et al. 2009) algorithm as shown in Fig. 2-
(b) instead of the deep learning-based optical flow estimate
network.
When there is Itframe and It+1, the intermediate frame
Iiand future frame Ifcan be synthesized through Eq. (1)
and Eq. (2), respectively.
(a) PWC -Net Pretrained Weight (b) TVL1Figure 2: Qualitative comparison of optical flow extracted
with PWC-Net and TV-L1 algorithms.
Ii=g(It, Op (It+1, It)√óŒ±), (1)
If=g(It+1, Op (It+1, It)√óŒ±), (2)
where Op(., .)is a TV-L1 optical flow estimation algorithm.
Theg(.)is a backward warping function. Parameter Œ±is the
interval between time tandt+1. For example, if Œ±is set 0.5,
an intermediate frame It+0.5can be generated, or a future
frame It+1.5can be generated.
Multi Temporal Fusion Network
The warp based on optical flow is not an optimal value be-
cause IiorIfare generated by linear interpolation of Itor
It+ 1. Therefore, we need to refine the pixel intensity val-
ues that cannot be expressed by linear interpolation. There-
fore, we refine the intensity value of IiorIfusing the infor-
mation of ItandIt+1.
We propose a multi-temporal fusion layer based on Non-
local Neural Networks to converge multi-temporal informa-
tion. Figure 3 shows the structure of a multi-temporal fusionùêº!Encoding
ùêº!"#Encoding
ùêº$EncodingScale & SoftmaxRefine-NetFigure 3: Overview of multi-temporal fusion layers. In video
frame interpolation, ItandIt+1are input as query and key.
In future frame prediction, replace Iiwith the predicted fu-
ture frame.
layer. As shown in the Fig. 3, the features with high corre-
lation between ItandIt+1are weighted and multiplied by
Ii. Through this process, Iican be modeled for correlation
between ItandIt+1.
Refinement Network
Multi-temporal fusion features are input to U-Net (Ron-
neberger, Fischer, and Brox 2015) based Refinement Net-
work. Refinement Network restores the input feature to the
ground truth IigtorIfgt. When the input images Itand
It+1are given, the intermediate frame prediction‚Äôs objective
function of Warp and Refine Network is as follows:
lr=||Iigt‚àíRefine (MTF (It+1, It, Ii))||1, (3)
where MTF (., ., . )is a Multi temporal fusion network. The
Refine (.)is a refinement network. For future frame predic-
tion, simply change IitoIfin Eq. (3).
Experiments
Dataset We trained and evaluated WR-Net on a large-
scale GK2A geostationary satellite weather observation im-
agery to evaluate the intermediate and future frame predic-
tion performance. The GK2A (GEO-KOMPSAT-2A) satel-
lite dataset (Chung et al. 2020) from August 2020 to July
2021 is used for training and validation. The dataset is in 2-
minute intervals over the East-Asia regions with 2 km spatial
resolution . Since the GK2A dataset is very large-scale, only
1, 6, 11, 16, 21, and 26 days were selected and used.
We strictly followed the GK2A official user manual for
pre-processing the physical value data of GK2A. The image
size of the GK2A dataset is 1950 √ó1550, and in our ex-
periments, only channels of infrared ray (IR) 10.5 ¬µm, short
wave infrared ray (SW) 0.38 ¬µm, and water vapor (WV)
0.69¬µmwere used.
Training We set the time step parameter œÑto 0.25, the at-
tachment weight parameter Œªto 0.15, and the tightness pa-Method Channels PSNR ‚ÜëSSIM ‚Üë
Linear IR 38.667 0.745
SSM-T IR 43.285 0.831
WR-Net (Warp Only) IR 44.213 0.904
WR-Net (Full) IR 46.527 0.934
Linear SW 38.381 0.719
SSM-T SW 45.935 0.827
WR-Net (Warp Only) SW 48.237 0.922
WR-Net (Full) SW 50.538 0.936
Linear WV 43.526 0.766
SSM-T WV 51.842 0.895
WR-Net (Warp Only) WV 56.191 0.929
WR-Net (full) WV 58.350 0.973
Table 1: Quantitative experimental results of video frame in-
terpolation of WR-Net on the GK2A geostationary satellite
weather observation dataset.
rameter Œ∏to 0.3 when using the optical flow with the TV-
L1 algorithm. As for augmentation, we used random crop
augmentation with a size of 975 √ó775 and random rotate
90‚ó¶augmentation. Each input channel of the multi-modal
fusion network is embedded with 512 channels, and the U-
Net based on VGG16 is used as the refine network.
All experiments were performed on NVIDIA A100
GPU√ó8, batch size was 64, adaptive moment estimation
(Adam) was used as the optimizer, and the learning rate was
set to 1e-4. Note that the hyperparameters of the TV-L1 al-
gorithm has a significantly effect on the performance, so it
must be adjusted according to the dataset.
Video frame interpolation results The scale of weather
tracking and modeling, ecosystem monitoring and climate
change tracking depend on the spatial and temporal resolu-
tions of observations.
However, when we use multiple satellites, the temporal
resolution needs to be set to the past geostationary (GEO)
satellite resolution at intervals of 10-15 min, although we
have a higher resolution dataset with 1-2 min intervals. To
solve this problem, we use WR-Net to interpolate the tem-
poral resolution of geostationary (GEO) satellites and eval-
uate their performance through peak signal-to-noise ratio
(PSNR) and structural similarity index map (SSIM).
We interpolated satellite images at 4min intervals in 2min
increments and compared their performance with SSM-T
(state-of-the-art method (Vandal and Nemani 2021)) and lin-
ear interpolation.
Table 1 shows the results of video frame interpolation ex-
periments. As shown in Tab 1, WR-Net showed the highest
performance in IR 10.5 ¬µm, SW 0.38 ¬µm, and WV 0.69 ¬µm
channels. In addition, the performance of video frame inter-
polation by extracting the optical flow through the TV-L1
algorithm and simply warping the optical flow was higher
than that of SSM-T. These experimental results indicate that
the video frame interpolation method of superslomo series,
which extracts and wraps the optical flow as an unsupervised
learning method, is inefficient in the weather observation
geostationary satellite.0 MIN
+90 MIN (Ours)
+90 MIN (GT)Figure 4: Qualitative result of future frame prediction of WR-Net. WR-Net receives 0min and +4min as input and predicts
+8min, and inputs between +4min and predicted +8min again into WR-Net to predict +12min. Repeat this process to predict
up to +90min.
Finally, the performance of using the refinement network
was higher than the optical flow warping of WR-Net alone.
These experimental results indicate that the refinement net-
work operated significantly.
Future frame prediction results Future frame prediction
is a very important field that can be used in various fields
such as nowcasting, tracking temperature change, disaster
forecasting, and climate change forecasting. According to
this importance, deepmind‚Äôs deep generative model of rain
(DGMR) (Ravuri et al. 2021) or FourCastNet has been pro-
posed. All of these methods modeled multi-temporal infor-
mation, but they were limited to either concatenating and
inputting multi-temporal frames to the deep learning model
or using LSTM.
Optical flow is a modality that includes movement direc-
tion and velocity between two input frames. Especially in the
short term, the direction of movement is rarely reversed. Be-
cause of these characteristics, movement direction and speed
are very important analysis factors in weather forecasting.
Therefore, we applied WR-Net to verify the usability of
optical flow in the field of future frame prediction. Figure 4
is the qualitative result of predicting the frame after 90 min-
utes through WR-net.
As shown in the green box in Fig. 4, the cloud expan-
sion direction and size were predicted similarly to the actual
ground truth. These quantitative experimental results indi-
cate that optical flow is very helpful in predicting cloud ex-
pansion and direction, and we confirm the possibility of pre-dicting clouds generations with the refinement network (the
green box in Fig. 4). However, in some cases, it has limi-
tations to generate newly developed clouds (the red box in
Fig. 4).
Limitation and Future Work
Our study performed video frame interpolation or future
frame prediction using only single channels with short
timestamps. However, tasks such as weather forecasting and
climate forecasting can be performed with high performance
by fusion of information from various channels as well as
a single channel. In our future work, we plan to study a
method using multi-channel information. Also, we will ver-
ify WR-Net with tropical cyclone cases to check the gener-
ation and dissipation of cloud in the future study.
Conclusion
We solved the problem that PWC-Net or FlowNet did not
work in weather observation satellites through TV-L1 algo-
rithm and proposed WR-Net to achieve state-of-the-art in the
field of video frame interpolation. Also, as far as we know, it
showed the possibility of using optical flow for the first time
in the field of future frame prediction. Although our work
has only been evaluated by weather observation satellites,
we believe that our method has applications in a variety of
fields, including climate change and disaster prediction. We
hope that our research will have widespread applications in
earth science as well as weather observations.References
Chung, S.-R.; Ahn, M.-H.; Han, K.-S.; Lee, K.-T.; and Shin,
D.-B. 2020. Meteorological products of geo-KOMPSAT 2A
(GK2A) satellite.
Dosovitskiy, A.; Fischer, P.; Ilg, E.; Hausser, P.; Hazirbas,
C.; Golkov, V .; Van Der Smagt, P.; Cremers, D.; and Brox,
T. 2015. Flownet: Learning optical flow with convolutional
networks. In Proceedings of the IEEE international confer-
ence on computer vision , 2758‚Äì2766.
Jiang, H.; Sun, D.; Jampani, V .; Yang, M.-H.; Learned-
Miller, E.; and Kautz, J. 2018. Super slomo: High quality
estimation of multiple intermediate frames for video inter-
polation. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition , 9000‚Äì9008.
Pathak, J.; Subramanian, S.; Harrington, P.; Raja, S.; Chat-
topadhyay, A.; Mardani, M.; Kurth, T.; Hall, D.; Li, Z.; Az-
izzadenesheli, K.; et al. 2022. Fourcastnet: A global data-
driven high-resolution weather model using adaptive fourier
neural operators. arXiv preprint arXiv:2202.11214 .
Ravuri, S.; Lenc, K.; Willson, M.; Kangin, D.; Lam, R.;
Mirowski, P.; Fitzsimons, M.; Athanassiadou, M.; Kashem,
S.; Madge, S.; et al. 2021. Skilful precipitation nowcasting
using deep generative models of radar. Nature , 597(7878):
672‚Äì677.
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-
volutional networks for biomedical image segmentation. In
International Conference on Medical image computing and
computer-assisted intervention , 234‚Äì241. Springer.
Sun, D.; Yang, X.; Liu, M.-Y .; and Kautz, J. 2018. Pwc-
net: Cnns for optical flow using pyramid, warping, and cost
volume. In Proceedings of the IEEE conference on computer
vision and pattern recognition , 8934‚Äì8943.
Vandal, T. J.; and Nemani, R. R. 2021. Temporal interpo-
lation of geostationary satellite imagery with optical flow.
IEEE Transactions on Neural Networks and Learning Sys-
tems.
Wedel, A.; Pock, T.; Zach, C.; Bischof, H.; and Cremers, D.
2009. An improved algorithm for tv-l 1 optical flow. In Sta-
tistical and geometrical approaches to visual motion analy-
sis, 23‚Äì45. Springer.