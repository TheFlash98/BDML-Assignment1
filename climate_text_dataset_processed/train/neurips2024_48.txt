Wildflower Monitoring with Expert-annotated Images
and Flowering Phenology
Georgiana Manolache
Fontys University of Applied Sciences,
The Netherlands
g.manolache@fontys.nlGerard Schouten
Fontys University of Applied Sciences,
The Netherlands
g.schouten@fontys.nl
Abstract
Understanding biodiversity trends is essential for preservation policy planning, and
advanced computer vision solutions now enable large-scale automated monitoring
for many biodiversity use cases. Wildflower monitoring, in particular, presents
unique challenges. Visual similarities in shape and color may exist between differ-
ent species, while flowers within a species may have significant visual differences.
Moreover, flowers follow a growth cycle and look distinctly different over the
year, while different species flower at different times of the year. Having access to
flowering phenology , more accurate predictions may be made. We propose a novel
multi-modal wildflower monitoring task to better identify species, levering both
expert-annotated wildflower images and flowering phenology estimates. Moreover,
we benchmark several state-of-the-art models using two groups of common wild-
flower species that have high inter-class similarity, and show that this multi-modal
approach significantly outperforms image-only baselines. With this work, we aim
to encourage the development of standards for automated wildflower monitoring as
a step towards bending the curve of biodiversity loss.
1 Introduction
Habitat loss, pollution and climate change are the primary drivers of biodiversity loss, causing
ecosystems to degrade which, in turn, impacts human well-being [3]. Flowering plants play a vital part
in supporting ecosystems, attracting pollinators which in turn enable plants to develop seeds to produce
more flowers [18]. Nearly half of the world’s known flowering plant species are potentially threatened
with extinction according to a new study [2]. The UK alone is reported to have lost 97% of its
wildflower meadows since the 1930s [7]. It is crucial to understand biodiversity trends for preservation
policy planning. However, due to the amount of effort and expertise required for conventional field
monitoring, large knowledge gaps remain. In the last few years, deep learning—with a focus on
computer vision and object detection —has been proposed to automate in-situ wildflower monitoring
by identifying and counting species in images [10, 15, 22]. As it turns out, however, wildflower
monitoring combines a unique set of challenges for computer vision as illustrated in figure 1. The
latter two challenges are particularly hard for current models: some images of the same species
may have significant visual differences, while at the same time visual similarities in shape and
color may exist between different species. This visual confusion makes it difficult even for humans
to distinguish the species without deeper expertise, and subsequently limits the construction of
automated wildflower monitoring models.
We therefore propose a multi-modal approach that includes information on flowering phenology
to help overcome the flower inter-class similarity and intra-class variation challenges. Flowering
phenology refers to the study of the timing of seasonal events in flowering plants over their growth
cycle. Our approach leverages the fact that the visual characteristics as well as the presence of flowers
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.Figure 1: Challenges of flower identification. 1) viewpoint variations ( Papaver rhoeas ); 2) occlusion
(Ranunculus repens ); 3) clutter ( Achillea millefolium ); 4) light variation ( Leucanthemum vulgare );
5) deformations ( Bellis perennis ); 6) intra-class variation ( Ficaria verna ); 7) inter-class similarity
(Bellis perennis ,Leucanthemum vulgare ,Matricaria chamomilla ). Source: EWD [22]
at a given point in time highly depends on their estimated flowering time. We present a novel multi-
modal wildflower monitoring task (section 2) combining high-quality annotated wildflower images
from the Netherlands [22] with flowering phenology measurements from a public database [16]
and benchmark and discuss several multi-modal models (section 3) to encourage machine learning
research that may help us monitor, understand, and ultimately preserve biodiversity.
2 Task definition
We aim to jointly use expert-annotated images and flowering phenology estimates to identify wild-
flower species; thus, our task can be considered a supervised multi-modal object detection problem.
Remote sensing [1, 15, 24], drone acquired [8] or in-situ collected [10, 17, 23] wildflower image
datasets have been successfully used in object detection solutions, however, their annotation quality
varies wildly. For real-world images with many visually similar species, precisely labeled and
annotated high-quality images are essential [5, 6]. We use the Eindhoven Wildflower Dataset (EWD),
which offers a significantly larger collection of wildflower images comprising 160 species from
various habitats in the Netherlands with guaranteed in-situ high-quality expert-annotated images
based on well-established guidelines [22].
To the best of our knowledge, this is the first multi-modal object detection task allowing fine-grained
classification of a wide range of visually similar wildflower species using flowering phenology
estimates. Similar multi-modal approaches consider such auxiliary information into the classification,
for instance spatio-temporal information (latitude, latitude, altitude, date) as a source of non-visual a
priori information about flower species, as well as additional visual information from satellite images
based on spatio-temporal information [4, 14]. However, their temporal information is limited by the
presence-only data, which inherently lacks information about the species growth cycle. Furthermore,
disclosure of exact locations of flowers is less desirable, especially if the species are rare or red listed.
We use established (pre-trained) object detection architectures for image feature extraction and local-
ization and adjust the classification network. Our goal during training is to reduce misclassifications
of visually similar species. Figure 2 provides an overview of the learning task we aim to solve. Given
a set of images, each with a capture date d, and a set of possible species s1, ..., s nthat we aim to
detect, we extract the flowering phenology estimates of each species pd
s1, ..., pd
snand jointly predict
the vector y= (ys1, ..., y sn)of probabilities for each possible species.
Figure 2: An overview of the interaction between our data and the model.
2Figure 3: Selected flower species grouped by visual intra-species similarity: 1) Group 1: Buttercup *
(aggregate) ,Caltha palustris ,Ficaria verna ; 2) Group 2: Bellis perennis ,Chamomille * (aggregate) ,
Leucanthemum vulgare ; 3) Phenology data for Group 1 (top) and Group 2 (bottom).
3 Benchmark
Dataset We select two groups of common wildflowers from EWD [22], each group containing three
species with high inter-class similarity, shown in figure 3. The images were collected from flower
beds of approximately 1m2taken (near-)vertically downward at a height ranging from 1.5 to 1.9m,
in the region of Eindhoven, the Netherlands, over the years 2021 and 2022. Since the EWD images
are significantly large (6720 ×4480 pixels) containing many flowers and other plants, we slice the
original images into 15 tiles, and tiles without object annotations are disregarded. We extract the
flowering phenology from the Dutch National Database Flora en Fauna (NDFF) [16] which modeled
their estimates from observations from the period 2000-2021. We randomly select images amounting
to 550 objects for the training sets, 100 objects for validation set, and 50 objects in test set for each
species.
Baselines We use the Faster-RCNN [21] object detection model, and learn image features with
ResNet50 [9] with pre-trained weights, which performed best in wildflower detection studies [19,
22]. For the fusion variants, we concatenate feature vectors with the flowering phenology vector
before feeding it to the classifier. The flowering phenology is passed into a 1-dimensional vector
of length n. The values of the flowering phenology feature vector are extracted from the flowering
phenology graphs at a given image creation date and passed as input alongside the image. We also
propose to simultaneously learn features from flowering phenology. To understand how different
fusion operations impacted predictive performance, we experimented with three fusion operations: 1)
concatenation , 2)element-wise multiplication and 3) element-wise addition [11]. We extract 1024
image features for each detected object, and combine them with phenology feature vectors using
fusion variants described above. We train separate models for each group for 30 epochs and use 5
seeds for training-validation-test sets.
Results Table 1 offers a holistic view of each model performance. We observe that fusion models
outperform image-only models for both groups. This highlights the value of flowering phenology for
our task. Precision increases significantly with additional features for the learned feature fusion with
element-wise concatenation and addition variants (non-parametric paired sample t-test p<0.01). The
learned feature fusion method likely captures more nuanced relationships between learned features
Table 1: COCO [13] mAP over all classes at 0.50 and [0.50,0.95] interval with 0.05 step IoU for all
models averaged over 5 seeds.
ModelmAP
@.50IoU @[.50:.95]IoU
Image only 0.77 ±0.07 0.59 ±0.06
Feature fusion 0.88 ±0.04 0.67 ±0.04
Group 1 Learned feature fusion concatenation 0.89 ±0.04 0.68 ±0.04
Learned feature fusion multiplication 0.89 ±0.04 0.68 ±0.04
Learned feature fusion addition 0.90±0.03 0.68 ±0.04
Image only 0.67 ±0.11 0.48 ±0.07
Feature fusion 0.69 ±0.13 0.51 ±0.10
Group 2 Learned feature fusion concatenation 0.80 ±0.07 0.58 ±0.05
Learned feature fusion multiplication 0.74 ±0.14 0.54 ±0.08
Learned feature fusion addition 0.81±0.08 0.59 ±0.05
3from both modalities and optimizes the combination process, leading to improved performance
compared to a simple concatenation approach. The variant using element-wise addition fusion had
the best predictive performance which could imply that the learned image and flowering phenology
features were relevant and complementary.
To better evaluate misclassification, table 2 shows the inferred performance scores for each species
in a confusion matrix. Interestingly, there are less missed detections in the fusion model at a 0.75
confidence. This may indicate that more robust and generalizable features are learned with the
addition of flowering phenology. There are also significantly less misclassifications in Group 1.
There are slightly more missclassifications between Bellis perennis andLeucanthemum vulgare in the
multi-modal variant. This is primarily caused by the training-validation-test splits which did not take
into account image creation date. We suggest balancing training, validation, and test set splits also
on dates. Data augmenting techniques such as random flipping, noise, blur, contrast and brightness
can be leveraged to improve the robustness of the models [20]. Furthermore, taking into account
climate change could perhaps improve species encounter and better represent their growth cycle.
Another limitation of our study is the smaller training setup. However, training with more image
data may not always improve model performance [25]. Nevertheless, the results of the multi-modal
models are still remarkable. We also experimented with the object detection capabilities of the largest
multi-modal large language model (MLLMs), GPT-4 for vision (GPT-4v), but it did not perform
well on visually similar wildflower species. Thus, GPT-4v in its current state is not near object
detection state-of-the-art, and improving a MLLM is less desirable as training and inferring is energy
expensive [12].
4 Conclusion
We propose a multi-modal object detection task and benchmark for wildflower monitoring using
their flowering phenology estimates. Extensive experiments corroborated the effectiveness of our
multi-modal approach using phenology in reducing misclassification. As this work is intended to
directly impact wildflower monitoring, we hope such input will be valuable to researchers seeking
to understand biodiversity and climate change, as well as policymakers interested in evaluating
preservation priorities across different areas of land.
References
[1] Johanna Ärje et al. “Automatic flower detection and classification system using a light-weight
convolutional neural network”. In: EUSIPCO Workshop on Signal Processing, Computer
Vision and Deep Learning for Autonomous Systems . 2019.
[2] Steven P Bachman et al. “Extinction risk predictions for the world’s flowering plants to support
their conservation”. In: New Phytologist 242.2 (2024), pp. 797–808.
[3] Eduardo Sonnewend Brondízio et al. “Global assessment report on biodiversity and ecosystem
services of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem
Services”. In: (2019).
[4] Riccardo De Lutio et al. “Digital taxonomist: Identifying plant species in community scientists’
photographs”. In: ISPRS journal of photogrammetry and remote sensing 182 (2021), pp. 112–
121.
Table 2: Average confusion matrix with confidence over 0.75 and IoU over 0.50 for image-only (top)
and learned feature level fusion element-wise addition (bottom) models averaged over 5 seeds.
missedButtercup Caltha Ficaria
(aggregate) palustris verna
Buttercup0.38 0.42 0.14 0.06(aggregate)
Caltha0.28 0.076 0.64 0.012palustris
Ficaria0.11 0.004 0.12 0.76verna
Buttercup0.18 0.78 0.000 0.000(aggregate)
Caltha0.11 0.028 0.85 0.008palustris
Ficaria0.076 0.000 0.02 0.90vernamissedBellis Chamomile Leucanthemum
perennis (aggregate) vulgare
Bellis0.64 0.34 0.024 0.000perennis
Chamomile0.89 0.04 0.028 0.044(aggregate)
Leucanthemum0.57 0.068 0.008 0.35vulgare
Bellis0.33 0.65 0.004 0.016perennis
Chamomile0.42 0.004 0.56 0.016(aggregate)
Leucanthemum0.48 0.08 0.000 0.44vulgare
4[5] Chris S Elphick. How you count counts: the importance of methods research in applied ecology .
2008.
[6] Elizabeth J Farnsworth et al. “Next-generation field guides”. In: BioScience 63.11 (2013),
pp. 891–899.
[7] Robin M Fuller. “The changing extent and conservation interest of lowland grasslands in
England and Wales: a review of grassland surveys 1930–1984”. In: Biological conservation
40.4 (1987), pp. 281–300.
[8] Johannes Gallmann et al. “Flower mapping in grasslands with drones and deep learning”. In:
Frontiers in plant science 12 (2022), p. 774965.
[9] Kaiming He et al. “Deep residual learning for image recognition”. In: Proceedings of the IEEE
conference on computer vision and pattern recognition . 2016, pp. 770–778.
[10] Damien Hicks et al. “Deep learning object detection to estimate the nectar sugar mass of
flowering vegetation”. In: Ecological Solutions and Evidence 2.3 (2021), e12099.
[11] Gregory Holste et al. “End-to-end learning of fused image and non-image features for im-
proved breast cancer classification from mri”. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision . 2021, pp. 3294–3303.
[12] IEA. Electricity 2024 . Licence: CC BY 4.0. Paris, 2024. URL:https://www.iea.org/
reports/electricity-2024 .
[13] Tsung-Yi Lin et al. Microsoft COCO: Common Objects in Context . 2015. arXiv: 1405.0312 .
[14] Oisin Mac Aodha, Elijah Cole, and Pietro Perona. “Presence-only geographical priors for
fine-grained image classification”. In: Proceedings of the IEEE/CVF International Conference
on Computer Vision . 2019, pp. 9596–9606.
[15] Hjalte MR Mann et al. “Automatic flower detection and phenology monitoring using time-
lapse cameras and deep learning”. In: Remote Sensing in Ecology and Conservation 8.6 (2022),
pp. 765–777.
[16] NDFF. NDFF Distribution Atlas . 2024. URL:https://www.verspreidingsatlas.nl .
[17] Maria-Elena Nilsback and Andrew Zisserman. “Automated flower classification over a large
number of classes”. In: 2008 Sixth Indian conference on computer vision, graphics & image
processing . IEEE. 2008, pp. 722–729.
[18] Jeff Ollerton, Rachael Winfree, and Sam Tarrant. “How many flowering plants are pollinated
by animals?” In: Oikos 120.3 (2011), pp. 321–326.
[19] Isha Patel and Sanskruti Patel. “An optimized deep learning model for flower classification
using NAS-FPN and Faster R-CNN”. In: International Journal of Scientific & Technology
Research 9.03 (2020), pp. 5308–5318.
[20] Sylvestre-Alvise Rebuffi et al. “Data augmentation can improve robustness”. In: Advances in
Neural Information Processing Systems 34 (2021), pp. 29935–29948.
[21] Shaoqing Ren et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
Networks . 2016. arXiv: 1506.01497 [cs.CV] .
[22] Gerard Schouten, Bas SHT Michielsen, and Barbara Gravendeel. “Data-centric AI approach
for automated wildflower monitoring”. In: (2024). DOI:10.1101/2024.04.18.590040 .
[23] Marco Seeland et al. “Plant species classification using flower images—A comparative study
of local feature representations”. In: PloS one 12.2 (2017), e0170629.
[24] Dat Thanh Tran et al. “Automatic flower and visitor detection system”. In: 2018 26th European
Signal Processing Conference (Eusipco) . IEEE. 2018, pp. 405–409.
[25] Xiangxin Zhu et al. “Do We Need More Training Data or Better Models for Object Detection?.”
In: Citeseer.
5