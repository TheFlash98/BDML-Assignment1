Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
SUB-SEASONAL TO SEASONAL FORECASTS THROUGH
SELF -SUPERVISED LEARNING
Jannik Thuemmel
University of Tübingen
Machine Learning in Climate Science
jannik.thuemmel@uni-tuebingen.deFelix Strnad
University of Tübingen
Machine Learning in Climate Science
Jakob Schlör
University of Tübingen
Machine Learning in Climate ScienceMartin V . Butz
University of Tübingen
Neuro-Cognitive Modelling
Bedartha Goswami
University of Tübingen
Machine Learning in Climate Science
ABSTRACT
Sub-seasonal to seasonal (S2S) weather forecasts are an important decision-making
tool that informs economical and logistical planning in agriculture, energy man-
agement, and disaster mitigation. They are issued on time scales of weeks to
months and differ from short-term weather forecasts in two important ways: (i) the
dynamics of the atmosphere on these timescales can be described only statistically
and (ii) these dynamics are characterized by large-scale phenomena in both space
and time. While deep learning (DL) has shown promising results in short-term
weather forecasting, DL-based S2S forecasts are challenged by comparatively
small volumes of available training data and large fluctuations in predictability due
to atmospheric conditions. In order to develop more reliable S2S predictions that
leverage current advances in DL, we propose to utilize the masked auto-encoder
(MAE) framework to learn generic representations of large-scale atmospheric
phenomena from high resolution global data. Besides exploring the suitability
of the learned representations for S2S forecasting, we will also examine whether
they account for climatic phenomena (e.g., the Madden-Julian Oscillation) that are
known to increase predictability on S2S timescales.
1 I NTRODUCTION
Goal We aim to implement a self-supervised deep learning pipeline ( 50) that facilitates the encoding
and anticipation of large-scale atmospheric patterns including, but not limited to, the Madden Julian
Oscillation ( 51) and the Silk Road pattern ( 6), which are well-suited for the generation of explainable
S2S forecasts (34).
Potential impacts Extreme weather events are likely to increase in magnitude and frequency
under ongoing global climate change ( 37), in part due to seasonal weather dynamics, which are also
anticipated to intensify in the future ( 19). Reliable S2S forecasts ( 48;43) can limit the impact of future
extreme events by informing decision-makers and stakeholders well in advance, allowing time for
appropriate mitigating measures to be adopted. The development of improved S2S forecasting systems
is, in fact, a vital part of climate change impact mitigation ( 1;2), and our approach, if successful, can
potentially help stakeholders in agriculture, energy management, and disaster mitigation (28).
Climate background How one forecasts atmospheric dynamics depends mainly on the timescale
of interest. Forecasts up to approximately two weeks can be achieved by deterministic forward
simulation from initial conditions ( 26) and have been refined to impressive levels of accuracy ( 4).
1Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
More recently, DL-based methods have produced forecasts on these timescales that rival or even
surpass the quality of physics-based numerical methods ( 29;5). Longer timescales ranging from
several weeks to months and beyond, however, can not be addressed in the same fashion because
the chaotic behaviour of the atmosphere imposes an inherent limit on deterministic forecasts ( 31).
Instead, the long-term behavior of the atmosphere needs to be described in statistical terms ( 20),
as is done in probabilistic or ensemble-based forecasts. The complex interplay of atmospheric and
oceanic processes ( 18), however, leads to large variations in possible forecast horizons and associated
uncertainties. Identifying conditions which allow skillful forecasts, known as windows of opportunity
(34), is a key challenge for S2S forecasting.
Typical windows of opportunity often stem from large-scale modes of climatic variability, such as the
Madden-Julian Oscillation (MJO ( 33;32)), the Northern Atlantic Oscillation (NAO ( 24)), and the
El Niño Southern Oscillation (ENSO), which have intra- to inter-annual periodicities that an S2S
forecast system would seek to utilize. Here, we consider the MJO, an oscillating convective system
that moves eastward along the equator and has a period of approximately 40—60 days ( 51). The
intensity and location of the MJO are closely related to the occurrence of extreme rainfall events over
India, the Maritime Continent, and South Asia ( 40). Moreover, these large-scale patterns are known
to offer a window of opportunity for S2S forecasts over Europe, Australia, and North America (34).
State-of-the-art and challenges Previous work on S2S forecasts utilizing DL has focused on global
temperature and precipitation ( 44), the occurrence of heatwaves ( 30;25;11), or climate indices like
the MJO index ( 13) and the Nino 3.4 index ( 9). Major obstacles when training DL models for
S2S tasks are the comparatively small number of available observations of S2S phenomena and the
intermittency of their predictability. The latter is not accounted for in standard DL tasks, which
means that, for a fixed lead time, all model errors will be treated the same, regardless whether skillful
forecasts were feasible or not, which may harm convergence ( 35). Mainly due to these issues, current
S2S-DL models are small (for DL standards) and often restricted to coarse resolution data.
Key idea Recent progress in Natural Language Processing ( 8;14) has been primarily due to the
development of foundation models, i.e. large pre-trained models that are later fine-tuned to various
downstream tasks with little additional data ( 7). We believe that foundation models can drastically
improve DL-based weather forecasting ( 36), especially on S2S timescales, but the approach crucially
depends on the careful design of learning tasks that lead to useful representations. Previous work
on atmospheric self-supervised learning has utilized lead-time dependent tasks, either predicting
at a given lead time ( 36) or inferring the temporal distance between two atmospheric states ( 23).
Following our intuition that global teleconnection patterns ( 6) and large-scale modes of variability
are critical for S2S forecasts ( 34), we propose to utilize a pre-training task that incentivizes global
structure, rather than specific dynamics: masked auto-encoding (14; 21).
2 M ETHODOLOGY
Masked Auto-Encoding Masked auto-encoders (MAE) are trained to reconstruct data from a
corrupted input, a form of denoising ( 42), and have led to promising results in spatio-temporal
domains, such as videos ( 41;17) or time-series ( 49). As a simple form of corruption, masking
replaces parts of the input e.g. patches of pixels or certain temporal windows with a learnable ‘mask-
token’ ( 21). The model is then tasked with reconstructing regions of the input that were masked. To
achieve this at high fidelity, the model is encouraged to exploit structural information in the input.
For example, the peaks and troughs of an atmospheric wave could be accurately inferred even if a
large part of the wave is masked out. Interestingly, best results on videos ( 17;41) have been achieved
with extremely high (up to 95%) masking ratios, which drastically reduce the computational costs
associated with training on high-resolution atmospheric data.
Data Suitable atmospheric data is available from the ERA5 reanalysis dataset of observations from
1959—2022 (22) and the CMIP6 climate model runs (16; 39). For downstream validation purposes,
the S2S-database ( 43) offers access to a wide range of past S2S forecasts against which our model
can be compared. All input, for training and evaluation, will be based on anomalies - deviations from
the mean (climatology) computed with respect to the day of year at each spatial location - rather than
2Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
absolute values. Anomaly time-series are commonly used in the atmospheric sciences e.g. when
computing climate indices (47) or modelling extreme events (6; 40).
Network architecture MAE as a framework is architecture agnostic, but models that utilize a
graph-like connectivity structure, such as Vision Transformers (ViT) ( 15) or Graph Networks (GNN)
(3) are ideal since they allow for easy application of masking schemes ( 21). The current state-of-the-
art models for deep learning-based weather prediction ( 5;29;38) are based on these architectures as
well. As a starting point, we intend to follow the work of ( 29) and ( 27) and implement our model as a
Mesh-GNN with multiple spatial scales.
Evaluation of learned representations Representation learning approaches are commonly evalu-
ated using linear probing (see e.g. ( 21;12), in which the encoder model is held fixed while a simple
(linear) model is trained to map from the encoded features to an evaluation target. The evaluation
target should aggregate semantically important features and, in the context of representing large-scale
atmospheric phenomena, be independent of small-scale variations that are irrelevant for S2S forecasts.
Thus, we propose to evaluate the representations on climate indices which are used to describe major
modes of variability in the atmosphere. The spatial extension of the MJO, for example, is described
by a bivariate index ( 47) that is derived from the first two principal components of zonal winds at an
atmospheric height of 200 hPa and 850 hPa as well as from the outgoing long-wave radiation, which
is a proxy for convection and rainfall ( 46). We believe that linear probing on climate indices will be
an important first step to alleviate concerns with the interpretability of DL-based forecasts, which
remain a substantial open challenge (10), and strengthen the explainability of generated forecasts
Downstream application: S2S forecasts Foundation models are meant to bootstrap other task-
specific models, enabling them to effectively learn solutions to challenging problems. For our
method, we will use the S2S-AI-challenge ( 44), which focuses on 3—6 week forecasts of global 2m
temperature and precipitation. Performance will be evaluated by the Ranked Probability Skill Score
(RPSS), with climatology and persistence as benchmarks for model performance. In addition to the
metrics from the S2S AI challenge, we will evaluate the developed model’s MJO predictions using
the Continuous Ranked Probability Score (CRPS) and Root Mean Square Error (RMSE) and compare
the results to available forecasting models from the S2S database (45) and previous DL work (13).
3 C ONCLUSION
We propose a self-supervised deep learning pipeline that can facilitate reliable sub-seasonal-to-
seasonal forecasts by learning useful representations of the global atmosphere.
Inspired by the recent success of foundation models in natural language processing and computer
vision this proposal outlines how we aim to use Masked Auto-encoders (implemented using a Mesh-
GNN) to first train our model on reanalysis data (from ERA5) and climate model output (from
CMIP6) before validating it on data from the S2S forecast database. The learned representations
are likely to be linked to well known slowly varying modes of the weather system, such as, e.g., the
MJO and the ENSO, which typically provide so-called windows of opportunity for S2S forecasts. To
evaluate the quality of the learned representations, we propose to use linear probing on climate indices
which describe large-scale patterns of atmospheric variability. In particular, we plan to evaluate the
performance of the final model in predicting the MJO, as the MJO is known to be linked to extreme
rainfall in Southeast Asia.
S2S forecasting is a challenging task for numerical weather models and deep learning models alike.
Particularly in the face of ongoing climate change and increased risk of exposure to weather extremes,
a successful S2S forecast model not limited by deterministic forecast horizons can be of great benefit
to civil society.
From a methodological standpoint, our work will contribute to the development of foundation models
in weather forecasting and climate projections. Additionally, the proposed validation scheme for
learned representations of the weather system will potentially help increase the explainability, and
consequently the trustworthiness, of deep learning based weather forecasts.
3Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
REFERENCES
[1]Next Generation Earth System Prediction: Strategies for Subseasonal to Seasonal Forecasts .
National Academies Press, Washington, D.C., 2016.
[2]E. K.-M. Chang A. Lang P. A. Dirmeyer K. Pegion D. Barrie C. A. Mariotti Baggett, E.
A. Barnes. Bridging the Weather-to-Climate Prediction Gap, February 2019.
[3]Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zam-
baldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,
Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani,
Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra,
Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational
inductive biases, deep learning, and graph networks. arXiv:1806.01261 [cs, stat] , October 2018.
arXiv: 1806.01261.
[4]Peter Bauer, Alan Thorpe, and Gilbert Brunet. The quiet revolution of numerical weather
prediction. Nature , 525(7567):47–55, September 2015. Bandiera_abtest: a Cg_type: Nature
Research Journals Number: 7567 Primary_atype: Reviews Publisher: Nature Publishing
Group Subject_term: Atmospheric dynamics;Climate sciences Subject_term_id: atmospheric-
dynamics;climate-sciences.
[5]Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Pangu-Weather:
A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast, November 2022.
arXiv:2211.02556 [physics].
[6]Niklas Boers, Bedartha Goswami, Aljoscha Rheinwalt, Bodo Bookhagen, Brian Hoskins, and
Jürgen Kurths. Complex networks reveal global pattern of extreme-rainfall teleconnections.
Nature , 566(7744):373–377, February 2019. Number: 7744 Publisher: Nature Publishing
Group.
[7]Rishi Bommasani, Drew A Hudson, Ehsan Adeli Russ Altman, and Simran Arora. On the
Opportunities and Risks of Foundation Models.
[8]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners, July
2020. arXiv:2005.14165 [cs].
[9]Salva Rühling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest Pokropek, Willa
Potosnak, Suyash Bire, Salomey Osei, and Björn Lütjens. The World as a Graph: Improving El
Ni\~no Forecasts with Graph Neural Networks. arXiv:2104.05089 [physics, stat] , May 2021.
arXiv: 2104.05089.
[10] Matthew Chantry, Hannah Christensen, Peter Dueben, and Tim Palmer. Opportunities and
challenges for machine learning in weather and climate modelling: hard, medium and soft AI.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering
Sciences , 379(2194):20200083, April 2021. Publisher: Royal Society.
[11] Ashesh Chattopadhyay, Ebrahim Nabizadeh, and Pedram Hassanzadeh. Analog
Forecasting of Extreme-Causing Weather Patterns Using Deep Learning. Journal
of Advances in Modeling Earth Systems , 12(2):e2019MS001958, 2020. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001958.
[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework
for Contrastive Learning of Visual Representations, June 2020. arXiv:2002.05709 [cs, stat].
[13] Antoine Delaunay and Hannah M. Christensen. Interpretable Deep Learning for Probabilistic
MJO Prediction. Geophysical Research Letters , 49(16):e2022GL098566, 2022. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022GL098566.
4Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding, May 2019. arXiv:1810.04805
[cs].
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale, June 2021. arXiv:2010.11929 [cs].
[16] Veronika Eyring, Sandrine Bony, Gerald A. Meehl, Catherine A. Senior, Bjorn Stevens, Ronald J.
Stouffer, and Karl E. Taylor. Overview of the Coupled Model Intercomparison Project Phase 6
(CMIP6) experimental design and organization. Geoscientific Model Development , 9(5):1937–
1958, May 2016. Publisher: Copernicus GmbH.
[17] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked Autoencoders As
Spatiotemporal Learners, October 2022. arXiv:2205.09113 [cs].
[18] Laura Ferranti, Susanna Corti, and Martin Janousek. Flow-dependent verifica-
tion of the ECMWF ensemble over the Euro-Atlantic sector. Quarterly Jour-
nal of the Royal Meteorological Society , 141(688):916–924, 2015. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.2411.
[19] Hayley J. Fowler, Geert Lenderink, Andreas F. Prein, Seth Westra, Richard P. Allan, Nikolina
Ban, Renaud Barbero, Peter Berg, Stephen Blenkinsop, Hong X. Do, Selma Guerreiro, Jan O.
Haerter, Elizabeth J. Kendon, Elizabeth Lewis, Christoph Schaer, Ashish Sharma, Gabriele
Villarini, Conrad Wasko, and Xuebin Zhang. Anthropogenic intensification of short-duration
rainfall extremes. Nature Reviews Earth & Environment , 2(2):107–122, February 2021. Number:
2 Publisher: Nature Publishing Group.
[20] K. Hasselmann. Stochastic climate models Part I. Theory. Tellus , 28(6):473–485, 1976. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2153-3490.1976.tb00696.x.
[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
Autoencoders Are Scalable Vision Learners, December 2021. arXiv:2111.06377 [cs].
[22] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-
Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, Adrian Simmons,
Cornel Soci, Saleh Abdalla, Xavier Abellan, Gianpaolo Balsamo, Peter Bechtold, Gionata
Biavati, Jean Bidlot, Massimo Bonavita, Giovanna De Chiara, Per Dahlgren, Dick Dee, Michail
Diamantakis, Rossana Dragani, Johannes Flemming, Richard Forbes, Manuel Fuentes, Alan
Geer, Leo Haimberger, Sean Healy, Robin J. Hogan, Elías Hólm, Marta Janisková, Sarah
Keeley, Patrick Laloyaux, Philippe Lopez, Cristina Lupu, Gabor Radnoti, Patricia de Rosnay,
Iryna Rozum, Freja Vamborg, Sebastien Villaume, and Jean-Noël Thépaut. The ERA5 global
reanalysis. Quarterly Journal of the Royal Meteorological Society , 146(730):1999–2049, 2020.
_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3803.
[23] Sebastian Hoffmann and Christian Lessig. AtmoDist: Self-supervised Representation Learning
for Atmospheric Dynamics, August 2022. arXiv:2202.01897 [physics].
[24] James W. Hurrell and Clara Deser. North Atlantic climate variability: The role of the North
Atlantic Oscillation. Journal of Marine Systems , 78(1):28–41, August 2009.
[25] Valérian Jacques-Dumas, Francesco Ragone, Pierre Borgnat, Patrice Abry, and Freddy Bouchet.
Deep Learning-Based Extreme Heatwave Forecast. Frontiers in Climate , 4, 2022.
[26] Eugenia Kalnay. Atmospheric Modeling, Data Assimilation and Predictability, November 2002.
ISBN: 9780511802270 Publisher: Cambridge University Press.
[27] Ryan Keisler. Forecasting Global Weather with Graph Neural Networks, February 2022.
Number: arXiv:2202.07575 arXiv:2202.07575 [physics].
[28] Maximilian Kotz, Anders Levermann, and Leonie Wenz. The effect of rainfall changes on
economic production. Nature , 601(7892):223–227, January 2022. Number: 7892 Publisher:
Nature Publishing Group.
5Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
[29] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato,
Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, Weihua Hu,
Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, Oriol Vinyals, Shakir
Mohamed, and Peter Battaglia. GraphCast: Learning skillful medium-range global weather
forecasting, December 2022. arXiv:2212.12794 [physics].
[30] Ignacio Lopez-Gomez, Amy McGovern, Shreya Agrawal, and Jason Hickey. Global Extreme
Heat Forecasting Using Neural Weather Models, November 2022. arXiv:2205.10972 [physics].
[31] Edward N. Lorenz. The predictability of a flow which possesses many scales of motion.
Tellus , 21(3):289–307, 1969. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2153-
3490.1969.tb00444.x.
[32] Roland A. Madden and Paul R. Julian. Detection of a 40–50 Day Oscillation in the Zonal
Wind in the Tropical Pacific. Journal of the Atmospheric Sciences , 28(5):702–708, July 1971.
Publisher: American Meteorological Society Section: Journal of the Atmospheric Sciences.
[33] Roland A. Madden and Paul R. Julian. Observations of the 40–50-Day Tropical Oscilla-
tion—A Review. Monthly Weather Review , 122(5):814–837, May 1994. Publisher: American
Meteorological Society Section: Monthly Weather Review.
[34] Annarita Mariotti, Cory Baggett, Elizabeth A. Barnes, Emily Becker, Amy Butler, Dan C.
Collins, Paul A. Dirmeyer, Laura Ferranti, Nathaniel C. Johnson, Jeanine Jones, Ben P. Kirtman,
Andrea L. Lang, Andrea Molod, Matthew Newman, Andrew W. Robertson, Siegfried Schubert,
Duane E. Waliser, and John Albers. Windows of Opportunity for Skillful Forecasts Subseasonal
to Seasonal and Beyond. Bulletin of the American Meteorological Society , 101(5):E608–E625,
May 2020. Publisher: American Meteorological Society Section: Bulletin of the American
Meteorological Society.
[35] Luke Metz, C. Daniel Freeman, Samuel S. Schoenholz, and Tal Kachman. Gradients are Not
All You Need, January 2022. Number: arXiv:2111.05803 arXiv:2111.05803 [cs, stat].
[36] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, and Aditya Grover.
ClimaX: A foundation model for weather and climate, January 2023. arXiv:2301.10343 [cs].
[37] Tim Palmer and Bjorn Stevens. The scientific challenge of understanding and estimating climate
change. Proceedings of the National Academy of Sciences , 116(49):24390–24395, December
2019. Publisher: Proceedings of the National Academy of Sciences.
[38] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram
Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar. FourCastNet: A Global Data-
driven High-resolution Weather Model using Adaptive Fourier Neural Operators, February 2022.
Number: arXiv:2202.11214 arXiv:2202.11214 [physics].
[39] Martina Stockhause and Michael Lautenschlager. CMIP6 Data Citation of Evolving Data. Data
Science Journal , 16(0):30, June 2017. Number: 0 Publisher: Ubiquity Press.
[40] Felix M. Strnad, Jakob Schloer, Ruth Geen, Niklas Boers, and Bedartha Goswami. Extreme
rainfall propagation within Boreal Summer Intraseasonal Oscillation modulated by Pacific sea
surface temperature, February 2023. arXiv:2302.00425 [physics].
[41] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked Autoencoders are
Data-Efficient Learners for Self-Supervised Video Pre-Training, July 2022. arXiv:2203.12602
[cs].
[42] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning - ICML ’08 , pages 1096–1103, Helsinki, Finland, 2008. ACM
Press.
6Published as a workshop paper at "Tackling Climate Change with Machine Learning", ICLR 2023
[43] F. Vitart, C. Ardilouze, A. Bonet, A. Brookshaw, M. Chen, C. Codorean, M. Déqué, L. Ferranti,
E. Fucile, M. Fuentes, H. Hendon, J. Hodgson, H.-S. Kang, A. Kumar, H. Lin, G. Liu, X. Liu,
P. Malguzzi, I. Mallas, M. Manoussakis, D. Mastrangelo, C. MacLachlan, P. McLean, A. Mi-
nami, R. Mladek, T. Nakazawa, S. Najm, Y . Nie, M. Rixen, A. W. Robertson, P. Ruti, C. Sun,
Y . Takaya, M. Tolstykh, F. Venuti, D. Waliser, S. Woolnough, T. Wu, D.-J. Won, H. Xiao,
R. Zaripov, and L. Zhang. The Subseasonal to Seasonal (S2S) Prediction Project Database.
Bulletin of the American Meteorological Society , 98(1):163–173, January 2017. Publisher:
American Meteorological Society Section: Bulletin of the American Meteorological Society.
[44] F. Vitart, A. W. Robertson, A. Spring, F. Pinault, R. Roškar, W. Cao, S. Bech, A. Bienkowski,
N. Caltabiano, E. De Coning, B. Denis, A. Dirkson, J. Dramsch, P. Dueben, J. Gierschendorf,
H. S. Kim, K. Nowak, D. Landry, L. Lledó, L. Palma, S. Rasp, and S. Zhou. Outcomes of
the WMO Prize Challenge to Improve Subseasonal to Seasonal Predictions Using Artificial
Intelligence. Bulletin of the American Meteorological Society , 103(12):E2878–E2886, De-
cember 2022. Publisher: American Meteorological Society Section: Bulletin of the American
Meteorological Society.
[45] Frédéric Vitart. Madden—Julian Oscillation prediction and teleconnections in the S2S database.
Quarterly Journal of the Royal Meteorological Society , 143(706):2210–2220, 2017. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3079.
[46] Duane E. Waliser, Nicholas E. Graham, and Catherine Gautier. Comparison of the Highly
Reflective Cloud and Outgoing Longwave Radiation Datasets for Use in Estimating Tropical
Deep Convection. Journal of Climate , 6(2):331–353, February 1993. Publisher: American
Meteorological Society Section: Journal of Climate.
[47] Matthew C. Wheeler and Harry H. Hendon. An All-Season Real-Time Multivariate MJO Index:
Development of an Index for Monitoring and Prediction. Monthly Weather Review , 132(8):1917–
1932, August 2004. Publisher: American Meteorological Society Section: Monthly Weather
Review.
[48] Christopher J. White, Henrik Carlsen, Andrew W. Robertson, Richard J.T. Klein, Jeffrey K.
Lazo, Arun Kumar, Frederic Vitart, Erin Coughlan de Perez, Andrea J. Ray, Virginia Murray,
Sukaina Bharwani, Dave MacLeod, Rachel James, Lora Fleming, Andrew P. Morse, Bernd
Eggen, Richard Graham, Erik Kjellström, Emily Becker, Kathleen V . Pegion, Neil J. Holbrook,
Darryn McEvoy, Michael Depledge, Sarah Perkins-Kirkpatrick, Timothy J. Brown, Roger Street,
Lindsey Jones, Tomas A. Remenyi, Indi Hodgson-Johnston, Carlo Buontempo, Rob Lamb,
Holger Meinke, Berit Arheimer, and Stephen E. Zebiak. Potential applications of subseasonal-
to-seasonal (S2S) predictions. Meteorological Applications , 24(3):315–325, 2017. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/met.1654.
[49] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eick-
hoff. A Transformer-based Framework for Multivariate Time Series Representation Learning.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining ,
pages 2114–2124, Virtual Event Singapore, August 2021. ACM.
[50] Chaoning Zhang, Chenshuang Zhang, Junha Song, John Seon Keun Yi, Kang Zhang, and In So
Kweon. A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond,
July 2022. arXiv:2208.00173 [cs].
[51] Chidong Zhang. Madden-Julian Oscillation. Reviews of Geophysics , 43(2), 2005. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1029/2004RG000158.
7