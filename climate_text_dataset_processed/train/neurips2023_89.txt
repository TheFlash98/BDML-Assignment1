DeepEn2023: Energy Datasets for Edge Artificial
Intelligence
Xiaolong Tu
Department of Computer Science
Georgia State University
Atlanta, GA 30302
xtu1@student.gsu.eduAnik Mallik
Department of Electrical and Computer Engineering
The University of North Carolina at Charlotte
Charlotte, NC 28223
amallik@uncc.edu
Haoxin Wang
Department of Computer Science
Georgia State University
Atlanta, GA 30302
haoxinwang@gsu.eduJiang Xie
Department of Electrical and Computer Engineering
The University of North Carolina at Charlotte
Charlotte, NC 28223
linda.xie@uncc.edu
Abstract
Climatechangeposesoneofthemostsignificantchallengestohumanity. Asaresult
of these climatic changes, the frequency of weather, climate, and water-related
disasters has multiplied fivefold overthe past 50 years, resulting in over 2million
deaths and losses exceeding $3.64 trillion USD. Leveraging AI-powered technolo-
gies for sustainable development and combating climate change is a promising
avenue. NumeroussignificantpublicationsarededicatedtousingAItoimprove
renewableenergyforecasting,enhancewastemanagement,andmonitorenviron-
mental changes in real time. However, very few research studies focus on making
AIitselfenvironmentallysustainable. Thisoversightregardingthesustainability
of AI within the field might be attributed to a mindset gap and the absence of
comprehensive energydatasets. In addition, withthe ubiquityof edge AIsystems
andapplications,especiallyon-devicelearning,thereisapressingneedtomeasure,
analyze, and optimize their environmental sustainability, such as energy efficiency.
Tothisend,inthispaper,weproposelarge-scaleenergydatasetsforedgeAI,named
DeepEn2023,coveringawiderangeofkernels,state-of-the-artdeepneuralnetwork
models, and popular edge AI applications. We anticipate that DeepEn2023 will
improve transparency in sustainability in on-device deep learning across a range of
edgeAIsystemsandapplications. Formoreinformation,includingaccesstothe
dataset and code, please visit https://amai-gsu.github.io/DeepEn2023.
1 Introduction
Environmentally-sustainableAIreferstothedesignanduseofartificialintelligence(AI)andmachine
learning (ML) technologies to tackle environmental issues and advance sustainability [ 1], which is a
two-sided research area: AI for sustainability and sustainability of AI [ 2,3]. While there is growing
interest in using AI to achieve the Sustainable Development Goals (SDGs) [ 4] related to climate
change, research addressing the environmental impact of AI itself remains limited [ 5] [6] [7] [8]. For
instance, asophisticated AI-empoweredInternet ofThings (IoT) systemcan bedeployed tomonitor
andpredictthetotalcarbonemissionsofabuildingorfactory,aligningwiththeobjectiveofAIfor
sustainability. However, this raises new questions: How much carbon does this AI system emit? How
sustainable is the AI system itself?
Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.On-device learning on edge devices, such as smartphones, IoT devices, and connected vehicles,
is increasingly prevalent for model personalization and enhanced data privacy, yet its impact in
terms of carbon emission is often overlooked [ 1,9,10]. This oversight might be attributed to the
typicallymodestpowerconsumptionandcarbonfootprintofindividualedgedevices. However,when
considering the immense proliferation of these AI-empowered devices worldwide, their cumulative
carbonfootprintwouldbesubstantialandcannotbeoverlooked. Forinstance,considerascenario
where an individual uses AI-powered applications on their smartphone for one hour every day. The
averagepowerconsumptionofasmartphoneis 3W[11]. With 6.4billionsmartphoneconnections
reported in 2022[12], the cumulative energy consumption of these smartphones amounts to 19,200
MWhperday. BasedontheU.S.electricitygenerationcarbonintensityof 371.2kgofcarbonperMWh
[13],theestimateddailycarbonemissionsfromthesesmartphoneswouldbe 7127.04metrictons. For
comparison,thisisequivalenttotheannualcarbonfootprintof 1,848gasoline-poweredpassenger
vehicles. [ 14]. Therefore, to understand and evaluate the sustainability of AI systems, especially
edge AI systems, we have developed three large-scale energy consumption datasets: kernel-level,
model-level, and application-level . Wehopeourenergydatasets,named DeepEn2023 ,willencourage
both the research community and end-users to prioritize sustainability in on-device learning and edge
AI, a principle that drives our research.
2 Energy Measurement Platform
We developed an energy measurement platform employing the MonsoonPower Monitorto capture
powerconsumptiondataduringmodelexecution. Thispowerdata,combinedwithinferencelatency,is
usedtogenerateenergydatasets. TheMonsoonPowerMonitorisselectedforitsmillisecond-leveldata
granularity. Since most DNN model latencies, typically between 10 to 200 ms on mobile CPUs, can
be significantly decreased to 1 to 50 ms on mobile GPUs. Compared to built-in smartphone sensors,
the Monsoon provides more accurate and detailed power consumption data, especially for models
runningonedgedevices. Fig.1illustratesthepowermeasurementplatformwehaveimplemented.
We connected battery-removed smartphones to the power monitor using power cables. Then use
Monsoon power monitor to power on the devices and measure power consumption during model
execution with a granularity of up to 0.2 ms. We generated thousands of TensorFlow Lite models
acrossvariouslevelsandexecutedthemondifferenthardwareplatformstocreateacomprehensive
dataset.
Forourstudy,weselectedeightmodernedgedevicesfeaturingeightdifferentmobileSoCs,including
at least one high-end and one mid-range SoC from leading chipset vendors such as Qualcomm,
HiSilicon, and MediaTek. These SoCs have been chosen for their status as representative and
advanced mobile AI silicon widely used in the last two years.
3 DeepEn2023: Energy Consumption Dataset
In this section, we provide details of our datasets and how it contributes to understanding the energy
consumption and carbon emissions of edge AI systems. We have generated comprehensive datasets
for typical kernels, models and applications across various configurations. We also discuss how each
datasetcanfacilitateresearcheffortsaimedataccessingtheadverseimpactof AIcarbonemissions
on global climate change.
3.1 Kernel-level Energy Consumption Dataset
Kernels constitute the fundamental units of execution in deep learning frameworks, with their types
and configuration parameters significantly influencing the energy consumption during DNN model
executions. InTable1welistninetypicalkernelsthatarepresentinalmostallCNNmodels,with
the energy consumption and the carbon emission range for different configurations. The primary
configurations include input height and width ( 𝐻𝑊)1, input channel number ( 𝐶𝑖𝑛), output chan-
nel number ( 𝐶𝑜𝑢𝑡), kernel size ( 𝐾𝑆), and stride ( 𝑆). Here are the key observations : 1) Energy
consumption varies significantly for same kernel with different configurations on CPU and GPU.
2)Differentconfigurationparametershavevaryingimpactsforthekernelsenergyconsumption3)
conv⧺bn⧺relukernelstypicallyconsumemoreenergythanotherkerneltypes. 4)Acrossalmostall
1In CNN models, input height usually is equal to input width.
2Energy consumptionCarbon emission
•8 different commercial smartphones with advanced AI chipsets from Qualcomm, HiSilicon, and MediaTek
•Thousands of kernel models•Hundreds of CNN models•6 different state-of-the-art AI applications 
Monsoon Power Monitor(with 0.2ms time-granularity)
Carbon intensity
Figure 1: Power Measurement Platform utilizing the Monsoon Power Monitor to capture energy
consumptiondata. Then,carbonintensityareusedto convertthisenergydataintocarbonemission
estimates.
Table 1: Measured kernels per device in our kernel-level dataset.
KernelsEnergy Consumption (mJ) Carbon Emission (gCO2eq/kWh)2# Measured kernels
Configurations CPU GPU CPU GPUCPU GPUmin - max min - max min - max min - max
conv⧺bn⧺relu 0.002 - 1200.083 0.002 - 120.152 1.762 × 10−10-1.057 × 10−41.762 × 10−10-1.058 × 10−51032 1032 ( 𝐻𝑊 , 𝐶𝑖𝑛, 𝐶𝑜𝑢𝑡, 𝐾𝑆, 𝑆)
dwconv ⧺bn⧺relu0.022 - 222.609 0.016 - 0.658 1.938 × 10−9-1.961 × 10−51.409 × 10−9-5.797 × 10−8349 349 ( 𝐻𝑊 , 𝐶𝑖𝑛, 𝐾𝑆, 𝑆)
bn⧺relu 0.002 - 161.334 0.001 - 14.594 1.762 × 10−10-1.421 × 10−58.811 × 10−11-1.285 × 10−6100 100 ( 𝐻𝑊 , 𝐶𝑖𝑛)
relu 0.001 - 141.029 0.003 - 6.86 8.811 × 10−11-1.242 × 10−52.643 × 10−10-6.044 × 10−746 46 ( 𝐻𝑊 , 𝐶𝑖𝑛)
avgpool 0.066 - 7.711 0.034 - 1.142 5.815 × 10−9-6.794 × 10−72.995 × 10−9-1.006 × 10−728 28 ( 𝐻𝑊 , 𝐶𝑖𝑛, 𝐾𝑆, 𝑆)
maxpool 0.054 - 7.779 0.032 - 1.214 4.758 × 10−9-6.854 × 10−72.819 × 10−9-1.069 × 10−728 28 ( 𝐻𝑊 , 𝐶𝑖𝑛, 𝐾𝑆, 𝑆)
fc 0.038 - 94.639 - 3.348 × 10−9-8.338 × 10−7- 24 - ( 𝐶𝑖𝑛, 𝐶𝑜𝑢𝑡)
concat 0.001 - 42.826 0.066 - 3.428 8.811 × 10−11-3.773 × 10−65.815 × 10−9-3.020 × 10−7142 142 ( 𝐻𝑊 , 𝐶𝑖𝑛1, 𝐶𝑖𝑛2, 𝐶𝑖𝑛3, 𝐶𝑖𝑛4)
others 0.001 - 132.861 0.003 - 10.163 8.811 × 10−11-1.170 × 10−52.643 × 10−10-8.954 × 10−798 72 ( 𝐻𝑊 , 𝐶𝑖𝑛)
the kernels, GPU exhibit better energy efficiency under same configurations. Studying the impact of
kernel configurations on energy consumption lays the foundation for a comprehensive understanding
of energy usage during DNN model executions on edge devices. This emphasizes the importance of
adaptiveconfigurationselecting,inenhancingtheenergyefficiencyofDNNmodelsandhowitcan
benefit researchers working toward carbon netural goal.
To build the dataset, we initially generate a large number of kernels with a variety of types (16 types
for CPU and 10 types for GPU) featuring a range of configurations in the tfliteformat (e.g., 1032
conv⧺bn⧺reluand349dwconv ⧺bn⧺relukernels). These kernel configurations are randomly
sampled. The number of sampled configurations for each kernel type hinges on two main factors: its
configurationdimensionanditsimpactontheoverallenergyconsumptionduringDNNexecutions.
This dataset provides researchers with detailed insights into how energy is consumed within models
and which configurations or parameters affect kernel energy efficiency. Researchers can use this
datasettoadaptconfigurationswithbesterergyefficiencyonedgedevices,consequentlyreducing
carbon emissions.
3.2 Model-level Energy Consumption Dataset
We also introduce our model-level energy dataset, which collects nine state-of-the-art DNN models.
These models represent a mix of both manually-designed and NAS-derived models, each with
distinct kernel types and configurations. For each model, we generate 50variants for conducting
power and energy measurements by re-sampling the 𝐶𝑜𝑢𝑡and𝐾𝑆for each layer. Specifically, we
randomlysamplethenewoutputchannelnumberfromarangeof 20%to180%oftheoriginal 𝐶𝑜𝑢𝑡,
2TheunitofmeasurementtypicallyusedforquantifyingandcomparingcarbonemissionsisCO2equivalents.
3AlexnetDenseNetGoogleNetMobileNetv1 MobileNetv2ProxylessNASResNet18
ShuffleNetv2SqueezeNetEnergy consumption percentage (%)conv dwconv fc concat others(a) Mobile CPU
AlexnetDenseNetGoogleNetMobileNetv1 MobileNetv2ProxylessNASResNet18
ShuffleNetv2SqueezeNetEnergy consumption percentage (%)conv dwconv fc concat others (b) Mobile GPU
Figure 2: DNN model energy consumption percentage breakdown. The top four most energy-
consuming kernel types are conv⧺bn⧺relu(conv),dwconv ⧺bn⧺relu(dwconv),fc, and concat.
whilethe 𝐾𝑆issampledfromthesetofvalues: {1,3,5,7,9}. Generally,runningthesemodelson
mobileGPUsresultsinanenergyconsumptionreductionofapproximately 49%to79%,compared
tothe executionon mobileCPUs. Fig. 2presents theenergyconsumption breakdownof individual
models by kernel types. The four kernel types that consume the most energy are conv⧺bn⧺relu,
dwconv ⧺bn⧺relu,fc,and concat. Theyaccountfor 79.27%,14.79%,2.03%,and 1.5%ofthetotal
model energy consumption on the mobile CPU, respectively. On the mobile GPU, these kernels
represent 78.17%,10.91%,4.01%,and 4.28%ofthetotalmodelenergyconsumption. Furthermore,in
most models, conv⧺bn⧺reluanddwconv ⧺bn⧺reluaccount for the main energy percentages. On
average, conv⧺bn⧺reluanddwconv ⧺bn⧺relutake 93.97%and87.74%of the total model energy
consumption on the mobile CPU and GPU, respectively. With this model-level energy consumption
dataset,researcherscanvisuallyseetheenergyconsumptionofdifferentmodelsonvariousplatforms,
helping them choose he most energy-efficient models according to their needs.
3.3 Application-level Energy Consumption Dataset
Thekernel-andmodel-leveldatasetscanbebeneficialforresearchersanddevelopersinunderstanding,
modelling, and optimizing power and energy efficiency of DNN executions. However, the energy
efficiency ofapplications on edgedeviceshas a moredirect impact oncarbon emissions. To adress
this, we create an application-level dataset, which uncovers the end-to-end energy consumption of six
popular edge AI applications, covering three main categories: vision-based (object detection, image
classification, super resolution, and image segmentation), NLP-based (natural language question
answering), and voice-based applications (speech recognition). As shown in Table 2, we measure
the power and energy consumption of each application with multiple reference DNN models that
operateunderfourdistinctcomputationalsettings,includingCPUwithasinglethread,CPUwithfour
threads,GPUdelegate,andtheNNAPIdelegate. Thedatasetcanserveasaresourceforexploring
the energy consumption distribution throughout the end-to-end processing pipeline of an edge AI
application. Forexample,wecanusethedatasettoexaminetheenergyconsumedingeneratingimage
frames, converting these frames from YUV to RGB, and conducting DNN inference within an object
detection application. It demonstrates that our application-level dataset can provide interpretable
observations for comprehending who is the primary energy consumer in the end-to-end edge AI
application. Fig. 3 depicts the energy consumption breakdown based on the processing phases in
the object detection. It demonstrates that our application-level dataset can provide interpretable
observations for comprehending who is the primary energy consumer in the end-to-end edge AI
application.
3.4 Beneficial For Global Climate Change
Thesethree datasetscancontribute toaddressingglobal climatechange fromdifferentperspectives.
Forexample,thekernel-leveldatasetcanassistresearchersinidentifyingthemostenergy-efficient
4Camera LensImage sensorBayer filterImage signal processingImage bufferScale & cropPreviewImage readerYUV to RGB & cropDetection resultsImage GenerationImage conversion
Inference
DNN(a) End-to-end processing pipeline for object detection
and image classification
0 20 40 60 80 100
Energy consumption percentage (%)Object
detection
Image
classificationImage generation Image conversion Inference(b) Energy consumption percentage breakdown
Figure3: End-to-endenergyconsumptionbreakdownforobjectdetectionandimageclassification
based on our application-level dataset.
Table 2: Measured edge AI applications per device in our application-level dataset.
DelegateCategory Application Reference DNN modelsCPU1 CPU4 GPU NNAPIModel size
(MB)
MobileNetv2, FP32, 300 ×300 pixels ✓ ✓ ✓ 24.2
MobileNetv2, INT8, 300 ×300 pixels ✓ ✓ ✓ 6.9
MobileNetv2, FP32, 640 ×640 pixels ✓ ✓ ✓ 12.3Image detection
MobileNetv2, INT8, 640 ×640 pixels ✓ ✓ ✓ 4.5
EfficientNet, FP32, 224 ×224 pixels ✓ ✓ ✓ ✓ 18.6
EfficientNet, INT8, 224 ×224 pixels ✓ ✓ ✓ 5.4
MobileNetv1, FP32, 224 ×224 pixels ✓ ✓ ✓ ✓ 4.3Image classification
MobileNetv1, INT8, 224 ×224 pixels ✓ ✓ ✓ 16.9
Super resolution ESRGAN , FP32, 50 ×50 pixels ✓ ✓ 5Vision-based
Image segmentation DeepLabv3 , FP32, 257 ×257 pixels ✓ 2.8
NLP-based Natural language question answering MobileBERT , FP32 ✓ ✓ ✓ 100.7
Voice-based Speech recognition Conv-Actions-Frozen , FP32 ✓ ✓ ✓ 3.8
kernelconfigurationsandparameters,findingthebalancebetweencomputingperformanceandcarbon
emissions. Wehaveusedourdatasettotrainarandomforestmodeltopredicttheenergyconsumption
andcarbonemissionsofunseenmodels,andtheaccuracyisquitepromising[ 15]. Themodel-level
dataset aids researchers in discovering the most energy-efficient models based on various deployment
requirements. Forinstance,ModelAdeployedonaCPUmayexhibitbetterenergyefficiencythan
Model B with the same accuracy for image classification. The application-level dataset provides
researchers with insights into the end-to-end energy consumption of an application on edge devices,
enabling them to implement more comprehensive measures to reduce energy consumption.
4 Conclusion
Inthispaper,wepresentourenergyconsumptiondatasets,DeepEn2023,fromkernel-level,model-
level, and application-level to facilitate research and development aimed at improving the energy
efficiency and reducing the carbon emissions of AI applications on diverse edge devices. These
datasets are valuable resources and tools for researchers and community to design energy-efficiency
AI systems with fewer greenhouse gas emissions, thus contributing to the global climate change
mitigation. We hope DeepEn2023 can help shift the mindset of both end-users and the research
community towards sustainable edge AI, a principle that drives our research.
Acknowledgments and Disclosure of Funding
Thisworkwas supported bythe USNational ScienceFoundation(NSF)underGrant No. 1910667,
1910891, and 2025284.
5References
[1]Carole-JeanWu,RamyaRaghavendra,UditGupta,BilgeAcun,NewshaArdalani,KiwanMaeng,
Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental
implications, challenges and opportunities. Proceedings of Machine Learning and Systems ,
4:795–813, 2022.
[2]AimeeVanWynsberghe. SustainableAI:AIforsustainabilityandthesustainabilityofAI. AI
and Ethics , 1(3):213–218, 2021.
[3]Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami
Domisch,AnnaFelländer,SimoneDanielaLanghans,MaxTegmark,andFrancescoFusoNerini.
The role of artificial intelligence in achieving the sustainable development goals. Nature
communications , 11(1):1–10, 2020.
[4]Sustainbale Development Goal. https://sdgs.un.org/goals/goal13 . Accessed on
September 2023.
[5]Jie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing
{GPU }energyconsumptionof {DNN }training. In 20th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 23) , pages 119–139, 2023.
[6]ManniWang,ShaohuaDing,TingCao,YunxinLiu,andFengyuanXu. Asymo: scalableand
efficientdeep-learninginferenceonasymmetricmobilecpus. In Proceedings of the 27th Annual
International Conference on Mobile Computing and Networking , pages 215–228, 2021.
[7]SiminChen,MirazulHaque,CongLiu,andWeiYang. Deepperform: Anefficientapproach
for performance testing of resource-constrained neural networks. In Proceedings of the 37th
IEEE/ACM International Conference on Automated Software Engineering , pages 1–13, 2022.
[8]Dongqi Cai, Qipeng Wang, Yuanqiang Liu, Yunxin Liu, Shangguang Wang, and Mengwei
Xu. Towardsubiquitouslearning: Afirstmeasurementofon-devicetrainingperformance. In
Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning , pages
31–36, 2021.
[9]Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan,
Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al. Machine learning at facebook:
Understanding inference at the edge. In Proceedings of 2019 IEEE International Symposium on
High Performance Computer Architecture (HPCA) , pages 331–344, 2019.
[10]Stefano Savazzi, Sanaz Kianoush, Vittorio Rampa, and Mehdi Bennis. A framework for energy
and carbon footprint analysis of distributed and federated edge learning. In Proceedings of
2021 IEEE 32nd Annual International Symposium on Personal, Indoor and Mobile Radio
Communications (PIMRC) , pages 1564–1569, 2021.
[11]Haoxin Wang, BaekGyu Kim, Jiang Xie, and Zhu Han. Energy drain of the object detection
processingpipelineformobiledevices: Analysisandimplications. IEEE Transactions on Green
Communications and Networking , 5(1):41–60, 2020.
[12]The Mobile Economy 2023. https://www.gsma.com/mobileeconomy/wp-content/
uploads/2023/03/270223-The-Mobile-Economy-2023.pdf . Accessed on September
2023.
[13]How much carbon dioxide is produced per kilowatthour of U.S. electricity generation? https:
//www.eia.gov/tools/faqs/faq.php?id=74&t=11 . Accessed on September 2023.
[14]Greenhouse Gas Equivalencies Calculator. https://www.epa.gov/energy/
greenhouse-gas-equivalencies-calculator#results . Accessed on September
2023.
[15]XiaolongTu,AnikMallik,DaweiChen,KyungtaeHan,OnurAltintas,HaoxinWang,andJiang
Xie. Unveiling energy efficiency in deep learning: Measurement, prediction, and scoring across
edgedevices. In Proc. The Eighth ACM/IEEE Symposium on Edge Computing (SEC) ,pages
1–14, 2023.
6