Generalized Policy Learning for Smart
Grids: FL TRPO Approach
Yunxiang Li
MBZUAI, UAE
yunxiang.li@mbzuai.ac.aeNicolas M. Cuadrado
MBZUAI, UAE
nicolas.avila@mbzuai.ac.ae
Samuel Horváth
MBZUAI, UAE
samuel.horvath@mbzuai.ac.aeMartin Takáč
MBZUAI, UAE
martin.takac@mbzuai.ac.ae
Abstract
The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
whilemaintainingdataprivacy, makingitsuitableforsmartgridapplications,
which often involve disparate data distributions and interdependencies
among features that hinder the suitability of linear models. This paper
introduces a framework that combines FL with a Trust Region Policy
Optimization (FL TRPO) aiming to reduce energy-associated emissions and
costs. Ourapproachrevealslatentinterconnectionsandemployspersonalized
encodingmethodstocaptureuniqueinsights, understandingtherelationships
between features and optimal strategies, allowing our model to generalize
to previously unseen data. Experimental results validate the robustness of
our approach, affirming its proficiency in effectively learning policy models
for smart grid challenges.
1 Introduction
The world faces the pressing challenge of climate change mainly due to energy use, leading to
an increased focus on renewable energy sources and energy-efficient systems as a mitigation
technique. Thesmartgriddomainconsidersenergystoragesystems, cutting-edgetechnologies,
distributed energy resources management, and automated demand response as promising
solutions that can help optimize energy usage and reduce carbon emissions. However, the
energy grid is a complex and interconnected system that suffers from the constantly changing
scenarios brought by the random nature of renewable.
Machine Learning seems to be a pivotal tool to address the complexities of the scenario
mentioned above Rolnick et al. (2019). Still, real-life applications must address the gen-
eralization of models and preservation of sensitive energy usage data. We propose an FL
TRPO model. This framework harnesses inherent generalization capability and privacy
guarantees from FL to discern latent relationships within the feature space. In conjunction
with TRPO, strategically incorporating a purposefully designed model to leverage prior
knowledge regarding the features, the FL TRPO model exhibits the robustness necessary to
map from feature space to optimal policy outcomes effectively.
•This research evaluates the performance of FL TRPO in the context of a smart grid
problem, leveraging the faster convergence of TRPO with the fewer communication rounds
time of FL.
•FL generalize effectively to previously unseen datasets Han et al. (2022). We constructed
training and testing datasets with different distributions to showcase this ability in the
smart grid scenario. We incorporated intricate environmental relationships where the
reward is a non-linear function of the features.
1•By exposing agents to varying training and testing data distributions, we conducted a
comparative analysis of performance with and without FL.
•Harnessing our prior understanding of the features, we engineered a model capable of
computing optimal policies, thereby elevating the overall model performance.
2 Related Work
Federated Learning. FL provides methods to train models across decentralized agents
while keeping privacy. Federated Averaging (FedAvg) McMahan et al. (2017) stands as the
pioneering framework in FL, where agents exchange weights after multiple updates on their
respective local data. Researchers have enhanced FL by building upon the notion of training
decentralized agents. Gossip algorithms ensure efficient communication Koloskova et al.
(2019), split learning offers a method to achieve personalization Vepakomma et al. (2018),
differential privacy ensures to safeguard privacy Geyer et al. (2017), novel computation and
communication techniques have been developed to handle heterogeneous data Diao et al.
(2021), and analysis generalization especially in FL Yuan et al. (2022). Benefiting from
its ability to handle heterogeneous data training, preserving privacy, and generalization on
unseen data, FL demonstrates promising performance in addressing challenges related to
smart grid scenarios.
Smart grids problem. Recently, the challenges within smart grid systems have garnered
attention across academic and industrial spheres. Ceusters et al. (2021) devised a model-
based reinforcement learning (RL) approach tailored to multi-energy systems, enhancing
it with constrained optimization algorithms to derive optimal solutions. Meanwhile, Su
et al. (2022) introduced a Federated RL (FRL) methodology, considering the dynamics
between Energy Data Owners (EDOs) and Energy Service Providers (ESPs), addressing
free-rider phenomena and non-independent and identically distributed (IID) data distribution
nuances. In Rezazadeh and Bartzoudis (2022), this work was further enriched by harnessing
a discretized Soft Actor-Critic (SAC) framework within the FRL paradigm, leading to faster
convergence and mitigating privacy breaches. Similarly, Lee and Choi (2022) leveraged FRL
and demonstrated faster convergence and better performance than agents without FRL.
Furthermore, Cuadrado et al. (2023a) introduced a hierarchical FRL architecture, yielding
performance enhancements. Cuadrado et al. (2023b) presented a holistic framework for
smart grid problems with three layers of different objectives. Despite these pivotal advances
highlighting FRL’s potency in energy management difficulties, the focus has primarily
centered on its operational efficacy.
3 Problem and Environment
We based our environment setting on CityLearn Vázquez-Canteli et al. (2019), which provides
an OpenAI Gym environment for building energy coordination and demand response based
on a real dataset Wilson et al. (2021). We generated two environments (training and
evaluation) containing five buildings, each equipped with a non-shiftable load, solar panels,
and a controllable battery managed by the agent. The grid price pattern mirrors demand
fluctuations and is charged as a Time-of-Use (ToU) electricity rate, peaking during high-
demand hours, while the solar panels contribute energy throughout the daylight hours.
Allowing the agent to stockpile energy during off-peak times and, when solar generation
exceeds immediate needs, later release stored energy during high-price periods. Emissions
follow the actual emission rate from the grid mix (kgCO2e/kWh) for the dataset used in the
environment.
Each episode spans twenty-four hours, and tis the index for the time step. For a building
i, an agent considers the solar panels’ energy and the grid energy price to decide the
charge or discharge of the battery in a time step ( Ebatt
t,i) aiming to meet the building’s load
requirements. Its primary objective is minimizing the overall energy cost of all buildings. We
streamlined the state features by choosing the following key observations: outdoor dry bulb
temperature Tout
t, outdoor relative humidity Hout
t, battery storage state Bsoc
t, net electricity
consumption Enet
t, electricity pricing Ct, and hour of the day t. For the reward, we defined it
2as the negative net consumption from the grid for a building Egrid
t,i, where Eload
t,iis building’s
non-shiftable load, and Esolar
t,i. The table 1 presents an overview of the RL case.
Action Ebatt
t,i∈[−1,1]
State {Tout
t, Hout
t, Bsoc
t, Enet
t, Ct, t}
Reward −Egrid
t,i=−max{Eload
t,i+Ebatt
t,i−Esolar
t,i,0}
Table 1: RL case definition
We represented the non-shiftable load and solar panel generation as a non-linear function
of temperature, humidity, and some respective bases, reflecting real-world dynamics. That
simulates real-world scenarios and effectively demonstrates the collaborative learning capacity
of interrelations between features with FL. Additionally, for a personalized approach, we
have set different coefficients for each building to cater to individual energy consumption
patterns and characteristics, such as the air conditioner efficiency and the solar panel size.
We present details of the environment in the Appendix.
4 Method
The model. Understanding how features interact, we designed a model to capture these
interactions faithfully, aiming to ensure the model’s effectivity to represent the complex
connections between features and the optimal policy. The architecture of our model is
depicted in Figure 1. We divided it into two parts: a personal component on the client
side capturing individual information and a shared component capturing common features
to enhance generalization. The personal component accommodates each building’s unique
demand and solar generation profiles, which are exclusively accessible to each building client,
maintaining privacy in line with Vepakomma et al. (2018). On the other hand, the shared
component captures the shared information by concatenating the temperature, humidity,
and personalized encoding, which embodies these distinctive patterns. Subsequently, we
obtain the policy and state values for agent actions by concatenating this with the processed
remaining features through an additional neural network layer.
Figure 1: Our model captures the inherent interdependencies among features in mapping
between states and policies.
FL TRPO with details. We use FL to collectively discern the interconnected relationships
among demand, solar generation, temperature, and humidity across buildings. Relying solely
on data from a single building risks overfitting, but leveraging diverse data distributions
from multiple buildings helps uncover latent connections. Moreover, the model adeptly
extrapolates unseen data instances by assimilating insights from disparate building data
distributions, handling anomalies like households on vacation or harsh weather conditions.
In our problem, a key challenge arises with the possibility of the agent overcharging and
discharging the battery, resulting in identical rewards as standard actions. To address this, we
3introduce a penalty to discourage unwarranted overcharging and discharging. Moreover, the
agent has to make a sequence of pivotal decisions: charging during periods of low electricity
pricing and discharging when prices are high. Given the expansive search space, we used
TRPO as it adeptly explores the vast space by identifying suitable step sizes within a confined
region, ensuring stable policy, monotonic improvement, and more effective exploration than
other policy gradient methods.
5 Experiments
We demonstrate the generalization ability of our model by employing different training and
testing data distributions. The reader can find detailed information regarding data generation
in Appendix 6.2. Using testing data, we establish our baseline, denoted as Upperbound , by
training individual agents in each building. Then, we trained individual agents on each
building using testing data as a reference that does not use FL, denoted as Ind. Agent . We
then compare the performance of FL TRPO against the conventional approach of training
separate TRPO agents per building.
Additionally, we conduct an ablation study of the personalized encoding component by
comparing our method with a model structured to share all features, denoted as FL. Our
model, referred to as FL Personalization , incorporates personalized encoding. Figure 2
presents five buildings’ average reward and emission trends. The Appendix 7 contains further
details regarding the reward and emission for each building.
Figure 2: Average reward and emission of the five buildings across five random seeds.
Upperboud (Blue): A single TRPO agent trained using the testing dataset to establish the
upper-performance limit. FL (Green): Model structured with all parts shared trained with
FL methodology. Ind. Agent (Red): TRPO agent trained separately for each building. FL
Personalization (Orange): FL TRPO with personalized encoding as detailed in Section 4,
trained using FL methodology.
As depicted in Figures 2 and 7, FL TRPO consistently outperforms FLandInd. Agents across
all buildings, approaching optimal baselines. Notably, FLandInd. Agents exhibit similar
performance, indicating that a simple model with FL training may not yield improvements.
We hypothesize that different buildings require distinct optimal policies with the same state.
FL, lacking the ability to differentiate between buildings, may compromise performance.
In contrast, with personalized encoding, FL leverages our prior knowledge, learns shared
information, and can tailor policies for different clients.
6 Conclusion
This paper delved into applying FL to the TRPO approach in multi-building energy manage-
ment. Our results underscore the effectiveness of FL in facilitating TRPO to discern intricate
feature interdependencies, thereby enabling robust generalization across unobserved data
distributions. Furthermore, introducing the integration of a feature-based model enhanced
the performance of FL TRPO. This contribution validates the potential of our methodology
to learn policies that effectively reduce both emissions and energy costs within microgrids
while guaranteeing generalization and privacy preservation.
4References
Glenn Ceusters, Román Cantú Rodríguez, Alberte Bouso García, Rüdiger Franke, Geert
Deconinck, Lieve Helsen, Ann Nowé, Maarten Messagie, and Luis Ramirez Camargo.
Model-predictive control and reinforcement learning in multi-energy system case studies.
CoRR, abs/2104.09785, 2021. URL https://arxiv.org/abs/2104.09785 .
Nicolas M. Cuadrado, Roberto Alejandro Gutiérrez Guillén, and Martin Takác. FRESCO:
federated reinforcement energy system for cooperative optimization. In Krystal Maughan,
Rosanne Liu, and Thomas F. Burns, editors, The First Tiny Papers Track at ICLR 2023,
Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023 . OpenReview.net, 2023a. URL
https://openreview.net/pdf?id=75mWq5j4iso .
Nicolas M. Cuadrado, Roberto A. Gutiérrez, Yongli Zhu, and Martin Takác. MAHTM: A
multi-agent framework for hierarchical transactive microgrids. CoRR, abs/2303.08447,
2023b. doi: 10.48550/arXiv.2303.08447. URL https://doi.org/10.48550/arXiv.2303.
08447.
EnmaoDiao, JieDing, andVahidTarokh. Heterofl: Computationandcommunicationefficient
federated learning for heterogeneous clients. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net,
2021. URL https://openreview.net/forum?id=TNkPBBYFkXg .
Robin C. Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning:
A client level perspective. CoRR, abs/1712.07557, 2017. URL http://arxiv.org/abs/
1712.07557 .
Dong-Jun Han, Do-Yeon Kim, Minseok Choi, Christopher G. Brinton, and Jaekyun Moon.
Splitgp: Achieving both generalization and personalization in federated learning. CoRR,
abs/2212.08343, 2022. doi: 10.48550/arXiv.2212.08343. URL https://doi.org/10.
48550/arXiv.2212.08343 .
Anastasia Koloskova, Sebastian U. Stich, and Martin Jaggi. Decentralized stochastic opti-
mization and gossip algorithms with compressed communication. In Kamalika Chaudhuri
and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97
ofProceedings of Machine Learning Research , pages 3478–3487. PMLR, 2019. URL
http://proceedings.mlr.press/v97/koloskova19a.html .
Sangyoon Lee and Dae-Hyun Choi. Federated reinforcement learning for energy management
of multiple smart homes with distributed energy resources. IEEE Trans. Ind. Informatics ,
18(1):488–497, 2022. doi: 10.1109/TII.2020.3035451. URL https://doi.org/10.1109/
TII.2020.3035451 .
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Aarti Singh
and Xiaojin (Jerry) Zhu, editors, Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale,
FL, USA , volume 54 of Proceedings of Machine Learning Research , pages 1273–1282.
PMLR, 2017. URL http://proceedings.mlr.press/v54/mcmahan17a.html .
Farhad Rezazadeh and Nikolaos G. Bartzoudis. A federated DRL approach for smart
micro-grid energy control with distributed energy resources. In 27th IEEE International
Workshop on Computer Aided Modeling and Design of Communication Links and Networks,
CAMAD 2022, Paris, France, November 2-3, 2022 , pages 108–114. IEEE, 2022. doi: 10.
1109/CAMAD55695.2022.9966919. URL https://doi.org/10.1109/CAMAD55695.2022.
9966919.
David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris
Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-
Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli,
Konrad P. Körding, Carla P. Gomes, Andrew Y. Ng, Demis Hassabis, John C. Platt, Felix
Creutzig, Jennifer T. Chayes, and Yoshua Bengio. Tackling climate change with machine
learning. CoRR, abs/1906.05433, 2019. URL http://arxiv.org/abs/1906.05433 .
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust
region policy optimization. In Francis R. Bach and David M. Blei, editors, Proceedings of
the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11
5July 2015 , volume 37 of JMLR Workshop and Conference Proceedings , pages 1889–1897.
JMLR.org, 2015. URL http://proceedings.mlr.press/v37/schulman15.html .
Zhou Su, Yuntao Wang, Tom H. Luan, Ning Zhang, Feng Li, Tao Chen, and Hui Cao.
Secure and efficient federated learning for smart grid with edge-cloud collaboration. IEEE
Trans. Ind. Informatics , 18(2):1333–1344, 2022. doi: 10.1109/TII.2021.3095506. URL
https://doi.org/10.1109/TII.2021.3095506 .
José R. Vázquez-Canteli, Jérôme Henri Kämpf, Gregor Henze, and Zoltán Nagy. Citylearn
v1.0: An openai gym environment for demand response with deep reinforcement learning.
InProceedings of the 6th ACM International Conference on Systems for Energy-Efficient
Buildings, Cities, and Transportation, BuildSys 2019, New York, NY, USA, November
13-14, 2019 , pages 356–357. ACM, 2019. doi: 10.1145/3360322.3360998. URL https:
//doi.org/10.1145/3360322.3360998 .
PraneethVepakomma, OtkristGupta, TristanSwedish, andRameshRaskar. Splitlearningfor
health: Distributed deep learning without sharing raw patient data. CoRR, abs/1812.00564,
2018. URL http://arxiv.org/abs/1812.00564 .
Wilson, Eric, Parker, Andrew, Fontanini, Anthony, Present, Elaina, Reyna, Janet, Adhikari,
Rajendra, Bianchi, Carlo, CaraDonna, Christopher, Dahlhausen, Matthew, Kim, Janghyun,
LeBar, Amy, Liu, Lixi, Praprost, Marlena, White, Philip, Zhang, Liang, DeWitt, Peter,
Merket, Noel, Speake, Andrew, Hong, Tianzhen, Li, Han, Mims Frick, Natalie, Wang,
Zhe, Blair, Aileen, Horsey, Henry, Roberts, David, Trenbath, Kim, Adekanye, Oluwatobi,
Bonnema, Eric, El Kontar, Rawad, Gonzalez, Jonathan, Horowitz, Scott, Jones, Dalton,
Muehleisen, Ralph, Platthotam, Siby, Reynolds, Matthew, Robertson, Joseph, Sayers,
Kevin, , and Qu. Li. End-use load profiles for the u.s. building stock, 10 2021. URL
https://data.openei.org/submissions/4520 .
Honglin Yuan, Warren Richard Morningstar, Lin Ning, and Karan Singhal. What do we
mean by generalization in federated learning? In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,
2022. URL https://openreview.net/forum?id=VimqQq-i_Q .
6Appendix
The source code and demo files have been anonymized and are available in this repository
link.
6.1 Preliminaries
6.1.1 FedAvg
FedAvg is a fundamental FL technique that trains a centralized model while respecting
data decentralization and privacy; it initiates the process by training localized models using
individual data. After a series of model updates, the parameters of these local models are
transmitted to a central server and averaged. This iterative cycle of local training and central
aggregation is recurrently executed by FedAvg, progressively enhancing the global model’s
efficacy with distinct datasets. Equation 1 presents the FedAvg update, in which the letter θ
represents the parameters to train, ηa learning rate, nkthe data samples of client k,nthe
total number of samples and gkthe gradient at client k.
θt+1=θt−ηKX
k=1nk
ngk. (1)
6.1.2 TRPO
RL addresses sequential decision problems modeled as Markov Decision Processes (DMP).
The problem is defined by the tuple ( S,A,P,s,γ), where SandAdenote the observation
and action spaces. The transition function P:S × A → S represents state transitions’
probabilities, and the reward function r:S × A × S → Rprovides immediate rewards for
actions. The variable γ∈[0,1]denotes the discounted factor. A policy on an MDP is a
mapping function S → A. RL’s primary objective is finding a policy that maximizes the
cumulative rewards. Given a policy π, letVπdenote the value function of state sdefined as
Vπ(s) =Eπ"∞X
t=0γtrt|s0=s#
,
and let Qπdenote the Q value for a state-action pair (s, a)∈ S × A
Qπ(s, a) =Eπ"∞X
t=0γtrt|s0=s, a 0=a#
.
Then we can define the advantage function Aπ(s, a)as
A(s, a) =Q(s, a)−V(s).
Policy Gradient (PG) methods constitute a major branch among RL algorithms. While
simple PG algorithms share similarities with first-order gradient methods, they are prone to
overconfidence and a subsequent decline in performance. Trust Region Policy Optimization
(TRPO) Schulman et al. (2015) emerges as a cutting-edge PG algorithm to tackle this issue.
TRPO takes substantial steps to ensure that improvements remain within predefined bounds.
It does so by defining a surrogate advantage function L(θk, θ)that measures how much
the new policy πθchanged concerning the old policy πθk. The change is measured using
an average KL-Divergence denoted ˆDKL(θk||θ). We framed the problem as a trust region
optimization problem with a surrogate objective:
L(θk, θ) =Es,a∼πθkπθ(a|s)
πθk(a|s)Aπθk(s, a)
, (2)
θt+1=argmax
θL(θk, θ)s.t.ˆDKL(θk||θ)≤γ. (3)
Its comprehensive search for the optimal direction and step length enables TRPO to require
fewer update steps than standard policy gradient algorithms. Consequently, TRPO demands
less gradient communication in FL algorithms, making it an ideal choice for FL, which relies
on efficient communication.
7Environment details
Figure 3: Scenario definition.Each building features a photovoltaic panel, a non-
shiftable load, and a battery. The buildings can pro-
cure electricity from the grid, store energy generated
by the photovoltaic panels, and regulate the battery
by charging and discharging to fulfill the required
load.
6.2 Experiment settings
WecomparedFLTRPO’sperformancetotrainingone
agent per building to demonstrate that our algorithm benefits from unseen data distribution
scenarios. Without collaboration or information sharing, one agent per building is the same
as decentralized multi-agent reinforcement learning.
Non-shiftable load and solar generation depend on non-linear functions that rely on environ-
mental factors like temperature, humidity, and building-specific base patterns. Furthermore,
to substantiate the efficacy of our approach in generalizing the ability to unseen data distri-
butions, we introduce random variables with distinct ranges to the temperature and humidity
data generation during the training and testing phases. We randomly sample noise from
different ranges for each episode’s data to assess our model’s generalization ability, creating
a distinction between training and testing data. This approach allows us to showcase the
model’s generalization capability across varying conditions. In our experimental environment,
we used five buildings characterized by unique configurations. Figure 4 and Figure 5 detail
the training and testing range for solar generation and the non-shiftable load of the five
buildings, correspondingly.
Figure 4: Training and testing solar generation data of each building.
During training and testing, we randomly sampled one set for each episode. As depicted
in the figures, a disparity exists between the training and testing datasets. This partition
shows the model’s ability to generalize to unseen data distributions. To further illustrate our
environment, we provide solar generation data for building one against other buildings in
Figure 6. With heterogeneous data distribution, our method can learn the hidden correlations
and harness them for effective generalization.
8Figure 5: Training and testing non-shiftable load data of each building.
Figure 6: Comparison between solar generation data of building one and the rest of buildings.
FL can learn the hidden relationship among the features with different data distributions.
9Figure 7: Reward and emission of all buildings across five random seeds. Upperboud
(Blue): A single TRPO agent trained using the testing dataset to establish the upper-
performance limit. FL (Green): Model structured with all parts shared trained with FL
methodology. Ind. Agent (Red): TRPO agent trained separately for each building. FL
Personalization (Orange): FL TRPO with personalized encoding as detailed in Section 4,
trained using FL methodology. The performance of FL Personalization closely approaches
the optimal baseline. We explain this improvement due to the integration of a model that
effectively captures inherent feature relationships and personalized optimal policies.
10