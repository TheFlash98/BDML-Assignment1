MAHTM: A Multi-Agent Framework for
Hierarchical Transactive Microgrids
Nicolas M. Cuadrado∗
MBZUAI, UAE
nicolas.avila@mbzuai.ac.aeRoberto A. Gutiérrez∗
MBZUAI, UAE
roberto.guillen@mbzuai.ac.ae
Yongli Zhu
Texas A&M University
College Station, USA
yzhu16@vols.utk.eduMartin Takáč
MBZUAI, UAE
Takac.MT@gmail.com
Abstract
Integrating variable renewable energy into the grid has posed challenges to
system operators in achieving optimal trade-offs among energy availability,
cost affordability, and pollution controllability. This paper proposes a multi-
agent reinforcement learning framework for managing energy transactions
in microgrids. The framework addresses the challenges above: it seeks to
optimize the usage of available resources by minimizing the carbon footprint
while benefiting all stakeholders. The proposed architecture consists of
three layers of agents, each pursuing different objectives. The first layer,
comprised of prosumers and consumers, minimizes the total energy cost.
The other two layers control the energy price to decrease the carbon impact
while balancing the consumption and production of both renewable and
conventional energy. This framework also takes into account fluctuations in
energy demand and supply.
1 Introduction
As technology and urban areas continue to grow, the demand for energy increases and is
expected to continue to be high. Because of this, the world is moving towards greener
options, increasing the demand for renewable energy from industry and residential consumers
Leibowicz et al. (2018). Between renewable energies, we have solar, wind, tidal, hydropower,
and bio-energy. The challenge behind lays not only in the generation but guaranteeing
there is enough to supply the demand since the generation of renewable energy is inherently
stochastic (depends on multiple climate factors).
There is a need to adapt to the randomness of the situation, which can be solved by creating
specific energy systems controlled by machine learning models, which optimize the usage of
the available resources Vázquez-Canteli et al. (2019). For example, the concept of “smart
transactive grids” has been proposed to organize the demand and production of energy in
communities. The idea is to create an intelligent system that uses different energy sources to
supply the demand with minimal human intervention. At the same time, it provides the
opportunity to sell any surplus energy produced.
Some previous work leveraged Reinforcement Learning (RL) to create technologies that enable
transactive microgrids. In Anvari-Moghaddam et al. (2017), an approach using different
RL agents is proposed, where the distributions of agents are different: one agent is used
for particular computation (e.g., optimization), called Service Agent; the other two agents
collect meteorological information, and forecast the power output based on the specified type
of energy (solar, wind, etc.). In the design of its energy management system, one battery
is shared across the residential households, and all the agents communicate with a “central
∗These authors contributed equally to this work.
1coordinator agent”. In Vazquez-Canteli et al. (2020), the authors proposed a Multi-Agent
Reinforcement Learning (MARL) approach consisting of agents sharing two variables and
following a leader-follower schema to manage energy demand and generation. They also
proposed a specific reward function with greedy and collective goals to incentivize the agents
to work together as a community.
Other MARL approaches could be relevant to solve similar issues. In COLA Xu et al. (2022),
a consensus learning approach is proposed, which is inspired by the DINO (Distillation with
No Labels) Caron et al. (2021). In DINO’s method, a student network is created to predict
the teacher network results to simplify the self-supervised training process for methods that
(originally) require centralized training and decentralized inference. Authors of Foerster et al.
(2017) proposed the Counterfactual Multi-Agent (COMA) policy gradients. In this work,
they propose an architecture with a centralized critic to estimate the action-value function
and decentralized actors to learn the optimal policy for each agent. The main innovation in
this approach is introducing a counterfactual baseline that allows each agent to compare its
current action contribution to the global reward with all the other possible actions. This
is a way to deal with the problem of credit assignment in the multi-agent context, which
happens when the agents do not consider their contribution to a collaborative objective.
This paper proposes and develops a Multi-Agent Hierarchical Framework for Transactive Mi-
crogrids (MAHTM). The framework considers the minimization of the carbon footprint
during the multi-agent learning process to tackle the challenges of climate change.
2 Methods
We propose a three-layer hierarchical RL architecture, as shown in Figure. 1. Each layer
owns a set of agents with different objectives, pursued greedily. In our framework, we denote
a set of Gmicrogrids as M={m1, m2, . . . , m i, . . . , m G}; we denote a group of Dihouseholds
belonging to a microgrid iasHi={hi,1, hi,2, . . . , h i,j, . . . , h i,Di}, the current time step is
denoted as t.
Figure 1: Illustration on the Three Layer Architecture. The solid lines represent the energy
and information flows.
2.1 First layer: household
In this layer, there are four different cases: 1) households that have no access to any energy
asset being only able to consume (”passive consumers”); 2) households that have access to
Photovoltaic (PV) panels to produce electricity during day-hours (”passive prosumers”); 3)
prosumer households that have access to batteries which allow them to have energy dispatch
capabilities and PV generation (”active prosumers”); and 4) consumer households who also
have access to energy storage which provide them the potential to sell surplus energy back
to the microgrid (”active consumers”). Households without batteries (”passive consumers”
or ”passive prosumers”) do not need to execute control actions as they do not have such
capabilities to react to energy fluctuations (e.g., due to weather variation). In contrast, those
”actionable” agents will determine how to charge and discharge the batteries and how to
alter the demand and supply in the microgrid. Based on the above logic, the equations of
2L1: Household L2: Microgrid L3: Distributor Type Unit
Net Enet
t,i,j Enet
t,i Enet
t Energy Wh
Demand Eload
t,i,j - - Energy Wh
PV Gen Epv
t,i,j - - Energy Wh
Battery Ebatt
t,i,j - - Energy Wh
Shortage Est
t,i,j Est
t,i Est
t Energy Wh
Surplus Esp
t,i,j Esp
t,i Esp
t Energy Wh
L1 Import Eimp 1
t,i,j - - Energy Wh
L1 Export Eexp1
t,i,j - - Energy Wh
L2 Import Eimp 2
t,i,j Eimp 2
t,i - Energy Wh
L2 Export Eexp2
t,i,j Eimp 2
t,i - Energy Wh
L3 Import Eimp 3
t,i,j Eimp 3
t,i Eimp 3
t Energy Wh
L3 Export Eexp3
t,i,j Eimp 3
t,i Eimp 3
t Energy Wh
Emission - - ct GHG CO 2/Wh
Sell rsh
t,i rsm
t rsd
t Price $/Wh
Buy rbh
t,i rbm
t rbd
t Price $/Wh
Table 1: Table of defined symbols.
this layer are as follows:
Est
t,i,j=Eimp 1
t,i,j +Eimp 2
t,i,j +Eimp 3
t,i,j (1)
Esp
t,i,j=Eexp1
t,i,j+Eexp2
t,i,j+Eexp3
t,i,j (2)
Enet
t,i,j=Est
t,i,j−Esp
t,i,j=Eload
t,i,j−Epv
t,i,j±Ebatt
t,i,j (3)
In the case of consumer households with no PV panel, the generation Epv
t,i,j= 0. When
Enet
t,i,j≥0(called “shortage” state), it means there is extra energy needed from external
sources (e.g., retailers or other households). When Enet
t,i,j<0(called “surplus” state), there
is surplus energy available to sell back to the external power grid or other households in
shortage. The Equation (3)presents a constraint that should be satisfied as it is impossible
to have both scenarios simultaneously. Finally, we define the objective function of this layer:
minEimp 3
t,i,j(rsd
t+ct) +Eimp 2
t,i,jrsm
t,i+Eimp 1
t,i,jrsh
t,i,ifEnet
t,i,j≥0,
Eexp3
t,i,jrbd
t+Eexp2
t,i,jrbm
t+Eexp1
t,i,jrbh
t,i, ifEnet
t,i,j<0.(4)
2.2 Second layer: microgrid
In this layer, an agent defines the prices rsh
t,iandrbh
t,i. Its objective is to maximize the use of
local energy in a microgrid by defining the pricing policy for local transactions. We described
it using the following equations:
Est
t,i=Eimp 2
t,i +Eimp 3
t,i, Eimp 2
t,i =P
jEimp 2
t,i,j, Eimp 3
t,i =P
jEimp 3
t,i,j,(5)
Esp
t,i=Eexp2
t,i+Eexp3
t,i, Eexp2
t,i=P
jEexp2
t,i,j, Eexp3
t,i=P
jEexp3
t,i,j,(6)
Enet
t,i=Est
t,i−Esp
t,i. (7)
A microgrid will experience an (energy) shortage state when the local energy is insufficient
to cover the internal demand and experience an (energy) surplus state when the distributed
generation surpasses the internal demand. In the first case, a microgrid could access energy
available in other microgrids. In the second case, it could sell energy to other microgrids
experiencing a shortage. If energy is unavailable/over-produced at the current microgrid
layer, it will be imported or exported to the third layer. With this, we can define this layer’s
objective function:
minEimp 3
t,i(rsd
t+ct) +Eimp 2
t,irsm
t,ifEnet
t,i≥0,
Eexp3
t,irbd
t+Eexp2
t,irbm
t, ifEnet
t,i<0.(8)
32.3 Third layer: distributor
In this layer, the agent tries to shape the overall load among the multiple microgrids, enabling
energy trading and simultaneously minimizing the carbon footprint by setting the buy ( rbm
t)
and sell ( rsm
t) prices among the microgrids. The prices for selling energy ( rsd
t) and accepting
surplus ( rbd
t) from the microgrids are not controlled in this layer and are treated as external
inputs (from the previous layer). To define the objective function of the distributor, we need
first to define the following:
Est
t=Eimp 3
t =P
iEimp 3
t,i (9)
Esp
t=Eexp3
t=P
iEexp3
t,i (10)
Enet
t=Est
t−Esp
t=Eimp 3
t−Eexp3
t(11)
Then, we can define the distributor’s objective function as follows:
min
Eimp 3
t(rsd
t+ct),ifEnet
t≥0,
Eexp3
trbd
t, ifEnet
t<0.(12)
In addition, we assume there is only one distributor and that the energy consumed within
or between microgrids has negligible carbon impact. We also implemented a simple local
energy market based on the physical distance between the household and the microgrids.
3 Results and Analysis
3.1 Experimental setup
We configured our environment (in OpenAI Gym) to present different sets of households
for training, validation, and testing. Detail about the precise attributes of the dataset is
present in the appendix. A critical difference from existing work like Xu et al. (2022) is the
possibility of enabling and disabling the stochasticity in our setup.
3.2 Model performance
We propose a performance metric based on how energy cost and carbon impact are improved
by optimally managing distributed storage. The metric measures the scenario without
batteries against the use of our hierarchical control. Each household contributes to the metric
individually, and the upper levels aggregate them to have microgrid-level and distributor-level
performance.
CVXPY MAHTM COMA
Train reward -0.915 -0.993 -1.3
Train price score -0.103 -0.097 0.35
Train emission score -0.223 -0.1522 0.35
Train time 0.9s 10m 2h
Test price score -0.0889 -0.064 0.0625
Test emission score -0.19 -0.097 0.0625
Table 2: Average performance of households (lower is better, except for reward).
Table 2 presents our current empirical results comparing the optimal solution for our scenario
using a linear solver (CVXPY), our framework, and COMA (one of the state-of-the-art MARL
algorithms). One of the things to highlight about our approach is its training speed and
simplicity versus COMA, which is very sensitive to hyperparameter tuning. Our framework
reached solutions very close to the optimal within a reasonable training time.
44 Conclusion
The proposed framework systematically applies the MARL technique to transactive
microgrids. The results are compared with one classic MARL algorithm. A customized
OpenAI Gym environment was also created to serve as the test bench for this work. Our
framework can help the development of local renewable energy markets, fostering emission
reduction and more consumer engagement.
(The source code and demo files have been anonymized and are available in this
repository link.)
References
Amjad Anvari-Moghaddam, Ashkan Rahimi-Kian, Maryam S. Mirian, and Josep M. Guerrero.
A multi-agent based energy management solution for integrated buildings and microgrid
system.Applied Energy , 203:41–56, 2017. ISSN 0306-2619. doi: https://doi.org/10.1016/j.
apenergy.2017.06.007. URL https://www.sciencedirect.com/science/article/pii/
S0306261917307572 .
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
Tang, and Wojciech Zaremba. Openai gym, 2016.
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging properties in self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 9650–
9660, 2021.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon
Whiteson. Counterfactual multi-agent policy gradients, 2017. URL https://arxiv.org/
abs/1705.08926 .
Benjamin D. Leibowicz, Christopher M. Lanham, Max T. Brozynski, José R. Vázquez-Canteli,
Nicolás Castillo Castejón, and Zoltan Nagy. Optimal decarbonization pathways for urban
residential building energy services. Applied Energy , 230:1311–1325, 2018. ISSN 0306-2619.
doi: https://doi.org/10.1016/j.apenergy.2018.09.046. URL https://www.sciencedirect.
com/science/article/pii/S0306261918313552 .
Ming Tan. Multi-agent reinforcement learning: Independent versus cooperative agents. In
International Conference on Machine Learning , 1993.
Jose R. Vazquez-Canteli, Gregor Henze, and Zoltan Nagy. Marlisa: Multi-agent reinforcement
learning with iterative sequential action selection for load shaping of grid-interactive
connected buildings. In Proceedings of the 7th ACM International Conference on Systems
for Energy-Efficient Buildings, Cities, and Transportation , BuildSys ’20, page 170–179,
New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450380614.
doi: 10.1145/3408308.3427604. URL https://doi.org/10.1145/3408308.3427604 .
José R. Vázquez-Canteli, Stepan Ulyanin, Jérôme Kämpf, and Zoltán Nagy. Fusing ten-
sorflow with building energy simulation for intelligent energy management in smart
cities.Sustainable Cities and Society , 45:243–257, 2019. ISSN 2210-6707. doi: https:
//doi.org/10.1016/j.scs.2018.11.021. URL https://www.sciencedirect.com/science/
article/pii/S2210670718314380 .
Zhiwei Xu, Bin Zhang, Dapeng Li, Zeren Zhang, Guangchong Zhou, and Guoliang Fan.
Consensus learning for cooperative multi-agent reinforcement learning, 2022. URL https:
//arxiv.org/abs/2206.02583 .
5Appendix
Algorithms
First layer: Policy Gradient (PG), Advantage Actor-Critic (A2C)
In this approach, the objective of our agent is to maximize the probability of having the
trajectories that show the higher sum reward. It is defined as:
J(θ) =Eπθ"X
tγtrt#
(13)
It can be understood as the expected sum of the discounted rewards obtained by completing
one episode following a defined policy πθ. The factor γhelps prevent the sum from going
infinite and gives more relevance to the rewards obtained in the short term. The whole
idea of this RL method is to maximize 13 using stochastic gradient ascent. By using the
definition of expectation, we can define the policy gradient as:
∇θJ(θ) =Eτ∼πθ(τ)" TX
t=1∇θlogπ θ(at|st)! TX
t=1r(st, at)!#
(14)
In A2C, we use an estimator (a neural network) to represent the policy πθ(at|st), named
Actor. The actor will map the states to the actions and learn the optimal ones. Its training
follows the next steps:
•Sample itrajectories τiusing the actor policy.
•Assuming the policy gradient definition in 14.
•Updating the weights θof the policy as follows: θ←θ+α∇θJ(θ).
Sequentially running multiple trajectories is a long process. For that reason, batch training
is generally implemented to speed up the learning of the policy estimator. By doing so, the
exploration speed increases, modifying the equation 14 as follows:
∇θJ(θ)≈1
NNX
i=1TX
t=1"
∇θlogπ θ(ai
t|si
t) TX
t=1r(si
t, ai
t)!#
(15)
However, by doing so, we add an issue: The variance of ∇θJ(θ)increases. To help solve
this, the advantage function was introduced. First, we start by understanding that the termPT
t=1r(si
t, ai
t)
is the Q(s, a)function, as it represents the expected reward we can get from
doing an action atwhile in state st. Finding a value Vindependent of the neural network
parameters θ, we can subtract it from the Qfunction to re-calibrate the rewards towards the
average action. Thus, the advantage function is defined as:
Aπ(st, at) =Qπ(st, at)−Vπ(st) (16)
The algorithm A2C gets its name from the use of the advantage function16, and the addition
of an extra neural network (the Critic) that approximates Vπ(st)and will be trained with
the experienced Qπ(st, at). In other words, the critic evaluates the actions taken by the
actor and approximates the corresponding values.
Multi-agent RL (MARL)
This is the simplest way of implementing policy gradients in a multi-agent configuration. In
this case, each agent has its actor and critic, interacting with an agent-specific action and
6observation history. This was first introduced in Tan (1993) with a Q-learning algorithm.
When using the same principle with an AC algorithm, it is called an Independent Actor-Critic
(IAC) as explained in Foerster et al. (2017).
In this approach, all the agents’ neural networks share parameters. Thus, only one agent and
one critic are learned in the end. However, each agent has access to different observations
and attributes associated with the household, allowing them to take different actions. This
method helps RL agents with similar tasks to learn faster and better.
Expanding on the equation (13) for the single-agent case, the equation (17) is the equivalent
for the multi-agent case. Where the generalization of the Markov decision process is the
stochastic game, the state transitions and the rewards of the agents ri,t+1result from their
joint actions.
J(θ) = E"X
tγtri,t|x0=x, h#
(17)
Environment
Using the OpenAI Gym toolkit Brockman et al. (2016) as a wrapper for an environment
that uses data generated synthetically. It standardizes the evaluation of diverse RL agents.
Our environment includes stochastic energy generation, agent energy market participation,
realistic battery representation, and a diversified set of household demand profiles.
Dataset
The data used in the current environment was synthetically generated based on real-life data.
For this work, we defined 24 steps representing the hours within a day, with the possibility
of extending to more steps if required. There are three different demand profiles, each
representing a specific use: family, with demand peaks in the morning and early afternoon;
teenagers , with peaks late in the afternoon till early morning; house business , with high
energy usage in the middle of the day (refer to Figure 5 to see the described trends). These
non-shiftable demands are generated with noise, different energy baselines, and a dependency
on stochastic variables such as temperature. Grid energy cost and carbon footprint are
defined by two different sources, nuclear and gas. The first has a more negligible cost and
carbon footprint than the latter. Nuclear generation is relatively more constant in price and
emissions since its production is more stable, though sometimes it is insufficient to supply
all the houses. Hence they decide to produce energy with gas which is more expensive than
nuclear energy and emits more carbon emissions (refer to 5d).
Dataset generation
The dataset generated for this problem has the following parameters (all of them normalized):
•The demand profiles (family, teenager, business) explained before.
•Peak load maximum: The maximum the house can consume.
•PV (photovoltaic) peak generation: The maximum is possible to generate with the
solar panels for that house.
Battery characteristics
•Random state of charge: to decide if the battery will start with a random percentage.
•Capacity: energy capacity of the battery/array of batteries in (kWh) when is not
normalized.
•Efficiency: A value between 0 and 1 represents the one-way efficiency of the battery,
considering the same efficiency for charging and discharging (%).
•State of charge (SoC) max and min: Value between 0 and 1 representing the highest
value and the lowest the SoC a battery can reach.
7PG A2C
Number of discrete actions 40 40
Learning rate of the actor 0.00381 0.00245
Hidden layers of the actor 128 128
Learning rate of the critic - 0.001
Hidden layers of the critic - 128
Discount factor 1.0 1.0
Batch size 32 32
Roll-out steps 24 24
Training steps 2000 2000
Table 3: Hyper-parameter configuration for RL algorithms of the first layer.
•P charge max: Maximum charging rate of a battery (%).
•P discharge Max: Minimum battery charging rate (%).
•Sell Price: Price for injecting energy into the battery (reward to the prosumers).
•Buy price: Price for using energy from the battery ($/kWh).
The configurations for the training, evaluating, and testing in the project are found in Figure
4, 5 and 6. As demonstrated, for train and evaluation, the microgrids are of 6 houses, but for
testing, there are 10. In the current version, the RL algorithm worked with multiple houses
(microgrid) simultaneously and, before, worked with only one.
The data generated for the demand of the different houses are based on the main pattern
for each profile, nonetheless what changes between homes are the state of the battery and
the generation of energy with the solar panels (PV), which is not the same because of the
incorporation of the noise (shown in Figure 2 and 3 in the subfigure “PV and Demand”) for
the generation and the energy load, modeled both of them using the Gaussian distributions
Npv(0,0.1)andNload(0,0.01). Solar energy generation takes a sine function, shifting it to
start after 5 am and shortening it to mimic the morning/daylight. After that, we incorporate
the noise to replicate the possible clouds or weather conditions that can be present. The
noise shows that there is a different result in the mean net energy through time (shown in
Figure 2 and 3 in the subfigure “Mean net energy through time”).
As shown in the tables above, some houses have no solar energy production (the ones in
0’s), which means they need to rely on the battery to make decisions related to the energy.
There is also no battery cell price so far, but this is one of the parameters planned to be
incorporated in the following steps to see more dynamics in the microgrid.
Hyperparameters
In the table 3, we defined the following hyperparameters for the training after fine-tuning
using grid search. Since there is less variance in the Advantage Actor-Critic (A2C), the
number of epochs needed is less than using a policy gradient (PG).
8house 1house 2 house 3house 4house 5 house 6
profile_type family business teenagers family business teenagers
profile_peak_load 1 1 1 0.5 0.3 0.2
battery_random_soc_0 False False False False False False
battery_capacity 1 1 1 1 1 1
battery_efficiency 1 1 1 1 1 1
battery_soc_max 0.9 0.9 0.9 0.9 0.9 0.9
battery_soc_min 0.1 0.1 0.1 0.1 0.1 0.1
battery_p_charge_max 0.8 0.8 0.8 0.8 0.8 0.8
battery_p_discharge_max 0.8 0.8 0.8 0.8 0.8 0.8
pv_peak_pv_gen 1 1 1 0.0 1 0.6
Table 4: Configuration of the houses in the microgrid for Training A2C.
house 1house 2 house 3house 4house 5 house 6
profile_type family business teenagers family business teenagers
profile_peak_load 1 0.8 0.5 0.2 0.3 0.2
battery_random_soc_0 False False False False False False
battery_capacity 1 1 1 0.5 0.9 0.9
battery_efficiency 1 1 1 1 1 1
battery_soc_max 0.9 0.9 0.9 0.9 0.9 0.9
battery_soc_min 0.1 0.1 0.1 0.1 0.1 0.1
battery_p_charge_max 0.8 0.8 0.8 0.8 0.8 0.8
battery_p_discharge_max 0.8 0.8 0.8 0.8 0.8 0.8
pv_peak_pv_gen 0.5 1 0 1 0.3 0.6
Table 5: Configuration of the houses in the microgrid for Evaluating A2C.
house 1house 2 house 3house 4house 5 house 6house 7house 8 house 9house 10
profile_type family business teenagers family business teenagers family business teenagers family
profile_peak_load 1 1 1 0.2 0.6 0.4 0.4 1 0.1 1
battery_random_soc_0 False False False False False False False False False False
battery_capacity 1 1 1 1 1 1 0.8 0.2 1 0.2
battery_efficiency 1 1 1 1 1 1 1 1 1 1
battery_soc_max 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9
battery_soc_min 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1
battery_p_charge_max 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8
battery_p_discharge_max 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8
pv_peak_pv_gen 0 0 0 0.7 1 0.7 1 1 1 0
Table 6: Configuration of the houses in the microgrid for Testing A2C.
9Figure 2: Results of the A2C with the dataset that has no noise.
10Figure 3: Results of the A2C with dataset that has noise.
11Figure 4: Emissions and Price score comparison between CVXPY solver and A2C in the
microgrid for all 3 stages(training, evaluation, and testing).
(a) Family.
(b) Teenagers.
(c) Business.
(d) Price and emissions.
Figure 5: Sample of solutions with solver (CVXPY).
12