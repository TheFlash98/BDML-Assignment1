Neural NERE: Neural Named Entity Relationship Extraction
for End-to-End Climate Change Knowledge Graph Construction
Prakamya Mishra1Rohan Mittal1
Abstract
This paper proposes an end-to-end Neural Named
Entity Relationship Extraction model (called Neu-
ralNERE) for climate change knowledge graph
(KG) construction, directly from the raw text of
relevant news articles. The proposed model will
not only remove the need for any kind of human
supervision for building knowledge bases for cli-
mate change KG construction (used in the case of
supervised or dictionary-based KG construction
methods), but will also prove to be highly valu-
able for analyzing climate change by summarising
relationships between different factors responsi-
ble for climate change, extracting useful insights
& reasoning on pivotal events, and helping indus-
try leaders in making more informed future deci-
sions. Additionally, we also introduce the Science
Daily Climate Change dataset (called SciDCC)
that contains over 11 kclimate change news ar-
ticles scraped from the Science Daily website,
which could be used for extracting prior knowl-
edge for constructing climate change KGs.
1. Introduction
News outlets have played a crucial role in increasing aware-
ness about climate change through their news articles, be-
cause of which more and more people have started to un-
derstand the consequences of climate change. The volume
of news articles published regarding climate change has
been increasing rapidly with the growth in news coverage.
This has made it challenging to extract valuable information
regarding climate change from these news articles. Algo-
rithms that can extract and organize climate change informa-
tion by condensing the relevant knowledge directly from a
large collection of noisy and redundant news articles could
prove to be highly valuable in analyzing relationships be-
tween different factors responsible for climate change. This
*Equal contribution1Independent Researcher, India. Corre-
spondence to: Prakamya Mishra <pkms.research@gmail.com >.
Tackling Climate Change with Machine Learning Workshop at
ICML 2021.would help in generating useful insight and reasoning about
the pivotal events which in turn will help industry leaders in
making more informed policies relating to climate change in
future. Such knowledge can be distilled from the structured
representation of knowledge graphs (KGs) generated from
these news articles.
There has been a growing interest in generating high-quality
KGs for information extraction from raw text (Yu et al.,
2020; Saxena et al., 2020; Wang et al., 2019; Nathani et al.,
2019; Lin et al., 2019; Wities et al., 2017). Previously,
KG construction approaches were either supervised (Bol-
lacker et al., 2008) or semi-supervised (Carlson et al., 2010;
Dong et al., 2014), both of which were expensive and time-
consuming due to the involvement of human supervision.
Recently, neural models have enabled KG construction with-
out any human involvement (Bosselut et al., 2019; Balaze-
vic et al., 2019; Xiong et al., 2018; Trivedi et al., 2017;
Garc ¬¥ƒ±a-Dur ¬¥an et al., 2018). The problem with the exist-
ing approaches is that they all use some prior knowledge
in the form of Knowledge Bases (KB) to learn to predict
relationships between the subject-object entity phrases for
constructing KGs. This is the major reason why not much
work has been done for constructing climate change KGs
since there doesn‚Äôt exist any well-established KBs for cli-
mate change.
To solve this problem, we propose Neural NERE, an end-
to-end Neural Named Entity Relationship Extraction model
for constructing climate change knowledge graphs directly
from the raw text of relevant news articles. Additionally,
we introduce a new climate change news dataset (called
SciDCC dataset1) containing over 11 knews articles scraped
from the Science Daily website, which can be used for
extracting prior knowledge for constructing climate change
knowledge graphs using Neural NERE.
2.SciDCC Dataset
The Science Daily Climate Change dataset called SciDCC
was created by web scraping news articles from the ‚ÄùEarth &
Climate‚Äù and ‚ÄùPlant & Animals‚Äù topics in the environmental
science section of the Science Daily (SD) website. The SD
1https://sites.google.com/view/scidccdatasetNeuralNERE
Figure 1. Neural NERE Model Architecture
Table 1. Key statistics of the SciDCC dataset.
# N EWS ARTICLES 11,539 # N EWS CATEGORY 20
AVG.TITLE LEN . 9.32 M AX.TITLE LEN . 65
AVG.SUMMARY LEN . 47.28 M AX.SUMMARY LEN . 488
AVG.BODY LEN . 523.18 M AX.BODY LEN . 1968
Table 2. News Category Statistics
NO.CATEGORY # NEWS ARTICLES
1 EARTHQUAKES 986
2 POLLUTION 945
3 GENETICALLY MODIFIED 914
4 HURRICANES CYCLONES 844
5 AGRICULTURE & F OOD 844
6 ANIMALS 758
7 WEATHER 719
8 ENDANGERED ANIMALS 701
9 CLIMATE 700
10 OZONE HOLES 623
11 BIOLOGY 620
12 NEWSPECIES 527
13 ENVIRONMENT 478
14 BIOTECHNOLOGY 460
15 GEOGRAPHY 407
16 MICROBES 398
17 EXTINCTION 356
18 ZOOLOGY 210
19 GEOLOGY 28
20 GLOBAL WARMING 21
news articles are relatively more scientiÔ¨Åc when compared
to other news outlets, which makes SD perfect for extracting
scientiÔ¨Åc climate change news. In total, we extracted over
11knews articles from 20 categories relevant to climate
change, where each article consists of a title, summary, and a
body. For each category, we were able to extract a maximum
of 1knews articles. The key statistics of the SciDCC dataset
are summarized in Table 1 and more detailed statistics can
be found below. Here, we provide more detailed statistics
about the SciDCC dataset. Table 2 summarises the no. of
news articles extracted per category. All the histogram
plots (right) in Fig. 2, shows the length distribution of title,
summary, and body of news articles, whereas all the density
plots (left) in Ô¨Ågure shows the cumulative distribution of
these lengths over the years.
Figure 2. Length Distribution statistics of the SciDND dataset.
3.Neural NERE
In the this section, we describe each component of the pro-
posed Neural NERE model (see Fig. 1). The primary objec-
tive of the Neural NERE model is to learn the embedding
representation of the intended relationship phrase that de-
scribes the relationship between any two named entities
present in the previously introduced SciDCC dataset. These
learned embedding representations of the intended relation-
ship phrase for every named entity pairs [Entity 1 (subject),
Entity 2 (Object)] present in the SciDCC dataset will later
be used to generate a climate change knowledge graph us-
ing the [Entity 1 (subject), Relationship, Entity 2 (Object)]
triplets.
We Ô¨Årst create an input corpus by extracting the raw text
from the summary part and body part of all the articles
present in the SciDCC dataset. This input corpus is Ô¨Årst pre-
processed (tokenization, lower-casing, stemming, lemmati-
zation) and then used for: (1) Ô¨Åne-tuning a language modelNeuralNERE
(a character-based language like FastText2or GloVe3) for
learning the word embedding representations corresponding
to every word present in the corpus; (2) for extracting all the
named entity phrases as well as all the possible relationship
phrases. The named entity phrases and the possible relation-
ship phrases are extracted by using Part-of-Speech Tagging
(POS Tagging). All the word tokens are marked with their
corresponding POS tags which are utilized to create (1) an
entity phrase list by extracting all the named entity phrases
using noun-phrase chunking, and (2) a relationship phrase
list by extracting all the possible relationship phrases us-
ing verb-phrase chunking. The Ô¨Åne-tuned language model
is then used to convert the extracted named entity phrases
from the entity phrase list, and relationship phrases from the
relationship phrase list into their corresponding embedding
representations. We propose to use a character-based lan-
guage model to avoid problems while generating embedding
representations for multi-word entity/relationship phrases.
All the extracted named entity phrases are represented by
E, and all the extracted possible relationship phrases are
represented by R.
E= [e1;:::;en];R= [r1;:::;rm]de;dr2N (1)
Here in Equation (1), ei2Rderepresents the de-
dimensional embedding representation of the ithnamed
entity phrase in E;ri2Rdrrepresents the dr-dimensional
embedding representation of ithrelationship phrase in R;
n&mare the number of phrases in EandRrespectively.
Next Neural NERE uses E&Ras input, and tries to learn
the intended relationship representations between all the pos-
sible entity pairs that can be generated from the entity phrase
list. These learned intended relationship phrase representa-
tion will later be used to construct the climate change KG.
For training, Neural NERE uses (1) A pair of entity phrase
representations represented by ( s,o), where the s2Rde
is the entity phrase representation of the ithentity phrase
(inE) which acts as the subject in the subject-object rela-
tionship, and o2Rdeis the entity phrase representation of
thejthentity phrase (in E) which acts as the object in the
subject-object relationship; (2) A relationship phrase matrix
Mrel2Rdrm, which is basically a matrix constructed by
concatenating (
) all themrelationship phrase representa-
tions together from the relationship phrase list, as shown in
Equation (2).
Mrel=r1
r2
:::
rm;Mrel2Rdrm;ri2Rdr(2)
Next, Neural NERE uses an encoder-decoder network for
encoding the relationship between the subject entity phrase
represented by sand object entity phrase represented by o
into an encoded representation zij2Rdr, having the same
embedding representation size as that of the relationship
2https://fasttext.cc/
3https://nlp.stanford.edu/projects/glove/phrases. The encoder network f, Ô¨Årst encodes the input
subject-object entity phrase pairs ( s,o) into an encoded
vectorzij, and the decoder network gthen decodes the
encoded vector zijinto reconstructions represented by ( ^s,
^o), as shown in Equation (3).
^s;^o=g(zij);zij=f(s;o) ^s;^o2Rde(3)
This encoded vector zijrepresents the embedding represen-
tation of the intended relationship phrase between subject-
object entity phrase pairs that Neural NERE is trying to
learn. Although we want the encoded vector zijto capture
the relationship between phrases represented by sando,
but in reality, we don‚Äôt really know much about the nature
of information being captured in the encoded vector. In
order to force zijto capture such relationship-based infor-
mation, Neural NERE uses the relationship phrase matrix
Mrelwhich contains embedding representations of all the
existing relationship phrase. To do this Neural NERE Ô¨Årst
generates attention scores ( ij) by taking a matrix-vector
product () between the transpose of normalized Mrelma-
trix andzij, as shown in Equation (4). The normalization of
Mrelmatrix is done by pre-multiplying it with a diagonal
matrixDrel=diag(1
jr1j;:::;1
jrmj), where the values at the
diagonal are the reciprocal of the absolute values of rith
column inMrelmatrix. These attention scores are then used
for taking an attention-based weighted sum of all the rela-
tionship phrase embedding representations for generating
a new encoded representation represented by ^zijthat also
captures the relationship-based information, as shown in
Equation (5). These attention scores enables Neural NERE
to give more attention to ri‚Äôs that better represents zij.
ij= [1;:::;m]T=DrelMT
relzij (4)
^zij=Mrelij (5)
Now we will use the above generated encoded vector ^zijto
enforce the encoded vector zijto capture relationship-based
information. We will do so by modifying the overall loss
function. The loss function of the proposed model will con-
sist of two terms, (1) The Ô¨Årst term will be the reconstruction
loss represented by L1in Equation (6), which will ensure the
reconstruction of input in the encoder-decoder network; (2)
Second term will be the cosine similarity loss (  logcos())
between the two encoded vectors ^zij&zijrepresented by
L2in Equation (6), which will ensure the learned encoded
representation to capture the relationship-based information
from the existing relationship phrase representations. The
overall loss function ( Loverall ) of the Neural NERE model
will be the addition of the above mentioned individual losses,
as shown in Equation (7).
L1=E(([^s;^o] [s;o])2);L2= logcos( ^zij;zij)(6)NeuralNERE
Figure 3. Effect ofon Knowledge Graph Construction
Loverall =L1+L2 (7)
Next after training the Neural NERE model with all the
possible combinations of subject-object entity phrase pairs
that can be constructed from the entity phrase list using
our custom loss function Loverall , the encoder network f
learns to generate the embedding representation of the in-
tended representation phrase between the subject-object
entity phrases. Now we use the trained encoder network f
to predict the embedding representation zpre
ijof the intended
relationship phrase for all the subject-object entity phrase
pairs. To extract the actual phrase corresponding to zpre
ij,
we Ô¨Årst compute the cosine similarities between zpre
ijand
allri2R. Then the relationship phrase corresponding to
therk, which has the highest cosine similarities with zpre
ijis
chosen as the intended relationship phrase. We don‚Äôt choose
any if all the computed cosine similarity values fall below a
threshold. Such a threshold keeps the model in check and
prohibits the generation of useless relationships (as shown
in Fig. 3). Finally, these triplets comprising of a subject
entity phrase, an object entity phrase, and the predicted rela-
tionship phrase (from Neural NERE) are used to construct
the climate change knowledge graph.
In the proposed model, we introduce a threshold to keep
the model in check during the relationship generation phase.
For intuition, limits the proximity of search for the in-
tended relationship phrase from the predicted representation
of the intended relationship phrase. Decreasing the value
of threshold parameter enables the exploration of relation-
ship phrases that are relatively distant from the predicted
representation of the intended relationship phrase, whereas
increasing the value of threshold parameter prohibits the
exploration relationship phrases that are relatively distant.
This is illustrated in the Figure above. As shown in case 1,
for some value of if there only exist a single rk2Rsuch
that the cosine similarity of rkandzpred
ij is more than the
threshold value , then the relationship phrase correspond-
ing torkwill be chosen as the intended relationship phrase.
Whereas as shown in case 2, for some value of if there
does not exist any rk2Rsuch that the cosine similarityofrkandzpred
ij is more than the threshold value , then no
relationship phrase will be extracted between the subject
and object entities. Case 2 demonstrates the example in
which the proposed model prohibits the generation of use-
less relationships. Finally as shown in case 3, for some value
ofif there exist rk1;rk22R(in other words more than
one relationship phrases) such that the cosine similarities
of bothrk1&rk2withzpred
ij are more than the threshold
value, then the relationship phrase corresponding to rki
with the highest cosine similarity with zpred
ij will be chosen
as the intended relationship phrase. Case 3 demonstrates the
example wherein the proposed model chooses the intended
relationship phrases whose representation is in the closest
proximity to the predicted representation of the intended
relationship phrase.
4. Projected Results
Using the proposed SciDCC dataset and Neural NERE
model we aim to give industry leaders, analyst, and pol-
icymakers a tool for:
‚Ä¢Extracting and organizing climate change information
from a large collection of news articles.
‚Ä¢Analyzing relationships between different factors re-
sponsible for climate change.
‚Ä¢Gathering insight/reasoning about the pivotal events
for more informed climate change policy making.
In conclusion, we proposed Neural NERE, an end-to-end
Neural Named Entity Relationship Extraction model for con-
structing climate change knowledge graphs directly from the
raw text of relevant news articles. We also introduced a new
climate change news dataset (called SciDCC dataset) for
extracting prior knowledge for constructing climate change
knowledge graphs using Neural NERE.
References
Balazevic, I., Allen, C., and Hospedales, T. TuckER: Ten-
sor factorization for knowledge graph completion. In
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pp. 5185‚Äì5194, Hong Kong,
China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-1522. URL https:
//www.aclweb.org/anthology/D19-1522 .
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and
Taylor, J. Freebase: A collaboratively created graph
database for structuring human knowledge. In Pro-
ceedings of the 2008 ACM SIGMOD International Con-
ference on Management of Data , SIGMOD ‚Äô08, pp.NeuralNERE
1247‚Äì1250, New York, NY , USA, 2008. Association for
Computing Machinery. ISBN 9781605581026. doi: 10.
1145/1376616.1376746. URL https://doi.org/
10.1145/1376616.1376746 .
Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celiky-
ilmaz, A., and Choi, Y . COMET: Commonsense trans-
formers for automatic knowledge graph construction. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pp. 4762‚Äì4779,
Florence, Italy, July 2019. Association for Computational
Linguistics. doi: 10.18653/v1/P19-1470. URL https:
//www.aclweb.org/anthology/P19-1470 .
Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hr-
uschka, E., and Mitchell, T. Toward an architecture
for never-ending language learning. Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence , 24(1),
Jul. 2010. URL https://ojs.aaai.org/index.
php/AAAI/article/view/7519 .
Dong, X., Gabrilovich, E., Heitz, G., Horn, W., Lao,
N., Murphy, K., Strohmann, T., Sun, S., and Zhang,
W. Knowledge vault: A web-scale approach to prob-
abilistic knowledge fusion. In Proceedings of the 20th
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining , KDD ‚Äô14, pp. 601‚Äì610,
New York, NY , USA, 2014. Association for Comput-
ing Machinery. ISBN 9781450329569. doi: 10.1145/
2623330.2623623. URL https://doi.org/10.
1145/2623330.2623623 .
Garc ¬¥ƒ±a-Dur ¬¥an, A., Duman Àáci¬¥c, S., and Niepert, M. Learn-
ing sequence encoders for temporal knowledge graph
completion. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing ,
pp. 4816‚Äì4821, Brussels, Belgium, October-November
2018. Association for Computational Linguistics. doi: 10.
18653/v1/D18-1516. URL https://www.aclweb.
org/anthology/D18-1516 .
Lin, B. Y ., Chen, X., Chen, J., and Ren, X. KagNet:
Knowledge-aware graph networks for commonsense rea-
soning. In Proceedings of the 2019 Conference on Empir-
ical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pp. 2829‚Äì2839, Hong Kong,
China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-1282. URL https:
//www.aclweb.org/anthology/D19-1282 .
Nathani, D., Chauhan, J., Sharma, C., and Kaul, M. Learn-
ing attention-based embeddings for relation prediction in
knowledge graphs. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 4710‚Äì4723, Florence, Italy, July 2019. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/P19-1466. URL https://www.aclweb.org/
anthology/P19-1466 .
Saxena, A., Tripathi, A., and Talukdar, P. Improving multi-
hop question answering over knowledge graphs using
knowledge base embeddings. In Proceedings of the 58th
Annual Meeting of the Association for Computational Lin-
guistics , pp. 4498‚Äì4507, Online, July 2020. Association
for Computational Linguistics. doi: 10.18653/v1/2020.
acl-main.412. URL https://www.aclweb.org/
anthology/2020.acl-main.412 .
Trivedi, R., Dai, H., Wang, Y ., and Song, L. Know-evolve:
Deep temporal reasoning for dynamic knowledge graphs.
InProceedings of the 34th International Conference on
Machine Learning - Volume 70 , ICML‚Äô17, pp. 3462‚Äì3471.
JMLR.org, 2017.
Wang, Q., Huang, L., Jiang, Z., Knight, K., Ji, H., Bansal,
M., and Luan, Y . PaperRobot: Incremental draft genera-
tion of scientiÔ¨Åc ideas. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 1980‚Äì1991, Florence, Italy, July 2019. Asso-
ciation for Computational Linguistics. doi: 10.18653/
v1/P19-1191. URL https://www.aclweb.org/
anthology/P19-1191 .
Wities, R., Shwartz, V ., Stanovsky, G., Adler, M., Shapira,
O., Upadhyay, S., Roth, D., Martinez Camara, E.,
Gurevych, I., and Dagan, I. A consolidated open knowl-
edge representation for multiple texts. In Proceedings of
the 2nd Workshop on Linking Models of Lexical, Senten-
tial and Discourse-level Semantics , pp. 12‚Äì24, Valencia,
Spain, April 2017. Association for Computational Lin-
guistics. doi: 10.18653/v1/W17-0902. URL https:
//www.aclweb.org/anthology/W17-0902 .
Xiong, W., Yu, M., Chang, S., Guo, X., and Wang, W. Y .
One-shot relational learning for knowledge graphs. In
Proceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 1980‚Äì1990,
Brussels, Belgium, October-November 2018. Associ-
ation for Computational Linguistics. doi: 10.18653/
v1/D18-1223. URL https://www.aclweb.org/
anthology/D18-1223 .
Yu, H., Li, H., Mao, D., and Cai, Q. A relationship ex-
traction method for domain knowledge graph construc-
tion. World Wide Web , 23(2):735‚Äì753, 2020. doi:
10.1007/s11280-019-00765-y. URL https://doi.
org/10.1007/s11280-019-00765-y .